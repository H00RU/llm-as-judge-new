# LLM-as-Judge: GRPO Training Framework for Mixed-Domain LLM Evaluation

A production-ready framework for training language models using **Group Relative Policy Optimization (GRPO)** on balanced mixed-domain datasets (Math, QA, Code).

## ğŸ¯ Current Status

âœ… **Plan B Implementation Complete** (Soft Learning with Metadata Flags)
- Constraint violations â†’ metadata flags + GRPO penalties (instead of hard blocks)
- Three-level penalty hierarchy: -5.0 (mismatch), -3.0 (validation), -8.0/-7.0/-10.0 (errors)
- 100% test pass rate (26/26 tests)
- [Details](PLAN_B_SESSION_SUMMARY.md)

âœ… **Configuration Restored** (Reference Project Alignment)
- LoRA rank: 64 | Batch size: 4 | Learning rate: 2.0e-5 | Temperature: 0.4
- Max tokens increased to 4096 (prevent truncation)
- [Config Details](CONFIG_RESTORATION_SUMMARY.md)

## ğŸš€ Quick Start

```bash
# Start training with Plan B configuration
python train.py --config config/training.yaml
```

**New to the project?**
ğŸ‘‰ [Read the SETUP guide](docs/SETUP.md) | ğŸ“š [Full documentation](docs/README.md) | ğŸ“‹ [Plan B Summary](PLAN_B_SESSION_SUMMARY.md)

---

## ğŸ“‹ Documentation

### Core Documentation (docs/)

| Document | Purpose |
|----------|---------|
| [README.md](docs/README.md) | ğŸ“– Complete project overview and architecture |
| [SETUP.md](docs/SETUP.md) | ğŸ”§ Installation and environment setup |
| [INSTALLATION.md](docs/INSTALLATION.md) | ğŸ“¥ Detailed installation steps |
| [DATA.md](docs/DATA.md) | ğŸ“Š Data mixing strategy (5:1 split, domain balance) |
| [TRAINING.md](docs/TRAINING.md) | ğŸ“ Training configuration and modes |
| [CONTRIBUTING.md](docs/CONTRIBUTING.md) | ğŸ¤ How to contribute |

### Implementation Documentation (Root)

| Document | Purpose |
|----------|---------|
| [PLAN_B_SESSION_SUMMARY.md](PLAN_B_SESSION_SUMMARY.md) | ğŸ¯ Plan B soft learning approach overview |
| [PLAN_B_IMPLEMENTATION_VERIFICATION.md](PLAN_B_IMPLEMENTATION_VERIFICATION.md) | âœ… Plan B test results (26/26 tests pass) |
| [CONFIG_RESTORATION_SUMMARY.md](CONFIG_RESTORATION_SUMMARY.md) | âš™ï¸ Configuration parameter recovery details |
| [CONFIG_QUICK_REFERENCE.txt](CONFIG_QUICK_REFERENCE.txt) | ğŸ“Œ Quick parameter reference card |
| [IMPLEMENTATION_COMPLETE.txt](IMPLEMENTATION_COMPLETE.txt) | ğŸ“ Complete implementation status report |

---

## ğŸ¯ Key Features

âœ… **Plan B Soft Learning**: Operator constraints via metadata flags + GRPO penalties (not hard blocks)
âœ… **6-Dataset Mixed Training**: GSM8K, MATH, SQuAD2.0, HotpotQA, HumanEval, MBPP
âœ… **Production-Ready GRPO**: Online learning with three-tier penalty hierarchy
âœ… **Multi-Model Support**: Qwen2.5-7B with LoRA (rank=64, alpha=64)
âœ… **Domain-Balanced Sampling**: 5:1 train/test split, 4:3:3 cross-domain ratio
âœ… **LLM Judge Integration**: gpt-4o-mini for semantic evaluation & AFlow execution
âœ… **Optimized Configuration**: LoRA rank=64, batch_size=4, learning_rate=2.0e-5, temperature=0.4

---

## ğŸ—ï¸ Project Structure

```
llm-as-judge/
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ training.yaml          # GRPO training (Plan B optimized)
â”‚   â”œâ”€â”€ aflow_llm.yaml         # AFlow executor config
â”‚   â””â”€â”€ aflow_operators.yaml   # Operator definitions
â”œâ”€â”€ docs/                      # Core documentation
â”œâ”€â”€ src/                       # Core training code (15 modules)
â”‚   â”œâ”€â”€ aflow_executor.py      # Plan B: soft constraint detection
â”‚   â”œâ”€â”€ grpo_trainer.py        # Plan B: three-tier penalty hierarchy
â”‚   â”œâ”€â”€ rl_workflow_generator.py # Plan B: soft generation guidance
â”‚   â”œâ”€â”€ workflow_validator.py  # Plan B: warning mode validation
â”‚   â””â”€â”€ ... (10 more modules)
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ scripts/                   # Data processing and evaluation
â”œâ”€â”€ train.py                   # Training entry point
â”œâ”€â”€ test_plan_b_changes.py     # Plan B verification (26/26 tests)
â””â”€â”€ requirements.txt           # Dependencies
```

---

## ğŸš¦ Training Workflow

```
1. Data Preparation
   â””â”€ Download datasets (GSM8K, MATH, HumanEval, etc.)
   â””â”€ Process and mix (5:1 train/test, 4:3:3 domains)

2. Model Training (Plan B)
   â””â”€ Generate workflows (RL policy)
   â””â”€ Execute with AFlow (gpt-4o-mini executor)
   â””â”€ Compute rewards with three-tier penalty system
   â””â”€ Update weights via GRPO gradients

3. Constraint Learning
   â””â”€ Operator-problem type mismatch â†’ -5.0 penalty
   â””â”€ Validation failures â†’ -3.0 penalty
   â””â”€ Execution errors â†’ -8.0 to -10.0 penalties
   â””â”€ RL model learns constraints naturally

4. Monitoring & Evaluation
   â””â”€ W&B tracking (metrics by domain and error type)
   â””â”€ Checkpoint saving every 25 steps
   â””â”€ No Fallback overhead (Plan B removes hard blocks)
```

## ğŸ“ Getting Help

- ğŸ“– **Full Docs**: [docs/README.md](docs/README.md)
- ğŸ¯ **Quick Setup**: [docs/SETUP.md](docs/SETUP.md)
- ğŸ“Š **Training Guide**: [docs/TRAINING.md](docs/TRAINING.md)
- ğŸ’¡ **Plan B Details**: [PLAN_B_SESSION_SUMMARY.md](PLAN_B_SESSION_SUMMARY.md)
- âš™ï¸ **Config Help**: [CONFIG_QUICK_REFERENCE.txt](CONFIG_QUICK_REFERENCE.txt)

---

**Implementation Status**: âœ… Complete (Plan B + Configuration Restored)
**Test Coverage**: âœ… 100% (26/26 tests pass)
**Generated with Claude Code** ğŸ¤–
