#!/usr/bin/env python3
"""
é¢„å¤„ç†æ•°æ®é›†ï¼Œç»Ÿä¸€ä¸ºJSONLæ ¼å¼ï¼Œæ”¯æŒæ··åˆè®­ç»ƒ

æ··åˆç­–ç•¥ï¼š
1. åŸå§‹åˆ†å‰²ï¼š83.3% train : 16.7% testï¼ˆ5:1æ¯”ä¾‹ï¼‰
2. åŸŸå†…å‡è¡¡ï¼šæ¯ä¸ªåŸŸå†…ä¸¤ä¸ªæ•°æ®é›†å„å 50%ï¼ˆå°æ•°æ®é›†ä¸Šé‡‡æ ·åˆ°å¤§æ•°æ®é›†å¤§å°ï¼‰
3. è·¨åŸŸæ··åˆï¼šæŒ‰4:3:3æ¯”ä¾‹æ··åˆä¸‰ä¸ªåŸŸï¼ˆmath:qa:codeï¼‰
4. è¾“å‡ºï¼štrain_mixedã€test_mixedä¸¤ä¸ªæ–‡ä»¶ï¼ˆå„è‡ªåˆ†åˆ«å¤„ç†ï¼‰
"""

import json
import random
from pathlib import Path
from typing import Dict, List, Any

# å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ•°æ®åˆ†å‰²çš„å¯é‡å¤æ€§
RANDOM_SEED = 42
random.seed(RANDOM_SEED)

print(f"âœ… éšæœºç§å­å·²å›ºå®šä¸º: {RANDOM_SEED} (ç¡®ä¿æ•°æ®åˆ†å‰²å¯é‡å¤)")
print("=" * 80)

class DatasetProcessor:
    """æ•°æ®é›†ç»Ÿä¸€å¤„ç†å™¨"""

    def __init__(self, raw_dir="data/raw", processed_dir="data/processed"):
        self.raw_dir = Path(raw_dir)
        self.processed_dir = Path(processed_dir)
        self.processed_dir.mkdir(parents=True, exist_ok=True)

    def _ensure_data_separation(self, train_samples, test_samples, dataset_name):
        """
        ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å®Œå…¨åˆ†ç¦»ï¼Œé¿å…æ•°æ®æ³„éœ²

        Args:
            train_samples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨
            test_samples: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
            dataset_name: æ•°æ®é›†åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            (filtered_train_samples, filtered_test_samples, duplicate_count)
        """
        # æ”¶é›†è®­ç»ƒé›†ä¸­çš„æ‰€æœ‰original_id
        train_ids = set()
        for sample in train_samples:
            orig_id = sample.get("metadata", {}).get("original_id", "")
            if orig_id:
                train_ids.add(orig_id)

        # è¿‡æ»¤æµ‹è¯•é›†ä¸­ä»»ä½•å¯èƒ½é‡å¤çš„ID
        filtered_test_samples = []
        duplicate_count = 0
        for sample in test_samples:
            orig_id = sample.get("metadata", {}).get("original_id", "")
            if orig_id and orig_id not in train_ids:
                filtered_test_samples.append(sample)
            elif not orig_id:  # å¦‚æœæ²¡æœ‰original_idï¼Œä¿ç•™ï¼ˆä¸ºäº†å‘åå…¼å®¹ï¼‰
                filtered_test_samples.append(sample)
            else:
                duplicate_count += 1

        if duplicate_count > 0:
            print(f"    âš ï¸  {dataset_name}: å‘ç°å¹¶ç§»é™¤äº† {duplicate_count} ä¸ªé‡å¤æ ·æœ¬")

        return train_samples, filtered_test_samples, duplicate_count

    def process_gsm8k(self):
        """å¤„ç†GSM8K"""
        print("\nå¤„ç† GSM8K...")
        input_file = self.raw_dir / "math" / "gsm8k.jsonl"
        output_dir = self.processed_dir / "gsm8k"
        output_dir.mkdir(exist_ok=True)

        if not input_file.exists():
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return 0

        samples = []
        with open(input_file) as f:
            for idx, line in enumerate(f):
                try:
                    item = json.loads(line)
                    sample = {
                        "id": f"gsm8k_{idx}",
                        "dataset": "gsm8k",
                        "domain": "math",
                        "question": item.get("question", ""),
                        "reference_answer": item.get("answer", "").split("\n#### ")[-1].strip(),
                        "answer_type": "numeric",
                        "metadata": {
                            "source": "gsm8k",
                            "original_id": str(idx)
                        }
                    }
                    samples.append(sample)
                except Exception as e:
                    print(f"    å¤„ç†ç¬¬{idx}æ¡å¤±è´¥: {e}")
                    continue

        random.shuffle(samples)
        n = len(samples)
        # 5:1åˆ†å‰² (83.3%:16.7%)
        train_idx = int(n * 5 / 6)

        train_samples = samples[:train_idx]
        test_samples = samples[train_idx:]

        # ç¡®ä¿æ•°æ®å®Œå…¨åˆ†ç¦»ï¼Œé¿å…æ³„éœ²
        final_train, final_test, duplicate_count = self._ensure_data_separation(
            train_samples, test_samples, "GSM8K"
        )

        self._save_jsonl(output_dir / "train.jsonl", final_train)
        self._save_jsonl(output_dir / "test.jsonl", final_test)

        meta = {
            "dataset": "gsm8k",
            "domain": "math",
            "total": len(samples),
            "train": len(final_train),
            "test": len(final_test),
            "filtered_duplicates": duplicate_count
        }
        with open(output_dir / "meta.json", "w") as f:
            json.dump(meta, f, indent=2)

        print(f"  âœ… GSM8K: {len(samples)} æ ·æœ¬ (train:{len(samples[:train_idx])} test:{len(samples[train_idx:])})")
        return len(samples)

    def process_math(self):
        """å¤„ç†MATH"""
        print("\nå¤„ç† MATH...")
        input_file = self.raw_dir / "math" / "math.jsonl"
        output_dir = self.processed_dir / "math"
        output_dir.mkdir(exist_ok=True)

        if not input_file.exists():
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return 0

        samples = []
        with open(input_file) as f:
            for idx, line in enumerate(f):
                try:
                    item = json.loads(line)
                    sample = {
                        "id": f"math_{idx}",
                        "dataset": "math",
                        "domain": "math",
                        "question": item.get("problem", ""),
                        "reference_answer": item.get("solution", ""),
                        "answer_type": "text",
                        "metadata": {
                            "source": "math",
                            "original_id": str(idx)
                        }
                    }
                    samples.append(sample)
                except Exception as e:
                    print(f"    å¤„ç†ç¬¬{idx}æ¡å¤±è´¥: {e}")
                    continue

        random.shuffle(samples)
        n = len(samples)
        train_idx = int(n * 5 / 6)

        self._save_jsonl(output_dir / "train.jsonl", samples[:train_idx])
        self._save_jsonl(output_dir / "test.jsonl", samples[train_idx:])

        meta = {
            "dataset": "math",
            "domain": "math",
            "total": len(samples),
            "train": len(samples[:train_idx]),
            "test": len(samples[train_idx:])
        }
        with open(output_dir / "meta.json", "w") as f:
            json.dump(meta, f, indent=2)

        print(f"  âœ… MATH: {len(samples)} æ ·æœ¬ (train:{len(samples[:train_idx])} test:{len(samples[train_idx:])})")
        return len(samples)

    def process_squad2(self):
        """å¤„ç†SQuAD2.0"""
        print("\nå¤„ç† SQuAD 2.0...")
        input_file = self.raw_dir / "qa" / "squad2.jsonl"
        output_dir = self.processed_dir / "squad2"
        output_dir.mkdir(exist_ok=True)

        if not input_file.exists():
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return 0

        samples = []
        with open(input_file) as f:
            for idx, line in enumerate(f):
                try:
                    item = json.loads(line)
                    # SQuAD 2.0æ ¼å¼å¤„ç† - åŒ…å«æ— ç­”æ¡ˆé—®é¢˜
                    answers = item.get("answers", {})
                    text_list = answers.get("text", [])

                    # å¤„ç†æ— ç­”æ¡ˆé—®é¢˜ï¼ˆSQuAD 2.0ç‰¹æœ‰ï¼‰
                    if isinstance(text_list, list) and len(text_list) > 0:
                        answer_text = text_list[0]
                    elif isinstance(text_list, str):
                        answer_text = text_list
                    else:
                        # æ— ç­”æ¡ˆé—®é¢˜ï¼Œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æˆ–è·³è¿‡
                        answer_text = "æ— æ³•å›ç­”"
                        # æˆ–è€…è·³è¿‡æ— ç­”æ¡ˆé—®é¢˜ï¼šcontinue

                    sample = {
                        "id": f"squad2_{idx}",
                        "dataset": "squad2",
                        "domain": "qa",
                        "question": item.get("question", ""),
                        "reference_answer": answer_text,
                        "answer_type": "text",
                        "metadata": {
                            "source": "squad2",
                            "original_id": item.get("id", str(idx)),
                            "context": item.get("context", "")[:200],
                            "is_impossible": len(text_list) == 0  # æ ‡è®°æ— ç­”æ¡ˆé—®é¢˜
                        }
                    }
                    samples.append(sample)
                except Exception as e:
                    print(f"    å¤„ç†ç¬¬{idx}æ¡å¤±è´¥: {e}")
                    continue

        random.shuffle(samples)
        n = len(samples)
        train_idx = int(n * 5 / 6)

        train_samples = samples[:train_idx]
        test_samples = samples[train_idx:]

        # ç¡®ä¿æ•°æ®å®Œå…¨åˆ†ç¦»ï¼Œé¿å…æ³„éœ²
        final_train, final_test, duplicate_count = self._ensure_data_separation(
            train_samples, test_samples, "SQuAD 2.0"
        )

        self._save_jsonl(output_dir / "train.jsonl", final_train)
        self._save_jsonl(output_dir / "test.jsonl", final_test)

        meta = {
            "dataset": "squad2",
            "domain": "qa",
            "total": len(samples),
            "train": len(final_train),
            "test": len(final_test),
            "filtered_duplicates": duplicate_count
        }
        with open(output_dir / "meta.json", "w") as f:
            json.dump(meta, f, indent=2)

        print(f"  âœ… SQuAD 2.0: {len(samples)} æ ·æœ¬ (train:{len(samples[:train_idx])} test:{len(samples[train_idx:])})")
        return len(samples)

    def process_hotpotqa(self):
        """å¤„ç†HotpotQA"""
        print("\nå¤„ç† HotpotQA...")
        input_file = self.raw_dir / "qa" / "hotpotqa.jsonl"
        output_dir = self.processed_dir / "hotpotqa"
        output_dir.mkdir(exist_ok=True)

        if not input_file.exists():
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return 0

        samples = []
        with open(input_file) as f:
            for idx, line in enumerate(f):
                try:
                    item = json.loads(line)
                    sample = {
                        "id": f"hotpotqa_{idx}",
                        "dataset": "hotpotqa",
                        "domain": "qa",
                        "question": item.get("question", ""),
                        "reference_answer": item.get("answer", ""),
                        "answer_type": "text",
                        "metadata": {
                            "source": "hotpotqa",
                            "original_id": str(idx)
                        }
                    }
                    samples.append(sample)
                except Exception as e:
                    print(f"    å¤„ç†ç¬¬{idx}æ¡å¤±è´¥: {e}")
                    continue

        random.shuffle(samples)
        n = len(samples)
        train_idx = int(n * 5 / 6)

        self._save_jsonl(output_dir / "train.jsonl", samples[:train_idx])
        self._save_jsonl(output_dir / "test.jsonl", samples[train_idx:])

        meta = {
            "dataset": "hotpotqa",
            "domain": "qa",
            "total": len(samples),
            "train": len(samples[:train_idx]),
            "test": len(samples[train_idx:])
        }
        with open(output_dir / "meta.json", "w") as f:
            json.dump(meta, f, indent=2)

        print(f"  âœ… HotpotQA: {len(samples)} æ ·æœ¬ (train:{len(samples[:train_idx])} test:{len(samples[train_idx:])})")
        return len(samples)

    def process_humaneval(self):
        """å¤„ç†HumanEval"""
        print("\nå¤„ç† HumanEval...")
        input_file = self.raw_dir / "code" / "humaneval.jsonl"
        output_dir = self.processed_dir / "humaneval"
        output_dir.mkdir(exist_ok=True)

        if not input_file.exists():
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return 0

        samples = []
        with open(input_file) as f:
            for idx, line in enumerate(f):
                try:
                    item = json.loads(line)
                    sample = {
                        "id": f"humaneval_{idx}",
                        "dataset": "humaneval",
                        "domain": "code",
                        "question": item.get("prompt", ""),
                        "reference_answer": item.get("canonical_solution", ""),
                        "answer_type": "code",
                        "entry_point": item.get("entry_point", ""),  # âœ… ä¿ç•™entry_point
                        "test": item.get("test", ""),  # âœ… ä¿ç•™testå­—æ®µï¼ˆå…³é”®ï¼ï¼‰
                        "metadata": {
                            "source": "humaneval",
                            "original_id": str(item.get("task_id", idx))
                        }
                    }
                    samples.append(sample)
                except Exception as e:
                    print(f"    å¤„ç†ç¬¬{idx}æ¡å¤±è´¥: {e}")
                    continue

        random.shuffle(samples)
        n = len(samples)
        train_idx = int(n * 5 / 6)

        self._save_jsonl(output_dir / "train.jsonl", samples[:train_idx])
        self._save_jsonl(output_dir / "test.jsonl", samples[train_idx:])

        meta = {
            "dataset": "humaneval",
            "domain": "code",
            "total": len(samples),
            "train": len(samples[:train_idx]),
            "test": len(samples[train_idx:])
        }
        with open(output_dir / "meta.json", "w") as f:
            json.dump(meta, f, indent=2)

        print(f"  âœ… HumanEval: {len(samples)} æ ·æœ¬ (train:{len(samples[:train_idx])} test:{len(samples[train_idx:])})")
        return len(samples)

    def process_mbpp(self):
        """å¤„ç†MBPP"""
        print("\nå¤„ç† MBPP...")
        input_file = self.raw_dir / "code" / "mbpp.jsonl"
        output_dir = self.processed_dir / "mbpp"
        output_dir.mkdir(exist_ok=True)

        if not input_file.exists():
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return 0

        samples = []
        with open(input_file) as f:
            for idx, line in enumerate(f):
                try:
                    item = json.loads(line)

                    # ä»codeä¸­æå–å‡½æ•°åä½œä¸ºentry_point
                    code = item.get("code", "")
                    import re as regex_module
                    match = regex_module.search(r'def\s+(\w+)\s*\(', code)
                    entry_point = match.group(1) if match else f"func_{idx}"

                    # å¤„ç†æµ‹è¯•ç”¨ä¾‹ï¼ˆtest_listè½¬æ¢ä¸ºtestå­—ç¬¦ä¸²ï¼‰
                    test_list = item.get("test_list", [])
                    if test_list:
                        # åˆå¹¶å¤šä¸ªæµ‹è¯•ç”¨ä¾‹ä¸ºä¸€ä¸ªæµ‹è¯•å‡½æ•°
                        test_code = f"def check(candidate):\n"
                        for test_case in test_list:
                            # æ¯ä¸ªtest_caseæ˜¯ä¸€ä¸ªassertè¯­å¥ï¼Œéœ€è¦å°†å‡½æ•°åæ›¿æ¢ä¸ºcandidate
                            test_case = test_case.replace(entry_point, "candidate")
                            test_code += f"    {test_case}\n"
                        test = test_code
                    else:
                        test = ""

                    sample = {
                        "id": f"mbpp_{idx}",
                        "dataset": "mbpp",
                        "domain": "code",
                        "question": item.get("text", ""),
                        "reference_answer": item.get("code", ""),
                        "answer_type": "code",
                        "entry_point": entry_point,  # âœ… ä»codeæå–å‡½æ•°å
                        "test": test,  # âœ… ä¿ç•™testå­—æ®µï¼ˆè½¬æ¢åçš„æµ‹è¯•å‡½æ•°ï¼‰
                        "metadata": {
                            "source": "mbpp",
                            "original_id": str(item.get("task_id", idx))
                        }
                    }
                    samples.append(sample)
                except Exception as e:
                    print(f"    å¤„ç†ç¬¬{idx}æ¡å¤±è´¥: {e}")
                    continue

        random.shuffle(samples)
        n = len(samples)
        train_idx = int(n * 5 / 6)

        self._save_jsonl(output_dir / "train.jsonl", samples[:train_idx])
        self._save_jsonl(output_dir / "test.jsonl", samples[train_idx:])

        meta = {
            "dataset": "mbpp",
            "domain": "code",
            "total": len(samples),
            "train": len(samples[:train_idx]),
            "test": len(samples[train_idx:])
        }
        with open(output_dir / "meta.json", "w") as f:
            json.dump(meta, f, indent=2)

        print(f"  âœ… MBPP: {len(samples)} æ ·æœ¬ (train:{len(samples[:train_idx])} test:{len(samples[train_idx:])})")
        return len(samples)

    def _balance_domain_split(self, domain: str, split: str, datasets_structure: Dict) -> List[Dict]:
        """
        åœ¨å•ä¸ªåŸŸå†…è¿›è¡Œå‡è¡¡é‡‡æ ·ï¼ˆ50:50ï¼‰

        Args:
            domain: åŸŸå (math/qa/code)
            split: æ•°æ®åˆ†å‰² (train/test)
            datasets_structure: æ•°æ®é›†ç»“æ„

        Returns:
            å‡è¡¡åçš„è¯¥åŸŸæ‰€æœ‰æ•°æ®
        """
        dataset_names = datasets_structure[domain]

        # åŠ è½½è¯¥åŸŸçš„ä¸¤ä¸ªæ•°æ®é›†çš„æŒ‡å®šåˆ†å‰²
        datasets_data = {}
        for ds_name in dataset_names:
            dataset_dir = self.processed_dir / ds_name
            split_file = dataset_dir / f"{split}.jsonl"

            if split_file.exists():
                with open(split_file) as f:
                    datasets_data[ds_name] = [json.loads(line) for line in f]
            else:
                datasets_data[ds_name] = []

        # æ‰¾åˆ°è¯¥åŸŸå†…æœ€å¤§çš„æ•°æ®é›†å¤§å°
        max_size = max(len(datasets_data[ds_name]) for ds_name in dataset_names)

        if max_size == 0:
            return []

        # å¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œé‡‡æ ·è‡³max_sizeï¼ˆå…è®¸é‡å¤ï¼‰
        print(f"    [{split.upper()}] {domain.upper()}åŸŸå‡è¡¡:")
        balanced_data = []

        for ds_name in dataset_names:
            data = datasets_data[ds_name]
            if len(data) < max_size:
                # å°æ•°æ®é›†ï¼šé‡å¤é‡‡æ ·
                balanced = random.choices(data, k=max_size)
                print(f"      {ds_name:15} {len(data):6,} â†’ {len(balanced):6,} (é‡é‡‡æ ·)")
            else:
                # å¤§æ•°æ®é›†æˆ–ç›¸ç­‰ï¼šç›´æ¥é‡‡æ ·
                balanced = random.sample(data, max_size)
                print(f"      {ds_name:15} {len(data):6,} â†’ {len(balanced):6,} (æ¬ é‡‡æ ·)")

            balanced_data.extend(balanced)

        return balanced_data

    def create_mixed_dataset(self):
        """
        åˆ›å»ºæ··åˆè®­ç»ƒæ•°æ®

        æ­¥éª¤ï¼š
        1. åŸå§‹æ•°æ®é›†åˆ†å‰²ï¼š83.3% train : 16.7% test
        2. å¯¹train/teståˆ†åˆ«æ‰§è¡Œï¼š
           a. åŸŸå†…å‡è¡¡ï¼šæ¯ä¸ªåŸŸå†…ä¸¤ä¸ªæ•°æ®é›†å„å 50%
           b. è·¨åŸŸ4:3:3æ··åˆï¼šmath:qa:code = 4:3:3
        3. è¾“å‡ºï¼štrain_mixedã€test_mixedä¸¤ä¸ªæ–‡ä»¶
        """
        print("\n" + "=" * 80)
        print("åˆ›å»ºæ··åˆè®­ç»ƒæ•°æ®")
        print("=" * 80)

        mixed_dir = self.processed_dir.parent / "mixed"
        mixed_dir.mkdir(exist_ok=True)

        datasets_structure = {
            "math": ["gsm8k", "math"],
            "qa": ["squad2", "hotpotqa"],
            "code": ["humaneval", "mbpp"]
        }

        # å¯¹train/testä¸¤ä¸ªåˆ†å‰²åˆ†åˆ«å¤„ç†
        mixed_data = {}

        for split in ["train", "test"]:
            print(f"\nğŸ“Š æ­¥éª¤1ï¼š{split.upper()}éƒ¨åˆ†çš„åŸŸå†…å‡è¡¡é‡‡æ ·")

            # æ­¥éª¤1ï¼šåŸŸå†…å‡è¡¡
            domain_balanced_data = {}
            for domain in datasets_structure.keys():
                domain_balanced_data[domain] = self._balance_domain_split(domain, split, datasets_structure)

            # æ­¥éª¤2ï¼šè·¨åŸŸ4:3:3æ··åˆ
            print(f"\nğŸ¯ æ­¥éª¤2ï¼š{split.upper()}éƒ¨åˆ†çš„è·¨åŸŸ4:3:3æ··åˆ")

            math_data = domain_balanced_data["math"]
            qa_data = domain_balanced_data["qa"]
            code_data = domain_balanced_data["code"]

            # è®¡ç®—è·¨åŸŸé‡‡æ ·å¤§å°ï¼ˆminåŸåˆ™ç¡®ä¿æ¯”ä¾‹ä¸€è‡´ï¼‰
            total_available = min(
                int(len(math_data) / 0.4),
                int(len(qa_data) / 0.3),
                int(len(code_data) / 0.3)
            )

            math_count = int(total_available * 0.4)
            qa_count = int(total_available * 0.3)
            code_count = int(total_available * 0.3)

            # é‡‡æ ·
            math_samples = random.choices(math_data, k=math_count) if len(math_data) > 0 else []
            qa_samples = random.choices(qa_data, k=qa_count) if len(qa_data) > 0 else []
            code_samples = random.choices(code_data, k=code_count) if len(code_data) > 0 else []

            print(f"  é‡‡æ ·ç»“æœ:")
            print(f"    math: {len(math_samples):8,} (40.0%)")
            print(f"    qa:   {len(qa_samples):8,} (30.0%)")
            print(f"    code: {len(code_samples):8,} (30.0%)")

            # åˆå¹¶å¹¶shuffle
            all_mixed = math_samples + qa_samples + code_samples
            random.shuffle(all_mixed)

            mixed_data[split] = all_mixed
            print(f"  æ€»è®¡: {len(all_mixed):,} æ ·æœ¬")

        # ä¿å­˜mixedæ•°æ®
        print(f"\nğŸ’¾ ä¿å­˜æ··åˆæ•°æ®:")
        self._save_jsonl(mixed_dir / "train_mixed.jsonl", mixed_data["train"])
        self._save_jsonl(mixed_dir / "test_mixed.jsonl", mixed_data["test"])

        print(f"  âœ… train_mixed.jsonl: {len(mixed_data['train']):,} æ ·æœ¬")
        print(f"  âœ… test_mixed.jsonl:  {len(mixed_data['test']):,} æ ·æœ¬")

        # ä¿å­˜ä¿¡æ¯
        info = {
            "split_ratio": "5:1 (train:test = 83.3%:16.7%)",
            "domain_intra_balance": "50:50 per domain (small dataset resampled to match large)",
            "cross_domain_ratio": "4:3:3 (math:qa:code)",
            "total_train": len(mixed_data["train"]),
            "total_test": len(mixed_data["test"]),
            "domain_distribution_train": {
                "math": sum(1 for x in mixed_data["train"] if x["domain"] == "math"),
                "qa": sum(1 for x in mixed_data["train"] if x["domain"] == "qa"),
                "code": sum(1 for x in mixed_data["train"] if x["domain"] == "code")
            }
        }

        # è®¡ç®—ç™¾åˆ†æ¯”
        for domain_key, count in info["domain_distribution_train"].items():
            info[f"{domain_key}_pct"] = round(count / len(mixed_data["train"]) * 100, 2) if mixed_data["train"] else 0

        with open(mixed_dir / "info.json", "w") as f:
            json.dump(info, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“‹ ä¿¡æ¯å·²ä¿å­˜åˆ°: {mixed_dir}/info.json")

        return info

    def create_test_sets(self):
        """åˆ›å»ºå•ä¸ªæ•°æ®é›†çš„testæ–‡ä»¶ä¾›åç»­è¯„ä¼°"""
        print("\n" + "=" * 80)
        print("åˆ›å»ºå•ä¸ªæ•°æ®é›†çš„testé›†ï¼ˆç”¨äºåˆ†åˆ«è¯„ä¼°å„åŸŸï¼‰")
        print("=" * 80)

        test_dir = self.processed_dir.parent / "test"
        test_dir.mkdir(exist_ok=True)

        test_index = {}
        for dataset_name in ["gsm8k", "math", "squad2", "hotpotqa", "humaneval", "mbpp"]:
            dataset_dir = self.processed_dir / dataset_name
            test_file = dataset_dir / "test.jsonl"

            if test_file.exists():
                output_file = test_dir / f"{dataset_name}_test.jsonl"
                with open(test_file) as src, open(output_file, "w") as dst:
                    for line in src:
                        dst.write(line)

                with open(test_file) as f:
                    count = sum(1 for _ in f)

                # å¤„ç†è·¯å¾„ï¼šå…ˆå°è¯•ç›¸å¯¹è·¯å¾„ï¼Œå¤±è´¥åˆ™ä½¿ç”¨ç»å¯¹è·¯å¾„
                try:
                    rel_path = output_file.relative_to(Path.cwd())
                    test_index[dataset_name] = str(rel_path)
                except ValueError:
                    # å¦‚æœä¸åœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹ï¼Œç›´æ¥ä½¿ç”¨è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„å­—ç¬¦ä¸²
                    test_index[dataset_name] = str(output_file.relative_to(test_dir.parent))

                print(f"  âœ… {dataset_name}_test.jsonl: {count} æ ·æœ¬")

        with open(test_dir / "test_index.json", "w") as f:
            json.dump(test_index, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… æ‰€æœ‰testé›†å·²å‡†å¤‡å¥½ï¼Œä½ç½®: {test_dir}/")

    def _save_jsonl(self, filepath: Path, samples: List[Dict]):
        """ä¿å­˜ä¸ºJSONL"""
        with open(filepath, "w") as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + "\n")

    def run_all(self):
        """è¿è¡Œæ‰€æœ‰å¤„ç†"""
        print("\n" + "=" * 80)
        print("å¼€å§‹å¤„ç†6ä¸ªæ•°æ®é›†ï¼ˆ5:1åˆ†å‰²ï¼‰")
        print("=" * 80)

        total_counts = {}
        total_counts["gsm8k"] = self.process_gsm8k()
        total_counts["math"] = self.process_math()
        total_counts["squad2"] = self.process_squad2()
        total_counts["hotpotqa"] = self.process_hotpotqa()
        total_counts["humaneval"] = self.process_humaneval()
        total_counts["mbpp"] = self.process_mbpp()

        self.create_mixed_dataset()
        self.create_test_sets()

        # ä¿å­˜æ€»ç´¢å¼•
        index = {
            "datasets": total_counts,
            "processed_dir": str(self.processed_dir),
            "mixed_dir": str(self.processed_dir.parent / "mixed"),
            "test_dir": str(self.processed_dir.parent / "test"),
            "total_samples": sum(total_counts.values()),
            "split_ratio": "5:1 (train:test)"
        }
        with open(self.processed_dir.parent / "index.json", "w") as f:
            json.dump(index, f, indent=2, ensure_ascii=False)

        print("\n" + "=" * 80)
        print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        print("=" * 80)
        print(f"\nğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
        print(f"  data/processed/          - å„æ•°æ®é›†åˆ†åˆ«å¤„ç†åï¼ˆtrain/testï¼‰")
        print(f"  data/mixed/              - æ··åˆåçš„train_mixed/test_mixed")
        print(f"  data/test/               - å„æ•°æ®é›†çš„testæ–‡ä»¶ï¼ˆç”¨äºå•ç‹¬è¯„ä¼°ï¼‰")

if __name__ == "__main__":
    processor = DatasetProcessor()
    processor.run_all()
