#!/usr/bin/env python3
"""
Phase 2: Data Schema Validation and Standardization
éªŒè¯æ‰€æœ‰æ•°æ®æ ·æœ¬ç¬¦åˆæ ‡å‡†schemaï¼Œæ£€æŸ¥å­—æ®µå®Œæ•´æ€§å’Œæ ¼å¼ä¸€è‡´æ€§
"""
import json
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

def validate_schema():
    """éªŒè¯æ•°æ®schemaå’Œå­—æ®µå®Œæ•´æ€§"""

    data_dir = Path("data/mixed")
    train_file = data_dir / "train_mixed.jsonl"
    test_file = data_dir / "test_mixed.jsonl"

    # å¿…éœ€å­—æ®µï¼ˆåŸºäºprocess_datasetsè¾“å‡ºæ ¼å¼ï¼‰
    REQUIRED_FIELDS = {
        "math": ["id", "dataset", "question", "reference_answer", "domain", "answer_type"],
        "code": ["id", "dataset", "question", "reference_answer", "domain", "answer_type", "entry_point", "test"],
        "qa": ["id", "dataset", "question", "reference_answer", "domain", "answer_type"],
    }

    def validate_file(filepath: Path, split_name: str) -> Dict:
        """éªŒè¯å•ä¸ªæ–‡ä»¶"""
        result = {
            "total_samples": 0,
            "valid_samples": 0,
            "invalid_samples": 0,
            "domain_distribution": defaultdict(int),
            "missing_fields": [],
            "type_mismatches": [],
            "format_issues": [],
            "samples_by_domain": defaultdict(list),
        }

        if not filepath.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return result

        print(f"\nğŸ“‹ éªŒè¯ {split_name}: {filepath}")

        with open(filepath, 'r') as f:
            for line_no, line in enumerate(f, 1):
                if not line.strip():
                    continue

                try:
                    sample = json.loads(line)
                    result["total_samples"] += 1

                    # æ£€æŸ¥domainå­—æ®µ
                    domain = sample.get("domain", "unknown")
                    if domain not in REQUIRED_FIELDS:
                        result["format_issues"].append(
                            f"Line {line_no}: Unknown domain '{domain}' (must be math/code/qa)"
                        )
                        continue

                    result["domain_distribution"][domain] += 1

                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                    required = REQUIRED_FIELDS[domain]
                    missing = [field for field in required if field not in sample or not sample.get(field)]

                    if missing:
                        result["missing_fields"].append({
                            "line": line_no,
                            "id": sample.get("id", "UNKNOWN"),
                            "domain": domain,
                            "missing": missing
                        })
                        continue

                    # æ£€æŸ¥å­—ï¿½ï¿½ï¿½ç±»å‹
                    type_checks = {
                        "id": str,
                        "dataset": str,
                        "question": str,
                        "reference_answer": str,
                        "domain": str,
                        "answer_type": str,
                    }

                    for field, expected_type in type_checks.items():
                        if field in sample and not isinstance(sample[field], expected_type):
                            result["type_mismatches"].append({
                                "line": line_no,
                                "field": field,
                                "expected": expected_type.__name__,
                                "actual": type(sample[field]).__name__
                            })

                    # å¯¹äºcodeç±»å‹ï¼Œæ£€æŸ¥entry_pointå’Œtestçš„æ ¼å¼
                    if domain == "code":
                        if "entry_point" in sample:
                            if not isinstance(sample["entry_point"], str) or not sample["entry_point"].strip():
                                result["format_issues"].append(
                                    f"Line {line_no}: Invalid entry_point format (should be non-empty string)"
                                )
                                continue
                        if "test" in sample:
                            if not isinstance(sample["test"], str) or not sample["test"].strip():
                                result["format_issues"].append(
                                    f"Line {line_no}: Invalid test format (should be non-empty string)"
                                )
                                continue

                    # æ£€æŸ¥answer_typeæ˜¯å¦æœ‰æ•ˆ
                    valid_answer_types = {
                        "math": ["numeric", "text"],
                        "code": ["code"],
                        "qa": ["text"],
                    }
                    answer_type = sample.get("answer_type", "")
                    if answer_type not in valid_answer_types.get(domain, []):
                        result["format_issues"].append(
                            f"Line {line_no}: Invalid answer_type '{answer_type}' for domain {domain}"
                        )
                        continue

                    # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
                    result["valid_samples"] += 1
                    result["samples_by_domain"][domain].append(sample["id"])

                except json.JSONDecodeError as e:
                    result["format_issues"].append(f"Line {line_no}: JSON decode error - {str(e)}")
                except Exception as e:
                    result["format_issues"].append(f"Line {line_no}: Unexpected error - {str(e)}")

        result["invalid_samples"] = result["total_samples"] - result["valid_samples"]

        return result

    # éªŒè¯è®­ç»ƒé›†
    train_result = validate_file(train_file, "TRAIN")

    # éªŒè¯æµ‹è¯•é›†
    test_result = validate_file(test_file, "TEST")

    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š SCHEMA VALIDATION REPORT")
    print("="*70)

    print("\nã€TRAIN SETã€‘")
    print(f"  æ€»æ ·æœ¬æ•°: {train_result['total_samples']}")
    print(f"  âœ… æœ‰æ•ˆæ ·æœ¬: {train_result['valid_samples']}")
    print(f"  âŒ æ— æ•ˆæ ·æœ¬: {train_result['invalid_samples']}")
    print(f"  æœ‰æ•ˆç‡: {100*train_result['valid_samples']/max(train_result['total_samples'], 1):.1f}%")

    print("\n  åŸŸåˆ†å¸ƒ:")
    total_train = sum(train_result['domain_distribution'].values())
    for domain in ["math", "code", "qa"]:
        count = train_result['domain_distribution'].get(domain, 0)
        ratio = 100 * count / total_train if total_train > 0 else 0
        print(f"    - {domain:6s}: {count:5d} ({ratio:5.1f}%)")

    if train_result['missing_fields']:
        print(f"\n  âš ï¸  ç¼ºå¤±å­—æ®µé—®é¢˜ ({len(train_result['missing_fields'])}):")
        for issue in train_result['missing_fields'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"    Line {issue['line']}: {issue['missing']} (ID: {issue['id']})")
        if len(train_result['missing_fields']) > 5:
            print(f"    ... è¿˜æœ‰ {len(train_result['missing_fields']) - 5} ä¸ªé—®é¢˜")

    if train_result['type_mismatches']:
        print(f"\n  âš ï¸  å­—æ®µç±»å‹é”™è¯¯ ({len(train_result['type_mismatches'])}):")
        for issue in train_result['type_mismatches'][:3]:
            print(f"    Line {issue['line']}: {issue['field']} expected {issue['expected']}, got {issue['actual']}")
        if len(train_result['type_mismatches']) > 3:
            print(f"    ... è¿˜æœ‰ {len(train_result['type_mismatches']) - 3} ä¸ªé—®é¢˜")

    if train_result['format_issues']:
        print(f"\n  âš ï¸  æ ¼å¼é—®é¢˜ ({len(train_result['format_issues'])}):")
        for issue in train_result['format_issues'][:5]:
            print(f"    {issue}")
        if len(train_result['format_issues']) > 5:
            print(f"    ... è¿˜æœ‰ {len(train_result['format_issues']) - 5} ä¸ªé—®é¢˜")

    print("\nã€TEST SETã€‘")
    print(f"  æ€»æ ·æœ¬æ•°: {test_result['total_samples']}")
    print(f"  âœ… æœ‰æ•ˆæ ·æœ¬: {test_result['valid_samples']}")
    print(f"  âŒ æ— æ•ˆæ ·æœ¬: {test_result['invalid_samples']}")
    print(f"  æœ‰æ•ˆç‡: {100*test_result['valid_samples']/max(test_result['total_samples'], 1):.1f}%")

    print("\n  åŸŸåˆ†å¸ƒ:")
    total_test = sum(test_result['domain_distribution'].values())
    for domain in ["math", "code", "qa"]:
        count = test_result['domain_distribution'].get(domain, 0)
        ratio = 100 * count / total_test if total_test > 0 else 0
        print(f"    - {domain:6s}: {count:5d} ({ratio:5.1f}%)")

    # æ€»ç»“
    print("\n" + "="*70)
    total_valid = train_result['valid_samples'] + test_result['valid_samples']
    total_all = train_result['total_samples'] + test_result['total_samples']

    if train_result['valid_samples'] == train_result['total_samples'] and test_result['valid_samples'] == test_result['total_samples']:
        print("âœ… å…¨éƒ¨æ ·æœ¬é€šè¿‡schemaéªŒè¯ï¼")
    else:
        print(f"âš ï¸  éªŒè¯å®Œæˆ: {total_valid}/{total_all} æ ·æœ¬æœ‰æ•ˆ ({100*total_valid/max(total_all, 1):.1f}%)")

    # æ£€æŸ¥è®­ç»ƒæ¯”ä¾‹
    train_total = sum(train_result['domain_distribution'].values())
    train_ratios = {
        domain: train_result['domain_distribution'].get(domain, 0) / train_total
        for domain in ["math", "code", "qa"]
    }

    print(f"\nã€æ•°æ®å‡è¡¡æ£€æŸ¥ã€‘")
    print(f"  é…ç½®æ¯”ä¾‹: math=0.4, code=0.3, qa=0.3")
    print(f"  å®é™…æ¯”ä¾‹: math={train_ratios['math']:.2%}, code={train_ratios['code']:.2%}, qa={train_ratios['qa']:.2%}")

    # è¿”å›éªŒè¯ç»“æœ
    return {
        "train": train_result,
        "test": test_result,
        "overall_valid": total_valid == total_all,
        "total_valid_samples": total_valid,
        "total_samples": total_all,
    }

if __name__ == "__main__":
    result = validate_schema()
    print("\nâœ… Schema validation complete!")
