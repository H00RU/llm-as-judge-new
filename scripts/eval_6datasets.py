#!/usr/bin/env python3
"""
è¯„ä¼°è„šæœ¬ - åœ¨6ä¸ªæ•°æ®é›†ä¸Šåˆ†åˆ«æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹

âœ… ä¿®å¤åçš„è¯„ä¼°æµç¨‹ï¼ˆä¸è®­ç»ƒæµç¨‹ä¸€è‡´ï¼‰ï¼š
é—®é¢˜ â†’ Qwenç”Ÿæˆworkflowä»£ç  â†’ AFlowæ‰§è¡Œworkflow â†’ gpt-4o-miniè¿è¡Œç®—å­ â†’ ç­”æ¡ˆ â†’ å‡†ç¡®æ€§è¯„ä¼°
      (RLç­–ç•¥æ¨¡å‹)           (å·¥ä½œæµå¼•æ“)       (æ‰§è¡Œå¼•æ“)      (ç²¾ç¡®åŒ¹é…/LLM Judge)

âŒ æ—§ç‰ˆæœ¬é”™è¯¯æµç¨‹ï¼ˆå·²åºŸå¼ƒï¼‰ï¼š
é—®é¢˜ â†’ Qwenç›´æ¥ç”Ÿæˆç­”æ¡ˆ â†’ ç®€å•å­—ç¬¦ä¸²åŒ¹é… â†’ "å‡†ç¡®ç‡"
"""

import os
import sys
import json
import argparse
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import numpy as np
from tqdm import tqdm
import yaml

sys.path.insert(0, 'src')

from rl_workflow_generator import RLWorkflowGenerator
from aflow_executor import AFlowExecutor
from reward_computer import RewardComputer

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨ - ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„workflowç”Ÿæˆâ†’æ‰§è¡Œæµç¨‹"""

    def __init__(self,
                 config_path: str = "config/training.yaml",
                 checkpoint_path: Optional[str] = None,
                 device: str = "cuda:0"):
        """
        Args:
            config_path: è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
            checkpoint_path: LoRAæƒé‡è·¯å¾„ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨base model
            device: ä½¿ç”¨çš„è®¾å¤‡
        """
        self.checkpoint_path = checkpoint_path
        self.device = device

        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        # åˆå§‹åŒ–workflowç”Ÿæˆå™¨
        print(f"\nğŸ“¦ åˆå§‹åŒ–è¯„ä¼°å™¨...")
        self.workflow_generator = RLWorkflowGenerator(
            model_name_or_path=checkpoint_path if checkpoint_path else self.config['base_model'],
            config=self.config,
            device=device
        )
        print("  âœ… Workflowç”Ÿæˆå™¨åŠ è½½å®Œæˆ")

        # åˆå§‹åŒ–AFlowæ‰§è¡Œå™¨
        self.executor = AFlowExecutor(
            aflow_config_path=self.config['aflow_config_path'],
            operator_descriptions_path=self.config['aflow_operator_descriptions_path']
        )
        print("  âœ… AFlowæ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨ï¼ˆç”¨äºè¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§ï¼‰
        self.reward_computer = RewardComputer(
            config=self.config,
            aflow_config_path=self.config['aflow_config_path']
        )
        print("  âœ… å¥–åŠ±è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")

    async def generate_and_execute_workflow(self,
                                            problem: str,
                                            problem_type: str,
                                            entry_point: str = '',
                                            test: str = '') -> Dict[str, Any]:
        """
        ç”Ÿæˆworkflowå¹¶æ‰§è¡Œï¼ˆä¸è®­ç»ƒæµç¨‹ä¸€è‡´ï¼‰

        Returns:
            DictåŒ…å«:
            - answer: æœ€ç»ˆç­”æ¡ˆ
            - workflow_code: ç”Ÿæˆçš„workflowä»£ç 
            - success: æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
            - metadata: æ‰§è¡Œå…ƒæ•°æ®
        """
        # 1. ç”Ÿæˆworkflowä»£ç 
        workflow_code = self.workflow_generator.generate_workflow(
            problem=problem,
            problem_type=problem_type
        )

        # 2. æ‰§è¡Œworkflow
        try:
            answer, cost, metadata = await self.executor.execute_workflow(
                workflow_code=workflow_code,
                problem=problem,
                problem_type=problem_type,
                entry_point=entry_point,
                test=test
            )

            return {
                'answer': answer,
                'workflow_code': workflow_code,
                'success': metadata.get('success', False),
                'metadata': metadata
            }
        except Exception as e:
            return {
                'answer': None,
                'workflow_code': workflow_code,
                'success': False,
                'metadata': {'error': str(e), 'success': False}
            }

    async def evaluate_dataset(self, dataset_name: str, test_file: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ•°æ®é›†"""
        print(f"\nğŸ§ª è¯„ä¼° {dataset_name}...")

        if not Path(test_file).exists():
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return {}

        # æ˜ å°„æ•°æ®é›†åˆ°é—®é¢˜ç±»å‹
        dataset_to_type = {
            "gsm8k": "math",
            "math": "math",
            "squad2": "qa",
            "hotpotqa": "qa",
            "humaneval": "code",
            "mbpp": "code"
        }
        problem_type = dataset_to_type.get(dataset_name, "qa")

        results = {
            "dataset": dataset_name,
            "problem_type": problem_type,
            "total": 0,
            "correct": 0,
            "execution_success": 0,
            "predictions": [],
            "metrics": {}
        }

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_samples = []
        with open(test_file) as f:
            for line in f:
                if line.strip():
                    try:
                        sample = json.loads(line)
                        test_samples.append(sample)
                    except json.JSONDecodeError:
                        continue

        results["total"] = len(test_samples)
        print(f"  æ€»æ ·æœ¬æ•°: {results['total']}")

        # é€ä¸ªè¯„ä¼°ï¼ˆé™åˆ¶å‰100ä¸ªç”¨äºå¿«é€Ÿè¯„ä¼°ï¼‰
        correct_count = 0
        success_count = 0

        for idx, sample in enumerate(tqdm(test_samples[:100], desc=f"Evaluating {dataset_name}")):
            question = sample.get("question", "")
            reference_answer = sample.get("reference_answer", "")
            entry_point = sample.get('entry_point', '')
            test = sample.get('test', '')

            if not question:
                continue

            # ç”Ÿæˆworkflowå¹¶æ‰§è¡Œ
            try:
                result = await self.generate_and_execute_workflow(
                    problem=question,
                    problem_type=problem_type,
                    entry_point=entry_point,
                    test=test
                )

                answer = result['answer']
                success = result['success']
                metadata = result['metadata']

                if success:
                    success_count += 1

                # ä½¿ç”¨reward_computerè¯„ä¼°æ­£ç¡®æ€§
                is_correct = self._evaluate_correctness(
                    prediction=answer,
                    reference=reference_answer,
                    problem_type=problem_type,
                    metadata=metadata,
                    problem=question
                )

                if is_correct:
                    correct_count += 1

                results["predictions"].append({
                    "question": question[:100],
                    "reference": str(reference_answer)[:100],
                    "prediction": str(answer)[:100],
                    "correct": is_correct,
                    "execution_success": success
                })

            except Exception as e:
                print(f"  è¯„ä¼°å¤±è´¥: {e}")
                results["predictions"].append({
                    "question": question[:100],
                    "reference": str(reference_answer)[:100],
                    "prediction": "ERROR",
                    "correct": False,
                    "execution_success": False,
                    "error": str(e)
                })

        # è®¡ç®—æŒ‡æ ‡
        sample_count = min(100, results["total"])
        accuracy = correct_count / sample_count if sample_count > 0 else 0
        execution_rate = success_count / sample_count if sample_count > 0 else 0

        results["metrics"]["accuracy"] = accuracy
        results["metrics"]["execution_success_rate"] = execution_rate
        results["correct"] = correct_count
        results["execution_success"] = success_count

        print(f"  âœ… å‡†ç¡®ç‡: {accuracy:.2%} | æ‰§è¡ŒæˆåŠŸç‡: {execution_rate:.2%}")

        return results

    def _evaluate_correctness(self,
                              prediction: Any,
                              reference: str,
                              problem_type: str,
                              metadata: Dict,
                              problem: str = '') -> bool:
        """
        è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§ - ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„LLM Judgeè¯„ä¼°

        Returns:
            bool: æ˜¯å¦æ­£ç¡®
        """
        # å¦‚æœæ‰§è¡Œå¤±è´¥ï¼Œç›´æ¥è¿”å›False
        if not metadata.get('success', False):
            return False

        # å¦‚æœç­”æ¡ˆä¸ºNoneæˆ–ç©ºï¼Œè¿”å›False
        if prediction is None or str(prediction).strip() == '':
            return False

        # ä½¿ç”¨reward_computerçš„LLM Judgeè¯„ä¼°ï¼ˆå…¬å…±æ¥å£ï¼‰
        try:
            is_correct = self.reward_computer.llm_judge_compare(
                problem=problem,
                prediction=str(prediction),
                ground_truth=str(reference),
                problem_type=problem_type
            )
            return is_correct
        except Exception as e:
            print(f"    âš ï¸ LLM Judgeè¯„ä¼°å¤±è´¥: {e}")
            return False

    async def evaluate_all(self, test_dir: str = "data/test") -> Dict[str, Any]:
        """è¯„ä¼°æ‰€æœ‰6ä¸ªæ•°æ®é›†"""
        print("\n" + "=" * 60)
        print(f"å¼€å§‹è¯„ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨workflowç”Ÿæˆâ†’æ‰§è¡Œæµç¨‹ï¼‰")
        print("=" * 60)

        test_dir = Path(test_dir)
        datasets = ["gsm8k", "math", "squad2", "hotpotqa", "humaneval", "mbpp"]

        all_results = {
            "checkpoint": self.checkpoint_path,
            "datasets": {}
        }

        for dataset_name in datasets:
            test_file = test_dir / f"{dataset_name}_test.jsonl"
            if test_file.exists():
                result = await self.evaluate_dataset(dataset_name, str(test_file))
                all_results["datasets"][dataset_name] = result
            else:
                print(f"\nâš ï¸  {dataset_name} æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        accuracies = []
        execution_rates = []
        for result in all_results["datasets"].values():
            if "accuracy" in result.get("metrics", {}):
                accuracies.append(result["metrics"]["accuracy"])
            if "execution_success_rate" in result.get("metrics", {}):
                execution_rates.append(result["metrics"]["execution_success_rate"])

        if accuracies:
            all_results["overall_accuracy"] = np.mean(accuracies)
        if execution_rates:
            all_results["overall_execution_success_rate"] = np.mean(execution_rates)

        return all_results


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹")
    parser.add_argument("--config", default="config/training.yaml",
                       help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default=None,
                       help="LoRAæƒé‡è·¯å¾„ï¼Œå¦‚ checkpoints/qwen25-7b/grpo_mixed/step_1000")
    parser.add_argument("--test_dir", default="data/test",
                       help="æµ‹è¯•æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", default="results/evaluation",
                       help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--device", default="cuda:0",
                       help="ä½¿ç”¨çš„è®¾å¤‡")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(
        config_path=args.config,
        checkpoint_path=args.checkpoint,
        device=args.device
    )

    # è¯„ä¼°æ‰€æœ‰æ•°æ®é›†ï¼ˆä½¿ç”¨asyncï¼‰
    print("\nğŸš€ å¼€å§‹è¯„ä¼°ï¼ˆworkflowç”Ÿæˆâ†’AFlowæ‰§è¡Œæµç¨‹ï¼‰...")
    results = asyncio.run(evaluator.evaluate_all(test_dir=args.test_dir))

    # ä¿å­˜ç»“æœ
    checkpoint_name = Path(args.checkpoint).name if args.checkpoint else "base_model"
    output_file = output_dir / f"{checkpoint_name}_results.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # æ‰“å°æ€»ä½“ç»“æœ
    print("\n" + "=" * 60)
    print("è¯„ä¼°ç»“æœæ±‡æ€»")
    print("=" * 60)

    for dataset_name, result in results["datasets"].items():
        if result:
            metrics = result.get("metrics", {})
            print(f"\n{dataset_name}:")
            for metric_name, value in metrics.items():
                if isinstance(value, float):
                    print(f"  {metric_name}: {value:.4f}")
                else:
                    print(f"  {metric_name}: {value}")

    if "overall_accuracy" in results:
        print(f"\næ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']:.4f}")
    if "overall_execution_success_rate" in results:
        print(f"æ€»ä½“æ‰§è¡ŒæˆåŠŸç‡: {results['overall_execution_success_rate']:.4f}")

if __name__ == "__main__":
    main()
