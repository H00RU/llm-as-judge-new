#!/usr/bin/env python3
"""
è¯„ä¼°è„šæœ¬ - åœ¨6ä¸ªæ•°æ®é›†ä¸Šåˆ†åˆ«æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import numpy as np
from tqdm import tqdm

sys.path.insert(0, 'src')

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self,
                 model_name: str = "qwen25-7b",
                 checkpoint_path: str = None,
                 device: str = "cuda:0"):
        """
        Args:
            model_name: æ¨¡å‹åç§° (qwen25-7b æˆ– qwen3-8b)
            checkpoint_path: LoRAæƒé‡è·¯å¾„ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨base model
            device: ä½¿ç”¨çš„è®¾å¤‡
        """
        self.model_name = model_name
        self.checkpoint_path = checkpoint_path
        self.device = device

        # æ¨¡å‹é…ç½®
        self.model_configs = {
            "qwen25-7b": {
                "base_model": "Qwen/Qwen2.5-7B-Instruct",
                "local_path": "./models/Qwen2.5-7B-Instruct"
            },
            "qwen3-8b": {
                "base_model": "Qwen/Qwen-3-8B",
                "local_path": "./models/Qwen-3-8B"
            }
        }

        self.model = None
        self.tokenizer = None
        self._load_model()

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        if self.model_name not in self.model_configs:
            raise ValueError(f"Unknown model: {self.model_name}")

        config = self.model_configs[self.model_name]
        model_id = config["base_model"]

        print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_name}")

        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
        if Path(config["local_path"]).exists():
            print(f"  ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {config['local_path']}")
            model_id = config["local_path"]

        # åŠ è½½base model
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

        # å¦‚æœæœ‰checkpointï¼ŒåŠ è½½LoRAæƒé‡
        if self.checkpoint_path and Path(self.checkpoint_path).exists():
            print(f"  åŠ è½½LoRAæƒé‡: {self.checkpoint_path}")
            self.model = PeftModel.from_pretrained(self.model, self.checkpoint_path)
            self.model = self.model.merge_and_unload()
        else:
            print(f"  ä½¿ç”¨base modelï¼ˆæœªå¾®è°ƒï¼‰")

        self.model.eval()
        print("  âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        """ç”Ÿæˆå›åº”"""
        inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=max_tokens,
                temperature=0.3,
                top_p=0.95,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
        return response.strip()

    def evaluate_dataset(self, dataset_name: str, test_file: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ•°æ®é›†"""
        print(f"\nğŸ§ª è¯„ä¼° {dataset_name}...")

        if not Path(test_file).exists():
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return {}

        results = {
            "dataset": dataset_name,
            "total": 0,
            "correct": 0,
            "predictions": [],
            "metrics": {}
        }

        # æ ¹æ®æ•°æ®é›†ç±»å‹ç¡®å®šè¯„ä¼°æŒ‡æ ‡
        metrics_config = {
            "gsm8k": ["accuracy"],
            "math": ["accuracy"],
            "squad2": ["exact_match", "f1"],
            "hotpotqa": ["exact_match", "f1"],
            "humaneval": ["pass@1"],
            "mbpp": ["pass@1"]
        }

        results["metrics_to_compute"] = metrics_config.get(dataset_name, ["accuracy"])

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_samples = []
        with open(test_file) as f:
            for line in f:
                if line.strip():
                    try:
                        sample = json.loads(line)
                        test_samples.append(sample)
                    except json.JSONDecodeError:
                        continue

        results["total"] = len(test_samples)
        print(f"  æ€»æ ·æœ¬æ•°: {results['total']}")

        # é€ä¸ªè¯„ä¼°
        correct_count = 0

        for idx, sample in enumerate(tqdm(test_samples[:100], desc=f"Evaluating {dataset_name}")):  # å¿«é€Ÿè¯„ä¼°ï¼Œåªç”¨å‰100ä¸ª
            question = sample.get("question", "")
            reference_answer = sample.get("reference_answer", "")

            if not question:
                continue

            # ç”Ÿæˆå›åº”
            try:
                prediction = self.generate(question, max_tokens=512)
            except Exception as e:
                print(f"  ç”Ÿæˆå¤±è´¥: {e}")
                prediction = ""

            # ç®€å•çš„å‡†ç¡®æ€§è¯„ä¼°ï¼ˆåŸºäºæ˜¯å¦åŒ…å«ç­”æ¡ˆçš„å…³é”®è¯ï¼‰
            is_correct = self._check_correctness(dataset_name, prediction, reference_answer)

            if is_correct:
                correct_count += 1

            results["predictions"].append({
                "question": question[:100],
                "reference": reference_answer[:100],
                "prediction": prediction[:100],
                "correct": is_correct
            })

        # è®¡ç®—æŒ‡æ ‡
        accuracy = correct_count / min(100, results["total"]) if results["total"] > 0 else 0
        results["metrics"]["accuracy"] = accuracy
        results["correct"] = correct_count

        print(f"  âœ… å‡†ç¡®ç‡: {accuracy:.2%}")

        return results

    def _check_correctness(self, dataset_name: str, prediction: str, reference: str) -> bool:
        """æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®ï¼ˆç®€å•å¯å‘å¼æ–¹æ³•ï¼‰"""
        prediction = prediction.lower().strip()
        reference = reference.lower().strip()

        if dataset_name in ["humaneval", "mbpp"]:
            # ä»£ç ä»»åŠ¡ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„å‡½æ•°å®šä¹‰æˆ–returnè¯­å¥
            return "def " in prediction or "return" in prediction

        elif dataset_name in ["squad2", "hotpotqa"]:
            # QAä»»åŠ¡ï¼šç®€å•çš„è¯æ±‡é‡å 
            pred_words = set(prediction.split())
            ref_words = set(reference.split())
            overlap = len(pred_words & ref_words)
            return overlap >= min(3, len(ref_words))

        else:  # math datasets
            # æ•°å­¦ä»»åŠ¡ï¼šæ£€æŸ¥æ•°å­—ç­”æ¡ˆ
            import re
            pred_nums = re.findall(r'-?\d+\.?\d*', prediction)
            ref_nums = re.findall(r'-?\d+\.?\d*', reference)

            if pred_nums and ref_nums:
                try:
                    return float(pred_nums[-1]) == float(ref_nums[-1])
                except ValueError:
                    return pred_nums[-1] == ref_nums[-1]

            return False

    def evaluate_all(self, test_dir: str = "data/test") -> Dict[str, Any]:
        """è¯„ä¼°æ‰€æœ‰6ä¸ªæ•°æ®é›†"""
        print("\n" + "=" * 60)
        print(f"å¼€å§‹è¯„ä¼° {self.model_name} æ¨¡å‹")
        print("=" * 60)

        test_dir = Path(test_dir)
        datasets = ["gsm8k", "math", "squad2", "hotpotqa", "humaneval", "mbpp"]

        all_results = {
            "model": self.model_name,
            "checkpoint": self.checkpoint_path,
            "datasets": {}
        }

        for dataset_name in datasets:
            test_file = test_dir / f"{dataset_name}_test.jsonl"
            if test_file.exists():
                result = self.evaluate_dataset(dataset_name, str(test_file))
                all_results["datasets"][dataset_name] = result
            else:
                print(f"\nâš ï¸  {dataset_name} æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        accuracies = []
        for result in all_results["datasets"].values():
            if "accuracy" in result.get("metrics", {}):
                accuracies.append(result["metrics"]["accuracy"])

        if accuracies:
            all_results["overall_accuracy"] = np.mean(accuracies)

        return all_results


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹")
    parser.add_argument("--model", default="qwen25-7b",
                       choices=["qwen25-7b", "qwen3-8b"],
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--checkpoint", default=None,
                       help="LoRAæƒé‡è·¯å¾„ï¼Œå¦‚ checkpoints/qwen25-7b/grpo_mixed/step_1000")
    parser.add_argument("--test_dir", default="data/test",
                       help="æµ‹è¯•æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", default="results/evaluation",
                       help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--device", default="cuda:0",
                       help="ä½¿ç”¨çš„è®¾å¤‡")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(
        model_name=args.model,
        checkpoint_path=args.checkpoint,
        device=args.device
    )

    # è¯„ä¼°æ‰€æœ‰æ•°æ®é›†
    results = evaluator.evaluate_all(test_dir=args.test_dir)

    # ä¿å­˜ç»“æœ
    output_file = output_dir / f"{args.model}_results.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # æ‰“å°æ€»ä½“ç»“æœ
    print("\n" + "=" * 60)
    print("è¯„ä¼°ç»“æœæ±‡æ€»")
    print("=" * 60)

    for dataset_name, result in results["datasets"].items():
        if result:
            metrics = result.get("metrics", {})
            print(f"\n{dataset_name}:")
            for metric_name, value in metrics.items():
                if isinstance(value, float):
                    print(f"  {metric_name}: {value:.4f}")
                else:
                    print(f"  {metric_name}: {value}")

    if "overall_accuracy" in results:
        print(f"\næ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']:.4f}")

if __name__ == "__main__":
    main()
