#!/usr/bin/env python3
"""
å¥–åŠ±è®¡ç®—å™¨ - æ”¹è¿›ç‰ˆ(å€Ÿé‰´ROLLå’ŒAgentFlowè®¾è®¡)
"""
import sys
import os
import re
from typing import Any, Dict, Optional

# æ·»åŠ AFlowåˆ°è·¯å¾„
sys.path.insert(0, os.getenv("AFLOW_PATH", "./AFlow"))

# å¯¼å…¥ç­”æ¡ˆæå–å™¨
try:
    from .answer_extractor import AnswerExtractor
except ImportError:
    from answer_extractor import AnswerExtractor


class RewardComputer:
    """
    æ”¹è¿›çš„å¥–åŠ±è®¡ç®—å™¨

    æ–°å¢ç‰¹æ€§(å€Ÿé‰´ROLL):
    1. æ ¼å¼å¥–åŠ± - æ£€æŸ¥<think>/<answer>æ ‡ç­¾
    2. é‡å¤æƒ©ç½š - N-gramé‡å¤æ£€æµ‹
    3. æ”¹è¿›çš„æ•°å­¦è¯„ä¼° - æ”¯æŒLaTeXå’Œboxed
    4. æ›´ç»†ç²’åº¦çš„è¯„åˆ†é˜¶æ¢¯
    5. LLM Judge - ä½¿ç”¨GPT OSS 120Bè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ(AgentFlowæ–¹æ³•)
    """

    def __init__(
        self,
        reward_weights: Optional[Dict[str, float]] = None,
        use_answer_extractor: bool = True,  # æ˜¯å¦ä½¿ç”¨ç­”æ¡ˆæå–å™¨
        use_llm_judge: bool = False,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨LLM Judge
        llm_config: Optional[Dict] = None  # æ–°å¢ï¼šLLMé…ç½®
    ):
        """
        Args:
            reward_weights: å¥–åŠ±æƒé‡é…ç½®ï¼ˆä»…ç”¨äºå‘åå…¼å®¹ï¼Œå®é™…ä½¿ç”¨äºŒå…ƒå¥–åŠ±ï¼‰
            use_answer_extractor: æ˜¯å¦ä½¿ç”¨ç­”æ¡ˆæå–å™¨æ¥æ ‡å‡†åŒ–ç­”æ¡ˆ
            use_llm_judge: æ˜¯å¦ä½¿ç”¨LLM Judgeè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ
            llm_config: LLMé…ç½®ï¼ˆç”¨äºLLM Judgeï¼‰
        """
        # ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œä½†ä¸å†ä½¿ç”¨
        self.reward_weights = reward_weights or {
            "correctness": 1.0
        }

        # åˆå§‹åŒ–ç­”æ¡ˆæå–å™¨
        self.use_answer_extractor = use_answer_extractor
        if use_answer_extractor:
            # ç¦ç”¨LLM fallbackä»¥é¿å…é¢å¤–æˆæœ¬ï¼ˆè§„åˆ™æå–å·²è¶³å¤Ÿå‡†ç¡®ï¼‰
            self.extractor = AnswerExtractor(use_llm_fallback=False)
        else:
            self.extractor = None

        # åˆå§‹åŒ–LLM Judge
        self.use_llm_judge = use_llm_judge
        self.llm_judge_client = None
        if use_llm_judge:
            self._init_llm_judge_client(llm_config)

        print(f"âœ… 10åˆ†åˆ¶å¥–åŠ±è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡å¼: æ­£ç¡®æ€§åˆ†æ•° [-10, 10] â†’ å½’ä¸€åŒ–å¥–åŠ± [0, 1]")
        print(f"  ç­”æ¡ˆæå–å™¨: {'å¯ç”¨' if use_answer_extractor else 'ç¦ç”¨'}")
        print(f"  LJM Judge: {'å¯ç”¨ (gpt-4o-mini)' if use_llm_judge else 'ç¦ç”¨'}")

    def _init_llm_judge_client(self, llm_config: Optional[Dict]):
        """åˆå§‹åŒ–LLM Judgeå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨OpenAI gpt-4o-miniï¼‰"""
        try:
            from openai import OpenAI
            import yaml

            # é¦–å…ˆå°è¯•ä»aflow_llm.yamlè¯»å–é…ç½®
            aflow_config_path = "config/aflow_llm.yaml"
            default_config = {
                "base_url": "https://api.openai.com/v1",
                "api_key": os.getenv("OPENAI_API_KEY", "sk-xxx"),  # å¤‡ç”¨ï¼šä»ç¯å¢ƒå˜é‡è¯»å–OpenAI API Key
                "model_name": "gpt-4o-mini"  # ä½¿ç”¨gpt-4o-mini
            }

            # å°è¯•è¯»å–aflowé…ç½®æ–‡ä»¶
            try:
                with open(aflow_config_path, 'r', encoding='utf-8') as f:
                    aflow_config = yaml.safe_load(f)

                # è·å–gpt-4o-minié…ç½®
                models_config = aflow_config.get('models', {})
                if 'gpt-4o-mini' in models_config:
                    gpt4omini_config = models_config['gpt-4o-mini']
                    default_config = {
                        "base_url": gpt4omini_config.get('base_url', default_config["base_url"]),
                        "api_key": gpt4omini_config.get('api_key', default_config["api_key"]),
                        "model_name": gpt4omini_config.get('model_name', default_config["model_name"])
                    }
                    print(f"  âœ… ä»{aflow_config_path}è¯»å–gpt-4o-minié…ç½®")
            except Exception as config_error:
                print(f"  âš ï¸  æ— æ³•è¯»å–{aflow_config_path}: {config_error}")
                print(f"  ğŸ”„ ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤é…ç½®")

            config = llm_config or default_config

            self.llm_judge_client = OpenAI(
                base_url=config.get("base_url", default_config["base_url"]),
                api_key=config.get("api_key", default_config["api_key"])
            )
            self.llm_judge_model = config.get("model_name", default_config["model_name"])

            print(f"  âœ… LLM Judgeå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            print(f"     æ¨¡å‹: {self.llm_judge_model}")
            print(f"     URL: {config.get('base_url', default_config['base_url'])}")
        except Exception as e:
            print(f"  âš ï¸  LLM Judgeå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_llm_judge = False
            self.llm_judge_client = None

    def llm_judge_compare(
        self,
        problem: str,
        prediction: str,
        ground_truth: str,
        problem_type: str
    ) -> bool:
        """
        ä½¿ç”¨LLM Judgeè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒï¼ˆAgentFlowæ–¹æ³•ï¼‰

        Args:
            problem: é—®é¢˜æ–‡æœ¬
            prediction: æ¨¡å‹é¢„æµ‹ï¼ˆå®Œæ•´å“åº”ï¼Œæœªæå–ï¼‰
            ground_truth: Ground truthç­”æ¡ˆ
            problem_type: é—®é¢˜ç±»å‹

        Returns:
            bool: Trueè¡¨ç¤ºç­‰ä»·ï¼ŒFalseè¡¨ç¤ºä¸ç­‰ä»·
        """
        if not self.llm_judge_client:
            print("âš ï¸  LLM Judgeå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œé™çº§ä¸ºè§„åˆ™æ¯”è¾ƒ")
            return False

        # æ„å»ºAgentFlowé£æ ¼çš„promptï¼ˆä¼˜åŒ–ç‰ˆ - æ›´æ˜ç¡®çš„æå–æŒ‡å¯¼ï¼‰
        query_prompt = f"""You are a precise mathematical and logical equivalence evaluator. Your task is to determine if the Model Response contains an answer equivalent to the Ground Truth.

**Step 1: Extract the Final Answer**
From the Model Response, extract ONLY the final answer, ignoring all reasoning steps, explanations, and intermediate calculations.

Look for answers in these formats (in order of priority):
1. Inside `\\boxed{{...}}` LaTeX notation
2. After phrases like "The answer is", "Therefore", "So", "Thus", "Final answer:"
3. In `<answer>...</answer>` tags
4. The last number, expression, or entity mentioned

**Step 2: Extract from Ground Truth**
Similarly extract the final answer from Ground Truth, which may contain:
- Step-by-step solutions (extract only the final result)
- Multiple numbers (take the last/final one)
- Explanatory text (ignore and find the answer)

**Step 3: Normalize Both Answers**
Before comparing, normalize both answers:
- **Numbers:** Convert to same format (0.5 == 1/2 == 50%)
- **Units/Currency:** Ignore ($30 == 30, 10 meters == 10)
- **Formatting:** Ignore spaces, case, punctuation
- **LaTeX:** Interpret mathematical meaning (\\frac{{1}}{{2}} == 0.5)

**Step 4: Compare Equivalence**
Answers are equivalent if:
- **Math:** Numerically/algebraically equal (even if different forms)
- **Text:** Same entity/concept (ignore synonyms, case)
- **Precision:** Allow reasonable rounding (42.0 == 42)

**Examples of CORRECT equivalence:**
- "1/2" == "0.5" âœ“
- "$30" == "30" âœ“
- "\\boxed{{42}}" == "42" âœ“
- "x^2+2x+1" == "(x+1)^2" âœ“ (algebraically equivalent)
- "10 meters" == "10" âœ“

**Examples of INCORRECT equivalence:**
- "John Smith" == "Jane Doe" âœ— (different entities)
- "42" == "43" âœ— (different numbers)
- "Paris" == "London" âœ— (different locations)

**Inputs:**
Question: {problem}
Model Response: {prediction}
Ground Truth: {ground_truth}

**Required Output Format:**
<analysis>Your reasoning in 1-2 sentences</analysis>
<true_false>True or False</true_false>

Be LENIENT with formatting differences but STRICT with factual/numerical differences.
"""

        try:
            # è°ƒç”¨LLM Judgeï¼ˆæœ€å¤šé‡è¯•1æ¬¡ï¼‰
            for attempt in range(2):  # 0=é¦–æ¬¡, 1=é‡è¯•
                response = self.llm_judge_client.chat.completions.create(
                    model=self.llm_judge_model,
                    messages=[
                        {"role": "system", "content": "You are a precise answer equivalence evaluator."},
                        {"role": "user", "content": query_prompt}
                    ],
                    temperature=0.0,
                    max_tokens=200
                )

                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                content = response.choices[0].message.content
                if content is None:
                    if attempt == 0:
                        print(f"âš ï¸  LLM Judgeé¦–æ¬¡è¿”å›ç©ºå†…å®¹ï¼Œé‡è¯•ä¸­...")
                        continue  # é‡è¯•
                    else:
                        print(f"âš ï¸  LLM Judgeé‡è¯•åä»è¿”å›ç©ºå†…å®¹ï¼Œfallbackåˆ¤å®šä¸ºFalse")
                        return False

                # æˆåŠŸè·å–å†…å®¹ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                result_text = content.strip()
                break

            # è§£æ<true_false>æ ‡ç­¾ - å¢å¼ºçš„é²æ£’æ€§åŒ¹é…
            import re
            # åŒ¹é…å¤šç§æ ¼å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰ï¼š
            # 1. <true_false>True</true_false>
            # 2. <true_false>: True
            # 3. **true_false**: True
            # 4. true_false: True
            # 5. ç›´æ¥åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾True/Falseï¼ˆæœ€åæ‰‹æ®µï¼‰

            # å°è¯•1: æ ‡å‡†XMLæ ‡ç­¾
            true_false_match = re.search(
                r'<true_false>\s*(True|False)\s*</true_false>',
                result_text,
                re.IGNORECASE
            )

            # å°è¯•2: å†’å·åˆ†éš”çš„æ ‡ç­¾
            if not true_false_match:
                true_false_match = re.search(
                    r'<true_false>\s*:\s*(True|False)',
                    result_text,
                    re.IGNORECASE
                )

            # å°è¯•3: Markdownç²—ä½“æ ¼å¼
            if not true_false_match:
                true_false_match = re.search(
                    r'\*\*true_false\*\*\s*:?\s*(True|False)',
                    result_text,
                    re.IGNORECASE
                )

            # å°è¯•4: ç®€å•çš„key: valueæ ¼å¼
            if not true_false_match:
                true_false_match = re.search(
                    r'true_false\s*:?\s*(True|False)',
                    result_text,
                    re.IGNORECASE
                )

            # å°è¯•5: æŸ¥æ‰¾ç‹¬ç«‹çš„True/Falseï¼ˆæœ€åæ‰‹æ®µï¼‰
            if not true_false_match:
                # åªåœ¨å“åº”æœ«å°¾æŸ¥æ‰¾ï¼Œé¿å…è¯¯åŒ¹é…åˆ†ææ–‡æœ¬ä¸­çš„True/False
                last_200_chars = result_text[-200:]
                true_false_match = re.search(
                    r'\b(True|False)\b',
                    last_200_chars,
                    re.IGNORECASE
                )

            if true_false_match:
                verdict = true_false_match.group(1).lower() == "true"

                # è°ƒè¯•è¾“å‡ºï¼ˆ20%é‡‡æ ·ï¼‰
                import random
                if random.random() < 0.2:
                    print(f"\nğŸ¤– LLM Judgeç»“æœ ({problem_type}):")
                    print(f"  é—®é¢˜: {problem[:60]}...")
                    print(f"  é¢„æµ‹: {str(prediction)[:60]}...")
                    print(f"  çœŸå€¼: {str(ground_truth)[:60]}...")
                    print(f"  åˆ¤å†³: {verdict}")
                    print(f"  LLMå“åº”: {result_text[:150]}...")

                return verdict
            else:
                # å®Œå…¨æ— æ³•è§£ææ—¶ï¼Œæ‰“å°å®Œæ•´å“åº”ç”¨äºè°ƒè¯•
                print(f"âš ï¸  æ— æ³•è§£æLLM Judgeå“åº”ï¼ˆå°è¯•äº†5ç§æ ¼å¼ï¼‰")
                print(f"  å®Œæ•´å“åº”: {result_text}")
                return False

        except Exception as e:
            print(f"âš ï¸  LLM Judgeè°ƒç”¨å¤±è´¥: {e}")
            return False

    def compute_reward(
        self,
        problem: str,
        prediction: Any,
        ground_truth: Any,
        problem_type: str = "math",
        metadata: Optional[Dict] = None,
        execution_metadata: Optional[Dict] = None  # æ–°å¢ï¼šæ‰§è¡Œå…ƒæ•°æ®ï¼ˆåŒ…æ‹¬ç”Ÿæˆè´¨é‡ï¼‰
    ) -> Dict:
        """
        ç»Ÿä¸€çš„å¥–åŠ±è®¡ç®—æ¡†æ¶ - æ•´åˆæ‰€æœ‰çº¦æŸå’Œè¯„ä¼°

        **ç»Ÿä¸€å½’ä¸€åŒ–**: æ‰€æœ‰å¥–åŠ±å½’ä¸€åŒ–åˆ°[-1.0, 1.0]èŒƒå›´

        Args:
            execution_metadata: åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
                - operator_problem_type_mismatch: Operator-é—®é¢˜ç±»å‹ä¸åŒ¹é…
                - validation_failed: éªŒè¯å¤±è´¥
                - error_type: æ‰§è¡Œé”™è¯¯ç±»å‹
                - success: æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
                - had_signature_error: æ˜¯å¦æœ‰ç­¾åé”™è¯¯
                - auto_fixes_applied: è‡ªåŠ¨ä¿®å¤åˆ—è¡¨
                - needed_fallback: æ˜¯å¦éœ€è¦ Fallback

        Returns:
            {
                'total': float,           # æ€»å¥–åŠ± (å½’ä¸€åŒ–åˆ°[-1.0, 1.0])
                'answer_quality': float,  # ç­”æ¡ˆè´¨é‡å¥–åŠ±
                'generation_quality': float,  # ç”Ÿæˆè´¨é‡å¥–åŠ±
                'breakdown': dict  # è¯¦ç»†åˆ†è§£
            }
        """
        metadata = metadata or {}
        execution_metadata = execution_metadata or {}

        # ========== ç»Ÿä¸€å¥–åŠ±æ¡†æ¶ ==========
        # ä¼˜å…ˆçº§1: çº¦æŸè¿è§„ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        # ä¼˜å…ˆçº§2: æ‰§è¡Œå¤±è´¥
        # ä¼˜å…ˆçº§3: æ‰§è¡ŒæˆåŠŸ â†’ è¯„ä¼°ç­”æ¡ˆè´¨é‡å’Œä»£ç è´¨é‡

        # === æƒ…å†µ1: Operator-é—®é¢˜ç±»å‹ä¸åŒ¹é… ===
        if execution_metadata.get('operator_problem_type_mismatch', False):
            reward = -0.6  # å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´
            return {
                'total': reward,
                'answer_quality': reward,
                'generation_quality': 0.0,
                'breakdown': {
                    'reason': 'operator_problem_type_mismatch',
                    'mismatch_type': execution_metadata.get('mismatch_type', 'Unknown'),
                    'penalty': -0.6
                }
            }

        # === æƒ…å†µ2: éªŒè¯å¤±è´¥ï¼ˆè¯­æ³•/æ ¼å¼é”™è¯¯ï¼‰ ===
        if execution_metadata.get('validation_failed', False):
            reward = -0.4  # æ¯”mismatchè½»
            return {
                'total': reward,
                'answer_quality': reward,
                'generation_quality': 0.0,
                'breakdown': {
                    'reason': 'validation_failed',
                    'validation_error': execution_metadata.get('validation_error', 'Unknown'),
                    'penalty': -0.4
                }
            }

        # === æƒ…å†µ3: æ‰§è¡Œå¤±è´¥ ===
        if not execution_metadata.get('success', True):
            error_type = execution_metadata.get('error_type', 'unknown')

            if error_type == 'empty_answer':
                reward = -0.8  # æ‰§è¡Œå®Œæˆä½†æ— è¾“å‡º
            elif error_type == 'code_leakage':
                reward = -0.7  # è¿”å›ç±»å‹é”™è¯¯
            else:
                reward = -1.0  # å®Œå…¨å¤±è´¥

            return {
                'total': reward,
                'answer_quality': reward,
                'generation_quality': 0.0,
                'breakdown': {
                    'reason': 'execution_failed',
                    'error_type': error_type,
                    'penalty': reward
                }
            }

        # === æƒ…å†µ4: æ‰§è¡ŒæˆåŠŸ â†’ è¯„ä¼°ç­”æ¡ˆè´¨é‡ + ç”Ÿæˆè´¨é‡ ===
        # 4.1 ç­”æ¡ˆè´¨é‡è¯„ä¼°ï¼ˆä½¿ç”¨LLM Judgeï¼‰
        is_correct = self.llm_judge_compare(
            problem=problem,
            prediction=str(prediction),
            ground_truth=str(ground_truth),
            problem_type=problem_type
        )

        # ç­”æ¡ˆè´¨é‡ï¼šæ­£ç¡®=+1.0, é”™è¯¯=-0.5ï¼ˆå½’ä¸€åŒ–ï¼‰
        answer_quality_score = 1.0 if is_correct else -0.5

        # 4.2 ç”Ÿæˆè´¨é‡è¯„ä¼°ï¼ˆä»£ç è´¨é‡ï¼‰
        generation_quality_score = 0.0

        # æ£€æŸ¥ç­¾åé”™è¯¯
        if execution_metadata.get('had_signature_error', False):
            generation_quality_score -= 0.3
        else:
            generation_quality_score += 0.15

        # æ£€æŸ¥æ‹¼å†™é”™è¯¯
        if execution_metadata.get('had_typo_errors', False):
            generation_quality_score -= 0.25

        # æ£€æŸ¥æœªåˆå§‹åŒ–å˜é‡
        if execution_metadata.get('had_uninitialized_vars', False):
            generation_quality_score -= 0.2

        # æ£€æŸ¥æ˜¯å¦éœ€è¦Fallback
        if execution_metadata.get('needed_fallback', False):
            generation_quality_score -= 0.15
        else:
            generation_quality_score += 0.15

        # æ£€æŸ¥éªŒè¯å¤±è´¥
        if execution_metadata.get('validation_failed', False):
            generation_quality_score -= 0.1

        # æ£€æŸ¥æœªåˆå§‹åŒ–operators
        if execution_metadata.get('had_uninitialized_operators', False):
            generation_quality_score -= 0.1
        else:
            if 'had_uninitialized_operators' in execution_metadata:
                generation_quality_score += 0.05

        # é™åˆ¶generation_quality_scoreèŒƒå›´åˆ°[-0.5, 0.3]
        generation_quality_score = max(-0.5, min(0.3, generation_quality_score))

        # 4.3 æ€»å¥–åŠ±ï¼ˆå½’ä¸€åŒ–ï¼‰
        total_score = answer_quality_score + generation_quality_score
        # ç¡®ä¿æ€»å¥–åŠ±åœ¨[-1.0, 1.0]èŒƒå›´å†…
        total_score = max(-1.0, min(1.0, total_score))

        # ========== æ‰“å°è¯¦ç»†çš„å¥–åŠ±åˆ†è§£ ==========
        print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š ç»Ÿä¸€å¥–åŠ±æ¡†æ¶ (å½’ä¸€åŒ–åˆ°[-1.0, 1.0])    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç­”æ¡ˆè´¨é‡:     {answer_quality_score:+6.2f}  {'âœ… æ­£ç¡®' if is_correct else 'âŒ é”™è¯¯'}
â”‚ ç”Ÿæˆè´¨é‡:     {generation_quality_score:+6.2f}
â”‚   â”œâ”€ ç­¾å: {'âœ… +0.15' if not execution_metadata.get('had_signature_error') else 'âŒ -0.30'}
â”‚   â”œâ”€ æ‹¼å†™: {'âœ… æ— è¯¯' if not execution_metadata.get('had_typo_errors') else 'âŒ -0.25'}
â”‚   â”œâ”€ åˆå§‹åŒ–: {'âœ… æ­£ç¡®' if not execution_metadata.get('had_uninitialized_vars') else 'âŒ -0.20'}
â”‚   â”œâ”€ æ‰§è¡Œ: {'âœ… +0.15' if not execution_metadata.get('needed_fallback') else 'âŒ -0.15'}
â”‚   â””â”€ ç®—å­: {'âœ… +0.05' if not execution_metadata.get('had_uninitialized_operators') else 'âŒ -0.10'}
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€»å¥–åŠ±:       {total_score:+6.2f}
â”‚ èŒƒå›´: [-1.0 å®Œå…¨å¤±è´¥, +1.0 å®Œç¾]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

        # ä¿å­˜åˆ°å…ƒæ•°æ®
        if metadata is not None:
            metadata['answer_quality_score'] = answer_quality_score
            metadata['generation_quality_score'] = generation_quality_score
            metadata['total_score'] = total_score
            metadata['used_llm_judge'] = True
            metadata['has_signature_error'] = execution_metadata.get('had_signature_error', False)

        return {
            'total': total_score,  # å·²å½’ä¸€åŒ–åˆ°[-1.0, 1.0]
            'answer_quality': answer_quality_score,
            'generation_quality': generation_quality_score,
            'breakdown': {
                'answer_quality_score': answer_quality_score,
                'generation_quality_score': generation_quality_score,
                'total_score': total_score,
                'is_correct': is_correct,
                'had_signature_error': execution_metadata.get('had_signature_error', False),
                'needed_fallback': execution_metadata.get('needed_fallback', False),
                'validation_failed': execution_metadata.get('validation_failed', False)
            }
        }

    def _is_correct(
        self,
        prediction: Any,
        ground_truth: Any,
        problem_type: str
    ) -> bool:
        """
        åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®

        Returns:
            bool: True if correct, False otherwise
        """
        if prediction is None:
            return False

        if problem_type == "math":
            return self._is_math_correct(prediction, ground_truth)
        elif problem_type == "code":
            return self._is_code_correct(prediction, ground_truth)
        elif problem_type == "qa":
            return self._is_qa_correct(prediction, ground_truth)
        else:
            return self._is_general_correct(prediction, ground_truth)

    def _is_math_correct(self, prediction: str, ground_truth: str) -> bool:
        """
        åˆ¤æ–­æ•°å­¦ç­”æ¡ˆæ˜¯å¦æ­£ç¡®

        æ”¯æŒ:
        - æ•°å­—æ¯”è¾ƒï¼ˆå«æµ®ç‚¹è¯¯å·®ï¼‰
        - åˆ†æ•°æ¯”è¾ƒï¼ˆå¦‚ 5/324 vs 0.0154...ï¼‰
        - å­—ç¬¦ä¸²åŒ¹é…
        """
        try:
            pred_str = str(prediction).strip()
            gt_str = str(ground_truth).strip()

            # å­—ç¬¦ä¸²å®Œå…¨åŒ¹é…
            if pred_str == gt_str:
                return True

            # è§£æä¸ºæ•°å€¼æ¯”è¾ƒï¼ˆæ”¯æŒåˆ†æ•°ï¼‰
            def parse_number(s: str) -> float:
                """è§£ææ•°å­—ï¼Œæ”¯æŒåˆ†æ•°æ ¼å¼"""
                if '/' in s:
                    parts = s.split('/')
                    return float(parts[0]) / float(parts[1])
                return float(s)

            try:
                pred_num = parse_number(pred_str)
                gt_num = parse_number(gt_str)

                # ä½¿ç”¨ç›¸å¯¹è¯¯å·®æ¯”è¾ƒï¼ˆå¤„ç†æµ®ç‚¹ç²¾åº¦ï¼‰
                rel_error = abs(pred_num - gt_num) / (abs(gt_num) + 1e-9)
                return rel_error < 1e-6
            except (ValueError, ZeroDivisionError, TypeError) as e:
                # æ•°å­—è§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                pass

            # æ–¹æ³•1: boxed æ ¼å¼
            pred_boxed = self._extract_boxed(pred_str)
            gt_boxed = self._extract_boxed(gt_str)
            if pred_boxed and gt_boxed:
                try:
                    pred_num = parse_number(pred_boxed)
                    gt_num = parse_number(gt_boxed)
                    rel_error = abs(pred_num - gt_num) / (abs(gt_num) + 1e-9)
                    if rel_error < 1e-6:
                        return True
                except (ValueError, ZeroDivisionError, TypeError) as e:
                    # boxedæ ¼å¼è§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                    pass

            # æ–¹æ³•2: æ•°å­—æå–
            pred_numbers = self._extract_numbers(pred_str)
            gt_numbers = self._extract_numbers(gt_str)

            if not gt_numbers:
                # æ— æ³•æå–æ•°å­—ï¼Œç”¨å­—ç¬¦ä¸²åŒ¹é…
                return gt_str.strip().lower() in pred_str.strip().lower()

            if not pred_numbers:
                return False

            # æ¯”è¾ƒæœ€åä¸€ä¸ªæ•°å­—
            pred_answer = pred_numbers[-1]
            gt_answer = gt_numbers[-1]

            return abs(pred_answer - gt_answer) < 1e-4

        except Exception:
            return False

    def _is_code_correct(self, prediction: str, ground_truth: str) -> bool:
        """åˆ¤æ–­ä»£ç ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        try:
            pred_str = str(prediction).strip()
            gt_str = str(ground_truth).strip()

            if not pred_str:
                return False

            # ç²¾ç¡®åŒ¹é…
            if pred_str.lower() == gt_str.lower():
                return True

            # åŒ…å«åŒ¹é…
            if gt_str.lower() in pred_str.lower():
                return True

            return False

        except Exception:
            return False

    def _is_qa_correct(self, prediction: str, ground_truth: str) -> bool:
        """åˆ¤æ–­QAç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        try:
            pred_str = str(prediction).strip().lower()
            gt_str = str(ground_truth).strip().lower()

            # ç²¾ç¡®åŒ¹é…
            if pred_str == gt_str:
                return True

            # åŒ…å«åŒ¹é…
            if gt_str in pred_str or pred_str in gt_str:
                return True

            # Tokené‡å é˜ˆå€¼
            pred_tokens = set(pred_str.split())
            gt_tokens = set(gt_str.split())

            if len(gt_tokens) == 0:
                return False

            overlap_ratio = len(pred_tokens & gt_tokens) / len(gt_tokens)
            return overlap_ratio > 0.8

        except Exception:
            return False

    def _is_general_correct(self, prediction: str, ground_truth: str) -> bool:
        """é€šç”¨æ­£ç¡®æ€§åˆ¤æ–­"""
        try:
            pred_str = str(prediction).strip().lower()
            gt_str = str(ground_truth).strip().lower()

            return pred_str == gt_str or gt_str in pred_str

        except Exception:
            return False

    def _compute_correctness_reward(
        self,
        prediction: Any,
        ground_truth: Any,
        problem_type: str
    ) -> float:
        """
        è®¡ç®—æ­£ç¡®æ€§å¥–åŠ±ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰

        Returns:
            reward: [-10, 10]
        """
        if prediction is None:
            return -10.0  # æ‰§è¡Œå¤±è´¥

        if problem_type == "math":
            return self._compute_math_correctness(prediction, ground_truth)
        elif problem_type == "code":
            return self._compute_code_correctness(prediction, ground_truth)
        elif problem_type == "qa":
            return self._compute_qa_correctness(prediction, ground_truth)
        else:
            return self._compute_general_correctness(prediction, ground_truth)

    def _compute_math_correctness(self, prediction: str, ground_truth: str) -> float:
        """
        æ•°å­¦é—®é¢˜æ­£ç¡®æ€§(æ”¹è¿›ç‰ˆ - å€Ÿé‰´ROLL)

        æ”¹è¿›:
        1. æ”¯æŒLaTeX \boxed{}æ ¼å¼
        2. æ›´ç»†ç²’åº¦çš„è¯„åˆ†é˜¶æ¢¯
        3. æ›´å¥½çš„æ•°å­—æå–
        """
        try:
            pred_str = str(prediction)
            gt_str = str(ground_truth)

            # æ–¹æ³•1: æå–boxedç­”æ¡ˆ(ROLLé£æ ¼)
            pred_boxed = self._extract_boxed(pred_str)
            gt_boxed = self._extract_boxed(gt_str)

            if pred_boxed and gt_boxed:
                try:
                    pred_num = float(pred_boxed)
                    gt_num = float(gt_boxed)
                    diff = abs(pred_num - gt_num)

                    if diff < 1e-4:
                        return 10.0   # å®Œå…¨æ­£ç¡®
                    elif diff < 0.1:
                        return 8.0    # éå¸¸æ¥è¿‘(æ–°å¢é˜¶æ¢¯)
                    elif diff < 1.0:
                        return 5.0    # æ¥è¿‘
                    elif diff < 10.0:
                        return 2.0    # æ•°é‡çº§æ­£ç¡®(æ–°å¢é˜¶æ¢¯)
                    else:
                        return -5.0   # é”™è¯¯
                except (ValueError, ZeroDivisionError, TypeError) as e:
                    # æ•°å­—è§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                    pass

            # æ–¹æ³•2: æ•°å­—æå–(æ”¹è¿›ç‰ˆ)
            pred_numbers = self._extract_numbers(pred_str)
            gt_numbers = self._extract_numbers(gt_str)

            if not gt_numbers:
                # æ— æ³•æå–ground truthæ•°å­—,ä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…
                if gt_str.strip().lower() in pred_str.strip().lower():
                    return 10.0
                else:
                    return -5.0

            if not pred_numbers:
                # æ— æ³•æå–é¢„æµ‹æ•°å­—
                return -8.0

            # å–æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºç­”æ¡ˆ
            pred_answer = pred_numbers[-1]
            gt_answer = gt_numbers[-1]

            # æ¯”è¾ƒ(æ›´ç»†ç²’åº¦)
            diff = abs(pred_answer - gt_answer)

            if diff < 1e-4:
                return 10.0   # å®Œå…¨æ­£ç¡®
            elif diff < 0.1:
                return 8.0    # éå¸¸æ¥è¿‘
            elif diff < 1.0:
                return 5.0    # æ¥è¿‘
            elif diff < 10.0:
                return 2.0    # æ•°é‡çº§æ­£ç¡®
            else:
                return -5.0   # é”™è¯¯

        except Exception as e:
            print(f"âš ï¸  æ•°å­¦è¯„ä¼°é”™è¯¯: {e}")
            return -5.0

    def _extract_boxed(self, text: str) -> Optional[str]:
        """æå–\boxed{}ä¸­çš„å†…å®¹(ROLLé£æ ¼)"""
        match = re.search(r'\\boxed\{([^}]+)\}', text)
        if match:
            return match.group(1).strip()
        return None

    def _extract_numbers(self, text: str) -> list:
        """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­—(æ”¹è¿›ç‰ˆ + æ–‡å­—æ•°å­—è¯†åˆ«)"""
        numbers = []

        # Method 1: Numeric extraction (existing)
        # åŒ¹é…æ•´æ•°ã€å°æ•°ã€è´Ÿæ•°ã€ç§‘å­¦è®¡æ•°æ³•
        pattern = r'-?\d+\.?\d*(?:[eE][+-]?\d+)?'
        matches = re.findall(pattern, text)
        for m in matches:
            if m:
                try:
                    numbers.append(float(m))
                except (ValueError, TypeError) as e:
                    # Floatè½¬æ¢å¤±è´¥ï¼Œè·³è¿‡æ­¤åŒ¹é…
                    pass

        # Method 2: Word-to-number recognition (NEW - fixes ~15-20% QA errors)
        # Aligns with SQuAD/HotpotQA standards for text-based answers
        word_to_num = {
            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4,
            'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9,
            'ten': 10, 'eleven': 11, 'twelve': 12, 'thirteen': 13,
            'fourteen': 14, 'fifteen': 15, 'sixteen': 16, 'seventeen': 17,
            'eighteen': 18, 'nineteen': 19, 'twenty': 20, 'thirty': 30,
            'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,
            'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000
        }

        text_lower = text.lower()
        for word, num in word_to_num.items():
            if word in text_lower:
                numbers.append(float(num))

        return numbers

    def _compute_code_correctness(self, prediction: str, ground_truth: str) -> float:
        """
        ä»£ç é—®é¢˜æ­£ç¡®æ€§(æ”¹è¿›ç‰ˆ)

        æ”¹è¿›è¯´æ˜ï¼š
        - åŒºåˆ†fallbackå ä½ç¬¦ (è¿”å›-3.0) vs çœŸæ­£çš„ç©ºé¢„æµ‹ (è¿”å›-10.0)
        - fallbackå ä½ç¬¦è¡¨ç¤ºè‡³å°‘å°è¯•äº†ï¼Œç»™äºˆéƒ¨åˆ†å­¦ä¹ ä¿¡å·
        - çœŸæ­£çš„ç©ºé¢„æµ‹è¯´æ˜å½»åº•å¤±è´¥ï¼Œç»™äºˆä¸¥å‰æƒ©ç½š
        """
        try:
            pred_str = str(prediction).strip()
            gt_str = str(ground_truth).strip()

            # å¦‚æœé¢„æµ‹ä¸ºç©º
            if not pred_str:
                return -10.0  # å½»åº•å¤±è´¥

            # æ£€æŸ¥æ˜¯å¦ä¸ºfallbackå ä½ç¬¦
            if "[Fallback placeholder for problem:" in pred_str:
                # Fallbackæœºåˆ¶æˆåŠŸè§¦å‘ï¼Œè‡³å°‘è¿”å›äº†æŸäº›å†…å®¹
                # ç»™äºˆéƒ¨åˆ†å­¦ä¹ ä¿¡å·ï¼Œè€Œä¸æ˜¯å®Œå…¨æƒ©ç½š
                return -3.0

            # å®Œå…¨åŒ¹é…(æœ€é«˜åˆ†)
            if pred_str.lower() == gt_str.lower():
                return 10.0

            # åŒ…å«åŒ¹é…
            if gt_str.lower() in pred_str.lower():
                return 10.0

            # æå–å‡½æ•°å®šä¹‰
            pred_funcs = self._extract_function_names(pred_str)
            gt_funcs = self._extract_function_names(gt_str)

            # æ£€æŸ¥å‡½æ•°åæ˜¯å¦åŒ¹é…
            if pred_funcs and gt_funcs:
                if any(pf == gf for pf in pred_funcs for gf in gt_funcs):
                    return 5.0  # éƒ¨åˆ†æ­£ç¡®

            # æ£€æŸ¥æ˜¯å¦è‡³å°‘åŒ…å«Pythonä»£ç ç‰¹å¾
            if "def " in pred_str and ("return" in pred_str or "print" in pred_str):
                # è‡³å°‘çœ‹èµ·æ¥åƒä»£ç ï¼Œç»™äºˆä¸­ç­‰æƒ©ç½š
                return -2.0

            return -5.0

        except Exception as e:
            print(f"âš ï¸  ä»£ç è¯„ä¼°é”™è¯¯: {e}")
            return -5.0

    def _extract_function_names(self, code: str) -> list:
        """ä»ä»£ç ä¸­æå–å‡½æ•°å"""
        pattern = r'def\s+(\w+)\s*\('
        matches = re.findall(pattern, code)
        return matches

    def _compute_qa_correctness(self, prediction: str, ground_truth: str) -> float:
        """
        QAé—®é¢˜æ­£ç¡®æ€§(ROLLé£æ ¼æ”¹è¿›)
        """
        try:
            pred_str = str(prediction).strip().lower()
            gt_str = str(ground_truth).strip().lower()

            if not pred_str:
                return -10.0

            # ç²¾ç¡®åŒ¹é…
            if pred_str == gt_str:
                return 10.0

            # åŒ…å«åŒ¹é…
            if gt_str in pred_str:
                return 8.0

            # Tokené‡å 
            pred_tokens = set(pred_str.split())
            gt_tokens = set(gt_str.split())

            if not gt_tokens:
                return -5.0

            overlap_ratio = len(pred_tokens & gt_tokens) / len(gt_tokens)

            if overlap_ratio > 0.8:
                return 6.0
            elif overlap_ratio > 0.5:
                return 3.0
            elif overlap_ratio > 0.2:
                return 0.0
            else:
                return -5.0

        except Exception as e:
            print(f"âš ï¸  QAè¯„ä¼°é”™è¯¯: {e}")
            return -5.0

    def _compute_general_correctness(self, prediction: str, ground_truth: str) -> float:
        """é€šç”¨æ­£ç¡®æ€§è¯„ä¼°"""
        return self._compute_qa_correctness(prediction, ground_truth)


def test_reward_computer():
    """æµ‹è¯•æ”¹è¿›ç‰ˆå¥–åŠ±è®¡ç®—å™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›ç‰ˆå¥–åŠ±è®¡ç®—å™¨")
    print("=" * 60)

    computer = RewardComputer()

    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "name": "æ•°å­¦ - å®Œç¾æ ¼å¼+æ­£ç¡®",
            "problem": "What is 15 + 27?",
            "prediction": "<think>Let me calculate: 15 + 27 = 42</think><answer>\\boxed{42}</answer>",
            "ground_truth": "42",
            "problem_type": "math",
            "metadata": {"cost": 0.002, "execution_time": 3.5}
        },
        {
            "name": "æ•°å­¦ - æ­£ç¡®ä½†æ— æ ¼å¼",
            "problem": "What is 15 + 27?",
            "prediction": "The answer is 42.",
            "ground_truth": "42",
            "problem_type": "math",
            "metadata": {"cost": 0.002, "execution_time": 3.0}
        },
        {
            "name": "æ•°å­¦ - æ¥è¿‘ç­”æ¡ˆ",
            "problem": "What is 15 + 27?",
            "prediction": "<think>Calculating</think><answer>42.1</answer>",
            "ground_truth": "42",
            "problem_type": "math",
            "metadata": {"cost": 0.001, "execution_time": 2.0}
        },
        {
            "name": "ä»£ç  - æ­£ç¡®+æ ¼å¼",
            "problem": "Write a function to square a number",
            "prediction": "```python\ndef square(x):\n    return x * x\n```",
            "ground_truth": "def square(x):\n    return x * x",
            "problem_type": "code",
            "metadata": {"cost": 0.003, "execution_time": 5.0}
        },
        {
            "name": "QA - æ­£ç¡®",
            "problem": "What is the capital of France?",
            "prediction": "The capital of France is Paris.",
            "ground_truth": "Paris",
            "problem_type": "qa",
            "metadata": {"cost": 0.001, "execution_time": 2.0}
        },
        {
            "name": "ä¸¥é‡é‡å¤",
            "problem": "Test",
            "prediction": "answer answer answer answer answer answer",
            "ground_truth": "answer",
            "problem_type": "qa",
            "metadata": {"cost": 0.001, "execution_time": 1.0}
        }
    ]

    for case in test_cases:
        reward = computer.compute_reward(
            problem=case["problem"],
            prediction=case["prediction"],
            ground_truth=case["ground_truth"],
            problem_type=case["problem_type"],
            metadata=case["metadata"]
        )

        print(f"\nğŸ“ {case['name']}")
        print(f"  é¢„æµ‹: {case['prediction'][:60]}...")
        print(f"  æ­£ç¡®ç­”æ¡ˆ: {case['ground_truth']}")
        print(f"  å¥–åŠ±: {reward:.2f}/10.0")


if __name__ == "__main__":
    test_reward_computer()
