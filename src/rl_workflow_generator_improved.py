#!/usr/bin/env python3
"""
RLå·¥ä½œæµç”Ÿæˆå™¨ - ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ

è®¾è®¡åŸåˆ™ï¼š
1. æ”¯æŒæ¨¡å‹å…±äº«ï¼ˆé«˜æ•ˆï¼Œç”¨äºGRPOè®­ç»ƒï¼‰å’Œç‹¬ç«‹æ¨¡å¼
2. Promptè®¾è®¡æ¸…æ™°ç®€æ´ï¼ˆé—®é¢˜ç±»å‹ç‰¹å®šï¼‰
3. éªŒè¯å’Œä¿®å¤é€»è¾‘ç»Ÿä¸€è€Œä¸æ˜¯åˆ†æ•£
4. æ–¹æ³•èŒè´£æ¸…æ™°ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–
"""
import torch
import json
import ast
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import sys
import os
import re


class RLWorkflowGenerator:
    """ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ"""

    def __init__(
        self,
        model: Optional[Any] = None,
        tokenizer: Optional[Any] = None,
        device: Optional[str] = None,
        base_model: str = "Qwen/Qwen2.5-7B-Instruct",
        lora_checkpoint: Optional[str] = None,
        device_ids: List[int] = None,
        operator_descriptions_path: Optional[str] = None,
        config: Optional[Dict] = None
    ):
        """
        åˆå§‹åŒ–å·¥ä½œæµç”Ÿæˆå™¨

        Args:
            model: å…±äº«çš„æ¨¡å‹å®ä¾‹ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            tokenizer: å…±äº«çš„tokenizerå®ä¾‹ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            device: å…±äº«çš„è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            base_model: åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆä»…åœ¨model=Noneæ—¶ä½¿ç”¨ï¼‰
            lora_checkpoint: LoRAæ£€æŸ¥ç‚¹è·¯å¾„
            device_ids: GPU IDåˆ—è¡¨ï¼ˆä»…åœ¨device=Noneæ—¶ä½¿ç”¨ï¼‰
            operator_descriptions_path: AFlowç®—å­æè¿°æ–‡ä»¶è·¯å¾„
            config: é¢å¤–é…ç½®
        """
        self.config = config or {}

        # æ¨¡å‹åˆå§‹åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å…±äº«æ¨¡å‹
        if model is not None:
            self._init_shared_model(model, tokenizer, device)
        else:
            self._init_standalone_model(base_model, lora_checkpoint, device_ids or [2, 3])

        # åŠ è½½ç®—å­æè¿°
        self.operator_descriptions = self._load_operator_descriptions(operator_descriptions_path)
        print(f"âœ… RLå·¥ä½œæµç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def _init_shared_model(self, model: Any, tokenizer: Any, device: str) -> None:
        """åˆå§‹åŒ–å…±äº«æ¨¡å‹æ¨¡å¼ï¼ˆç”¨äºGRPOè®­ç»ƒï¼‰"""
        print(f"ğŸ”§ åˆå§‹åŒ–å·¥ä½œæµç”Ÿæˆå™¨ï¼ˆæ¨¡å‹å…±äº«æ¨¡å¼ï¼‰")
        print(f"   å…±äº«æ¨¡å‹ID: {id(model)}")
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   âœ… èŠ‚çœ~15GB GPUå†…å­˜")

    def _init_standalone_model(self, base_model: str, lora_checkpoint: Optional[str], device_ids: List[int]) -> None:
        """åˆå§‹åŒ–ç‹¬ç«‹æ¨¡å‹æ¨¡å¼"""
        print(f"ğŸ”§ åˆå§‹åŒ–å·¥ä½œæµç”Ÿæˆå™¨ï¼ˆç‹¬ç«‹æ¨¡å¼ï¼‰")

        # è®¾ç½®CUDAè®¾å¤‡
        if torch.cuda.is_available():
            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, device_ids))

        self.device = f"cuda:{device_ids[0]}" if torch.cuda.is_available() else "cpu"
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   GPU: {device_ids}")

        # åŠ è½½tokenizerå’Œæ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½tokenizer: {base_model}")
        self.tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {base_model}")
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16,
            device_map={"": self.device},
            trust_remote_code=True
        )

        # åŠ è½½LoRAæƒé‡
        if lora_checkpoint:
            print(f"ğŸ“¥ åŠ è½½LoRAæ£€æŸ¥ç‚¹: {lora_checkpoint}")
            self.model = PeftModel.from_pretrained(self.model, lora_checkpoint)
            self.model.eval()

    def _load_operator_descriptions(self, descriptions_path: Optional[str]) -> Dict:
        """åŠ è½½AFlowç®—å­æè¿°"""
        if descriptions_path and Path(descriptions_path).exists():
            with open(descriptions_path, 'r') as f:
                return json.load(f)

        # é»˜è®¤ç®—å­æè¿°
        return {
            "Custom": {
                "description": "Generates anything based on customized input and instruction.",
                "interface": "custom(input: str, instruction: str) -> dict with key 'response'"
            },
            "AnswerGenerate": {
                "description": "Generates step-by-step reasoning and final answer.",
                "interface": "answer_generate(input: str) -> dict with keys 'thought' and 'answer'"
            },
            "Programmer": {
                "description": "Automatically writes and executes Python code.",
                "interface": "programmer(problem: str, analysis: str) -> dict with keys 'code' and 'output'"
            },
            "Test": {
                "description": "Tests code with test cases.",
                "interface": "test(problem: str, solution: str, entry_point: str) -> dict with 'result' and 'solution'"
            },
            "Review": {
                "description": "Reviews and provides feedback on a solution.",
                "interface": "review(problem: str, solution: str) -> dict with keys 'review_result' and 'feedback'"
            },
            "Revise": {
                "description": "Revises solution based on feedback.",
                "interface": "revise(problem: str, solution: str, feedback: str) -> dict with key 'solution'"
            },
            "ScEnsemble": {
                "description": "Uses self-consistency to select the most frequent solution.",
                "interface": "scensemble(solutions: List[str], problem: str) -> dict with key 'response'"
            }
        }

    def _build_generation_prompt(self, problem: str, problem_type: str) -> str:
        """æ„å»ºé—®é¢˜ç‰¹å®šçš„ç”ŸæˆPrompt"""

        if problem_type == "code":
            return self._build_code_prompt(problem)
        else:  # math, qa
            return self._build_math_qa_prompt(problem)

    def _build_code_prompt(self, problem: str) -> str:
        """CODEé—®é¢˜ä¸“ç”¨Prompt"""
        return f"""Generate a Python Workflow class to solve this CODE problem.

CRITICAL STRUCTURE (MUST follow exactly):
- class Workflow must inherit from CodeWorkflowBase
- __init__ must call super().__init__(name, llm_config, dataset)
- __call__ signature: async def __call__(self, problem: str, entry_point: str, test: str) -> Tuple[str, float]

Available Operators:
1. self.programmer(problem: str, analysis: str) -> {{'code': str, 'output': str}}
2. self.test(problem: str, solution: str, entry_point: str, test_loop: int) -> {{'result': bool, 'solution': str}}
3. self.review(problem: str, solution: str) -> {{'review_result': bool, 'feedback': str}}
4. self.revise(problem: str, solution: str, feedback: str) -> {{'solution': str}}
5. self.custom(input: str, instruction: str) -> {{'response': str}}

âœ… CORRECT Example:
```python
class Workflow(CodeWorkflowBase):
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        super().__init__(name, llm_config, dataset)

    async def __call__(self, problem: str, entry_point: str, test: str) -> Tuple[str, float]:
        self._test_input = test
        code_result = await self.programmer(problem=problem, analysis='')
        code = code_result.get('code', '')

        test_result = await self.test(problem=problem, solution=code, entry_point=entry_point, test_loop=3)
        if test_result.get('result', False):
            return test_result.get('solution', code), self.llm.get_usage_summary()["total_cost"]

        review = await self.review(problem=problem, solution=code)
        if not review.get('review_result', True):
            revised = await self.revise(problem=problem, solution=code, feedback=review.get('feedback', ''))
            code = revised.get('solution', code)

        return code, self.llm.get_usage_summary()["total_cost"]
```

PROBLEM:
{problem}

Generate the complete class now:
"""

    def _build_math_qa_prompt(self, problem: str) -> str:
        """MATH/QAé—®é¢˜ä¸“ç”¨Prompt"""
        return f"""Generate a Python Workflow class to solve this problem.

CRITICAL STRUCTURE (MUST follow exactly):
- class Workflow must inherit from MathWorkflowBase (for MATH) or QAWorkflowBase (for QA)
- __init__ must call super().__init__(name, llm_config, dataset)
- __call__ signature: async def __call__(self, problem: str) -> Tuple[str, float]

Available Operators:
1. self.answer_generate(input: str) -> {{'thought': str, 'answer': str}}
2. self.review(problem: str, solution: str) -> {{'review_result': bool, 'feedback': str}}
3. self.revise(problem: str, solution: str, feedback: str) -> {{'solution': str}}
4. self.scensemble(solutions: List[str], problem: str) -> {{'response': str}}
5. self.custom(input: str, instruction: str) -> {{'response': str}}

âœ… CORRECT Example:
```python
class Workflow(MathWorkflowBase):
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        super().__init__(name, llm_config, dataset)

    async def __call__(self, problem: str) -> Tuple[str, float]:
        ans = await self.answer_generate(input=problem)
        answer = ans.get('answer', '')

        review = await self.review(problem=problem, solution=answer)
        if not review.get('review_result', True):
            revised = await self.revise(problem=problem, solution=answer, feedback=review.get('feedback', ''))
            answer = revised.get('solution', answer)

        return answer, self.llm.get_usage_summary()["total_cost"]
```

PROBLEM:
{problem}

Generate the complete class now:
"""

    def generate_workflow(
        self,
        problem: str,
        problem_type: str = "math",
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        return_full_output: bool = False,
        custom_prompt: Optional[str] = None
    ) -> Dict:
        """ç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ"""

        # æ„å»ºæˆ–ä½¿ç”¨è‡ªå®šä¹‰Prompt
        prompt = custom_prompt or self._build_generation_prompt(problem, problem_type)

        # ç”Ÿæˆ
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=self.config.get('top_p', 0.95),
                top_k=self.config.get('top_k', 50),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç 
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        # è§£æå’ŒéªŒè¯
        workflow_code, is_valid, error_msg = self._parse_workflow_code(generated_text, problem_type)

        result = {
            "workflow_code": workflow_code,
            "valid": is_valid,
            "error": error_msg,
            "metadata": {
                "problem": problem,
                "problem_type": problem_type,
                "temperature": temperature,
                "tokens_generated": outputs.shape[1] - inputs['input_ids'].shape[1]
            }
        }

        if return_full_output:
            result["full_output"] = generated_text
            result["prompt"] = prompt

        return result

    def _parse_workflow_code(self, generated_text: str, problem_type: str) -> Tuple[str, bool, Optional[str]]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¹¶éªŒè¯å·¥ä½œæµä»£ç """

        # æ‰“å°åŸå§‹è¾“å‡ºï¼ˆDEBUGï¼‰
        print(f"\n{'='*60}")
        print(f"ğŸ” ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬:")
        print(f"{'='*60}")
        print(generated_text[:500] + "..." if len(generated_text) > 500 else generated_text)
        print(f"{'='*60}\n")

        # æå–ä»£ç å—
        code = self._extract_code_block(generated_text)
        if not code:
            print(f"âŒ æ— æ³•æå–ä»£ç å—ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
            return self._get_default_workflow(problem_type), False, "No code block found"

        # éªŒè¯å’Œä¿®å¤
        code = self._validate_and_fix_workflow(code, problem_type)

        # éªŒè¯è¯­æ³•
        try:
            ast.parse(code)
            print(f"âœ… ä»£ç éªŒè¯æˆåŠŸ")
            return code, True, None
        except SyntaxError as e:
            print(f"âŒ è¯­æ³•é”™è¯¯: {str(e)}")
            return self._get_default_workflow(problem_type), False, f"Syntax error: {str(e)}"

    def _extract_code_block(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–Pythonä»£ç å—"""

        # ç­–ç•¥1ï¼šæŸ¥æ‰¾```python```æ ‡è®°
        match = re.search(r'```python\n(.*?)\n```', text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # ç­–ç•¥2ï¼šæŸ¥æ‰¾class Workflowå®šä¹‰
        match = re.search(r'(class Workflow.*?)(?=\n(?:class|def|$)|\Z)', text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # ç­–ç•¥3ï¼šå¦‚æœä»£ç çœ‹èµ·æ¥å®Œæ•´å°±è¿”å›
        if 'class Workflow' in text and 'async def __call__' in text:
            # ä»classå¼€å§‹åˆ°æ–‡æœ¬æœ«å°¾
            start = text.find('class Workflow')
            return text[start:].strip()

        return ""

    def _validate_and_fix_workflow(self, code: str, problem_type: str) -> str:
        """éªŒè¯å¹¶è‡ªåŠ¨ä¿®å¤workflowä»£ç """

        # ä¿®å¤1ï¼šç¡®ä¿æœ‰å®Œæ•´çš„classå®šä¹‰
        code = self._enforce_correct_structure(code, problem_type)

        # ä¿®å¤2ï¼šä¿®å¤å¸¸è§çš„operatorå‚æ•°é”™è¯¯
        code = self._fix_operator_parameters(code, problem_type)

        return code

    def _enforce_correct_structure(self, code: str, problem_type: str) -> str:
        """å¼ºåˆ¶ä¿®å¤ä»£ç ç»“æ„ç¼ºé™·"""

        base_class = {
            'math': 'MathWorkflowBase',
            'code': 'CodeWorkflowBase',
            'qa': 'QAWorkflowBase'
        }.get(problem_type, 'MathWorkflowBase')

        # æ£€æŸ¥æ˜¯å¦æœ‰classå®šä¹‰
        if not re.search(r'class\s+Workflow', code):
            # åªæœ‰__call__æ–¹æ³•ä½“ï¼Œéœ€è¦åŒ…è£…
            if 'async def __call__' in code:
                call_match = re.search(r'(async def __call__.*?:)(.*)', code, re.DOTALL)
                if call_match:
                    call_body = call_match.group(2)
                    code = f"""class Workflow({base_class}):
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        super().__init__(name, llm_config, dataset)

    async def __call__(self, problem: str) -> Tuple[str, float]:
{self._indent(call_body, 8)}
"""
                    print(f"âœ… å·²ä¿®å¤ï¼šæ·»åŠ classå®šä¹‰å’Œ__init__")

        # æ£€æŸ¥ç»§æ‰¿
        if not re.search(rf'class\s+Workflow\s*\(\s*{base_class}\s*\)', code):
            code = re.sub(
                r'class\s+Workflow\s*\(\s*[^)]*\s*\)',
                f'class Workflow({base_class})',
                code
            )
            print(f"âœ… å·²ä¿®å¤ï¼šclassç»§æ‰¿")

        # æ£€æŸ¥super()è°ƒç”¨
        if 'def __init__' in code and 'super().__init__' not in code:
            code = re.sub(
                r'(def __init__\s*\([^)]*\)\s*:\s*\n)',
                r'\1        super().__init__(name, llm_config, dataset)\n',
                code
            )
            print(f"âœ… å·²ä¿®å¤ï¼šsuper().__init__() è°ƒç”¨")

        return code

    def _fix_operator_parameters(self, code: str, problem_type: str) -> str:
        """ä¿®å¤å¸¸è§çš„operatorå‚æ•°é”™è¯¯"""

        # ä¿®å¤answer_generateå‚æ•°
        if problem_type in ['math', 'qa']:
            code = re.sub(r'answer_generate\s*\(\s*problem\s*=', 'answer_generate(input=', code)

        # ä¿®å¤reviewç¼ºå°‘problemå‚æ•°
        code = re.sub(
            r'review\s*\(\s*solution\s*=\s*([^,\)]+)\s*\)',
            r'review(problem=problem, solution=\1)',
            code
        )

        # ä¿®å¤reviseç¼ºå°‘problemå‚æ•°
        code = re.sub(
            r'revise\s*\(\s*solution\s*=',
            r'revise(problem=problem, solution=',
            code
        )

        # ä¿®å¤testç¼ºå°‘problemå‚æ•°
        if problem_type == 'code':
            code = re.sub(
                r'test\s*\(\s*solution\s*=',
                r'test(problem=problem, solution=',
                code
            )

        return code

    @staticmethod
    def _indent(text: str, spaces: int) -> str:
        """ä¸ºä»£ç å—æ·»åŠ ç¼©è¿›"""
        indent = ' ' * spaces
        lines = text.split('\n')
        return '\n'.join(indent + line if line.strip() else line for line in lines)

    def _get_default_workflow(self, problem_type: str = "math") -> str:
        """é»˜è®¤å·¥ä½œæµ"""
        base_class = {
            'math': 'MathWorkflowBase',
            'code': 'CodeWorkflowBase',
            'qa': 'QAWorkflowBase'
        }.get(problem_type, 'MathWorkflowBase')

        if problem_type == 'code':
            return f"""class Workflow(CodeWorkflowBase):
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        super().__init__(name, llm_config, dataset)

    async def __call__(self, problem: str, entry_point: str, test: str) -> Tuple[str, float]:
        self._test_input = test
        code_result = await self.programmer(problem=problem, analysis='')
        return code_result.get('code', ''), self.llm.get_usage_summary()["total_cost"]
"""
        else:
            return f"""class Workflow({base_class}):
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        super().__init__(name, llm_config, dataset)

    async def __call__(self, problem: str) -> Tuple[str, float]:
        ans = await self.answer_generate(input=problem)
        return ans.get('answer', ''), self.llm.get_usage_summary()["total_cost"]
"""


def test_generator():
    """æµ‹è¯•ç”Ÿæˆå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•RLå·¥ä½œæµç”Ÿæˆå™¨")
    print("=" * 60)

    generator = RLWorkflowGenerator(
        base_model="Qwen/Qwen2.5-7B-Instruct",
        device_ids=[2, 3]
    )

    test_problem = "What is 15 + 27?"
    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_problem}")

    result = generator.generate_workflow(
        problem=test_problem,
        problem_type="math",
        temperature=0.7,
        max_new_tokens=1024
    )

    print(f"\nâœ… ç»“æœ:")
    print(f"   æœ‰æ•ˆ: {result['valid']}")
    if result['error']:
        print(f"   é”™è¯¯: {result['error']}")
    print(f"\nğŸ“„ ä»£ç :")
    print(result['workflow_code'])


if __name__ == "__main__":
    test_generator()
