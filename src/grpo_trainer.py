#!/usr/bin/env python3
"""
GRPOè®­ç»ƒå™¨ - åœ¨çº¿å­¦ä¹ æ¨¡å¼çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
"""
import os
import torch
import torch.nn.functional as F
import asyncio
import numpy as np
import gc  # âœ… PHASE 5 FIX: For memory cleanup
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import time
import json
import wandb  # âœ¨ æ–°å¢wandbé›†æˆ

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer

from data_manager import DataManager
from rl_workflow_generator import RLWorkflowGenerator
from aflow_executor import AFlowExecutor
from reward_computer import RewardComputer
from gpu_manager import GPUManager
from experience_buffer import ExperienceBuffer
from prompt_optimizer import PromptOptimizer
from operator_prompt_enhancer import OperatorPromptEnhancer


class GRPOTrainer:
    """GRPOè®­ç»ƒå™¨ï¼šåœ¨çº¿å­¦ä¹ æ¨¡å¼"""

    def __init__(self, config_path: str = "config/training.yaml",
                 model_name: Optional[str] = None,
                 device: Optional[str] = None,
                 output_dir: Optional[str] = None):
        """
        Args:
            config_path: è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
            model_name: æ¨¡å‹åç§° (qwen25-7b, qwen3-8b) - ä¼šè¦†ç›–configé…ç½®
            device: GPUè®¾å¤‡ (cuda:0, cuda:1ç­‰) - ä¼šè¦†ç›–configé…ç½®
            output_dir: æ£€æŸ¥ç‚¹è¾“å‡ºç›®å½• - ä¼šè¦†ç›–configé…ç½®
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        # å¤„ç†æ¨¡å‹åç§°è¦†ç›–
        if model_name:
            # æ¨¡å‹åç§°åˆ°base_modelè·¯å¾„çš„æ˜ å°„
            model_mapping = {
                "qwen25-7b": "Qwen/Qwen2.5-7B-Instruct",
                "qwen3-8b": "Qwen/Qwen-3-8B"
            }
            if model_name in model_mapping:
                self.config['base_model'] = model_mapping[model_name]
                print(f"âœ… è¦†ç›–base_model: {self.config['base_model']}")

        # å¤„ç†è®¾å¤‡è¦†ç›–
        if device:
            # ä»è®¾å¤‡å­—ç¬¦ä¸²è§£æGPU IDï¼ˆå¦‚ cuda:0 -> [0]ï¼‰
            if device.startswith("cuda:"):
                gpu_id = int(device.split(":")[-1])
                self.config['device_mapping'] = [gpu_id]
                self.config['physical_gpus'] = [gpu_id]
                print(f"âœ… è¦†ç›–è®¾å¤‡: {device}")

        # å¤„ç†è¾“å‡ºç›®å½•è¦†ç›–
        if output_dir:
            self.config['checkpointing'] = self.config.get('checkpointing', {})
            self.config['checkpointing']['save_dir'] = output_dir
            print(f"âœ… è¦†ç›–è¾“å‡ºç›®å½•: {output_dir}")

        print("=" * 60)
        print("ğŸš€ åˆå§‹åŒ–GRPOè®­ç»ƒå™¨")
        print("=" * 60)

        # GPUç®¡ç†ï¼ˆä½¿ç”¨ç‰©ç†GPU IDï¼‰
        physical_gpus = self.config.get('physical_gpus', self.config['device_mapping'])
        self.gpu_manager = GPUManager(
            target_gpus=physical_gpus,
            protected_pids=self.config.get('protected_pids', []),
            auto_clean=False  # ç¦ç”¨è‡ªåŠ¨æ¸…ç†
        )

        # è·³è¿‡GPUç¯å¢ƒéªŒè¯ï¼Œç›´æ¥ä½¿ç”¨
        print(f"âœ… ä½¿ç”¨GPU {physical_gpus}ï¼ˆå·²ç¦ç”¨æ¸…ç†å’ŒéªŒè¯ï¼‰")

        # ç®€åŒ–é…ç½®ï¼šä½¿ç”¨å›ºå®šgeneration config
        gen_config = self.config.get('generation_config', {})
        self.generation_temperature = gen_config.get('temperature', 0.4)
        print(f"\nğŸŒ¡ï¸  Generation Config:")
        print(f"  Temperature: {self.generation_temperature} (fixed)")

        # âœ¨ åˆå§‹åŒ–wandb
        self._initialize_wandb()

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

        print("=" * 60)
        print("âœ… GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60)

    def _initialize_wandb(self):
        """åˆå§‹åŒ–wandbç›‘æ§"""
        # ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è·å–wandbè®¾ç½®
        wandb_config = self.config.get('wandb', {})

        # è®¾ç½®API key(å¦‚æœæä¾›çš„è¯)
        wandb_api_key = wandb_config.get('api_key', os.getenv('WANDB_API_KEY'))

        # å°è¯•ç™»å½•,å¦‚æœå¤±è´¥åˆ™ä½¿ç”¨offlineæ¨¡å¼
        try:
            if wandb_api_key and len(wandb_api_key) == 40:
                wandb.login(key=wandb_api_key)
                mode = "online"
            else:
                print("âš ï¸  wandb API keyæ— æ•ˆæˆ–æœªæä¾›,ä½¿ç”¨offlineæ¨¡å¼")
                mode = "offline"
        except Exception as e:
            print(f"âš ï¸  wandbç™»å½•å¤±è´¥: {e}, ä½¿ç”¨offlineæ¨¡å¼")
            mode = "offline"

        # åˆå§‹åŒ–wandb run
        wandb.init(
            project=wandb_config.get('project', 'aflow-roll-integration'),
            name=wandb_config.get('run_name', f"grpo-training-{time.strftime('%Y%m%d-%H%M%S')}"),
            mode=mode,  # onlineæˆ–offline
            config={
                # è®­ç»ƒé…ç½®
                "base_model": self.config['base_model'],
                "learning_rate": self.config['learning_rate'],
                "batch_size": self.config['rollout_batch_size'],
                "num_sequences": self.config['num_return_sequences_in_group'],
                "max_steps": self.config['max_steps'],
                "lora_rank": self.config['lora_rank'],
                "lora_alpha": self.config['lora_alpha'],
                # æ•°æ®é…ç½®
                "domain_ratios": self.config['domain_ratios'],
                # å¥–åŠ±é…ç½®
                "reward_weights": self.config.get('reward_weights', {}),
            },
            tags=["grpo", "aflow", "roll", "workflow-generation"],
            notes="GRPO training with improved reward function (ROLL+AgentFlow design)"
        )

        print("\nâœ… wandbåˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡å¼: {mode}")
        print(f"  é¡¹ç›®: {wandb.run.project}")
        print(f"  Runåç§°: {wandb.run.name}")
        if mode == "online":
            print(f"  Run URL: {wandb.run.url}")
        else:
            print(f"  ç¦»çº¿æ—¥å¿—: wandb/offline-run-*")

    def _initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""

        # 1. æ•°æ®ç®¡ç†å™¨
        print("\nğŸ“‚ åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
        self.data_manager = DataManager(
            data_dir=self.config['data_dir'],
            domain_ratios=self.config['domain_ratios']
        )
        self.data_manager.initialize()

        # 2. RLæ¨¡å‹ï¼ˆQwen2.5-7B + LoRAï¼‰
        print("\nğŸ¤– åŠ è½½RLæ¨¡å‹...")
        self._load_rl_model()

        # 3. RLå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆä½¿ç”¨å…±äº«æ¨¡å‹ï¼‰
        print("\nğŸ”§ åˆå§‹åŒ–å·¥ä½œæµç”Ÿæˆå™¨...")
        self.generator = RLWorkflowGenerator(
            model=self.model,  # âœ¨ Pass shared model reference
            tokenizer=self.tokenizer,  # âœ¨ Pass shared tokenizer
            device=self.model.device,  # âœ¨ Pass shared device
            operator_descriptions_path=self.config.get('aflow_operator_descriptions_path')
        )
        print(f"  âœ… æ¨¡å‹å…±äº«éªŒè¯:")
        print(f"    Traineræ¨¡å‹ID: {id(self.model)}")
        print(f"    Generatoræ¨¡å‹ID: {id(self.generator.model)}")
        if id(self.model) == id(self.generator.model):
            print(f"    âœ… æ¨¡å‹å…±äº«æˆåŠŸï¼èŠ‚çœ ~15GB GPUå†…å­˜")
        else:
            print(f"    âŒ è­¦å‘Š: æ¨¡å‹æœªå…±äº«ï¼Œå­˜åœ¨å†…å­˜æµªè´¹ï¼")

        # 4. ExperienceBuffer - é«˜è´¨é‡æ ·æœ¬ç®¡ç†ï¼ˆéœ€å…ˆåˆå§‹åŒ–ï¼Œç”¨äºåç»­ç»„ä»¶ï¼‰
        print("\nğŸ“š åˆå§‹åŒ–ExperienceBuffer...")
        experience_config = self.config.get('experience_buffer', {})
        self.experience_buffer = ExperienceBuffer(
            buffer_size=experience_config.get('buffer_size', 100),
            reward_threshold=experience_config.get('reward_threshold', 8.0),
            persistence_dir=experience_config.get('persistence_dir', 'data/experience_buffer'),
            problem_types=["math", "code", "qa"]
        )
        print(f"  Bufferå¤§å°: {self.experience_buffer.buffer_size}")
        print(f"  å¥–åŠ±é˜ˆå€¼: {self.experience_buffer.reward_threshold}")

        # 5. PromptOptimizer - Layer 1åŠ¨æ€æç¤ºè¯ä¼˜åŒ–
        print("\nâœ¨ åˆå§‹åŒ–PromptOptimizer (Layer 1)...")
        prompt_config = self.config.get('prompt_optimizer', {})
        self.prompt_optimizer = PromptOptimizer()
        self.use_dynamic_prompts = prompt_config.get('enabled', True)
        print(f"  åŠ¨æ€æç¤ºè¯: {'å¯ç”¨' if self.use_dynamic_prompts else 'ç¦ç”¨'}")

        # 6. OperatorPromptEnhancer - Layer 2 operatoræç¤ºè¯å¢å¼º
        print("\nğŸ”§ åˆå§‹åŒ–OperatorPromptEnhancer (Layer 2)...")
        operator_config = self.config.get('operator_prompt_enhancer', {})
        self.operator_enhancer = OperatorPromptEnhancer(
            enable_enhancement=operator_config.get('enabled', True)
        )
        print(f"  Operatorå¢å¼º: {'å¯ç”¨' if self.operator_enhancer.enable_enhancement else 'ç¦ç”¨'}")

        # 7. AFlowæ‰§è¡Œå™¨ï¼ˆä¼ å…¥operator_enhancerï¼‰
        print("\nâš™ï¸  åˆå§‹åŒ–AFlowæ‰§è¡Œå™¨...")
        timeout = self.config.get('execution_timeout', 180)  # é»˜è®¤180ç§’

        # è¯»å–fallbacké…ç½®
        fallback_enabled = self.config.get('reward_system', {}).get('fallback', True)

        self.executor = AFlowExecutor(
            llm_config_path=self.config['aflow_config_path'],
            timeout=timeout,
            operator_enhancer=self.operator_enhancer,  # ä¼ é€’Layer 2å¢å¼ºå™¨
            enable_fallback=fallback_enabled  # ä¼ é€’fallbacké…ç½®
        )
        print(f"  æ‰§è¡Œè¶…æ—¶: {timeout}ç§’")
        print(f"  Fallbackæœºåˆ¶: {'å¯ç”¨' if fallback_enabled else 'ç¦ç”¨'}")

        # 8. å¥–åŠ±è®¡ç®—å™¨ - âœ¨ PHASE 1: NEW 5-tier reward system
        print("\nğŸ¯ åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨ (5-Tier System V2)...")
        use_llm_judge = False  # Set to True if OpenAI API key available
        if os.getenv("OPENAI_API_KEY"):
            use_llm_judge = True
            print("  âœ… LLM Judge enabled (gpt-4o-mini)")
        else:
            print("  âš ï¸  LLM Judge disabled (OPENAI_API_KEY not found)")

        self.reward_computer = RewardComputer(
            use_answer_extractor=True,  # âœ¨ Use enhanced 6-level extraction
            use_llm_judge=use_llm_judge,
            llm_config={
                "base_url": "https://api.openai.com/v1",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "model_name": "gpt-4o-mini"
            } if use_llm_judge else None
        )

        # 9. ä¼˜åŒ–å™¨
        print("\nğŸ”¬ åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 0.01)
        )

    def _load_rl_model(self):
        """åŠ è½½RLæ¨¡å‹ï¼ˆQwen2.5-7B + LoRAï¼‰"""
        device = f"cuda:{self.config['device_mapping'][0]}"

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['base_model'],
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½åŸºåº§æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config['base_model'],
            torch_dtype=torch.bfloat16 if self.config.get('bf16') else torch.float16,
            device_map={"": device},
            trust_remote_code=True
        )

        # âœ… FUNDAMENTAL FIX: Enable gradient checkpointing
        # Trade compute for memory - reduces peak memory by ~40-50%
        if self.config.get('use_gradient_checkpointing', True):
            self.model.gradient_checkpointing_enable()
            print("âœ… Gradient checkpointing enabled (trade compute for memory)")

        # åº”ç”¨LoRA
        if self.config.get('use_lora', True):
            lora_config = LoraConfig(
                r=self.config['lora_rank'],
                lora_alpha=self.config['lora_alpha'],
                target_modules=self.config['lora_target_modules'].split(','),
                lora_dropout=self.config['lora_dropout'],
                bias="none",
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(self.model, lora_config)

            print(f"âœ… LoRAåº”ç”¨å®Œæˆ")
            self.model.print_trainable_parameters()

        # âœ¨ Log GPU memory after model loading
        self._log_gpu_memory("Model Loaded")

    async def _process_sample_batch_parallel(self, batch, num_sequences, current_temp):
        """
        ğŸš€ Performance Fix: Parallel processing of workflow generation and execution
        Processes multiple samples concurrently using asyncio.gather
        """
        import asyncio
        from tqdm import tqdm

        # Create a semaphore to limit concurrent API calls (avoid rate limiting)
        semaphore = asyncio.Semaphore(8)  # Max 8 concurrent workflows

        async def process_single_sample_with_semaphore(sample):
            async with semaphore:
                return await self._process_single_sample(sample, num_sequences, current_temp)

        # Process all samples in parallel
        print(f"\nğŸš€ Parallel processing {len(batch)} samples with {num_sequences} sequences each")
        results = await asyncio.gather(
            *[process_single_sample_with_semaphore(sample) for sample in batch],
            return_exceptions=True
        )

        # Collect successful results and handle exceptions
        all_workflows = []
        all_answers = []
        all_rewards = []
        all_log_probs = []
        all_problem_types = []
        all_ground_truths = []

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âš ï¸ Sample {i} failed: {result}")
                continue

            workflows, answers, rewards, log_probs, problem_types, ground_truths = result
            all_workflows.extend(workflows)
            all_answers.extend(answers)
            all_rewards.extend(rewards)
            all_log_probs.extend(log_probs)
            all_problem_types.extend(problem_types)
            all_ground_truths.extend(ground_truths)

        return all_workflows, all_answers, all_rewards, all_log_probs, all_problem_types, all_ground_truths

    async def _process_single_sample(self, sample, num_sequences, current_temp):
        """Process a single sample and generate multiple workflows"""
        problem = sample['problem']
        ground_truth = sample['ground_truth']
        problem_type = sample['problem_type']

        # Storage for this sample's workflows
        workflows = []
        answers = []
        rewards = []
        log_probs = []
        problem_types = []
        ground_truths = []

        # Process all sequences for this sample in parallel
        async def process_single_sequence(i):
            # Build dynamic prompt if enabled
            custom_prompt = None
            if self.use_dynamic_prompts:
                custom_prompt = self.prompt_optimizer.build_dynamic_prompt(
                    problem=problem,
                    problem_type=problem_type
                )

            # Generate workflow
            result = self.generator.generate_workflow(
                problem=problem,
                problem_type=problem_type,
                temperature=current_temp,
                custom_prompt=custom_prompt
            )

            workflow_code = result['workflow_code']

            # Compute log probability
            log_prob = await self._compute_log_prob(problem, workflow_code, problem_type)

            # Execute workflow
            try:
                answer, cost, metadata = await self.executor.execute_workflow(
                    workflow_code=workflow_code,
                    problem=problem,
                    problem_type=problem_type,
                    entry_point=sample.get('entry_point', ''),
                    test=sample.get('test', '')
                )

                # Compute reward
                reward_result = self.reward_computer.compute_reward(
                    problem=problem,
                    prediction=answer,
                    ground_truth=ground_truth,
                    problem_type=problem_type,
                    execution_metadata=metadata
                )
                # Extract float reward value from dict
                reward = reward_result.get('reward', 0.0) if isinstance(reward_result, dict) else reward_result

                return workflow_code, answer, reward, log_prob

            except Exception as e:
                print(f"âš ï¸ Workflow execution failed: {e}")
                # Return failure values
                return workflow_code, "", 0.0, log_prob

        # Process all sequences in parallel
        sequence_results = await asyncio.gather(
            *[process_single_sequence(i) for i in range(num_sequences)],
            return_exceptions=True
        )

        # Collect results
        for result in sequence_results:
            if isinstance(result, Exception):
                print(f"âš ï¸ Sequence failed: {result}")
                continue

            workflow, answer, reward, log_prob = result
            workflows.append(workflow)
            answers.append(answer)
            rewards.append(reward)
            log_probs.append(log_prob)
            problem_types.append(problem_type)
            ground_truths.append(ground_truth)

        return workflows, answers, rewards, log_probs, problem_types, ground_truths

    def _log_gpu_memory(self, stage: str):
        """Log current GPU memory usage

        Args:
            stage: Description of current training stage
        """
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3  # GB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            free = total - allocated

            print(f"\nğŸ” GPU Memory [{stage}]:")
            print(f"  ğŸ“Š Allocated: {allocated:.2f} GB")
            print(f"  ğŸ“¦ Reserved: {reserved:.2f} GB")
            print(f"  âœ… Free: {free:.2f} GB")
            print(f"  ğŸ’¾ Total: {total:.2f} GB")
            print(f"  ğŸ“ˆ Usage: {(allocated/total)*100:.1f}%")


    async def train_step(self, step: int) -> Dict:
        """
        å•æ­¥GRPOè®­ç»ƒï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰

        Returns:
            metrics: è®­ç»ƒæŒ‡æ ‡
        """

        # 1. é‡‡æ ·batch
        batch = self.data_manager.sample_batch(
            batch_size=self.config['rollout_batch_size'],
            split="train"
        )

        # ç»Ÿè®¡
        batch_stats = self.data_manager.get_batch_stats(batch)
        print(f"\nğŸ“¦ Batch {step}: {len(batch)} æ ·æœ¬, åˆ†å¸ƒ: {batch_stats}")

        # ä½¿ç”¨å›ºå®štemperatureï¼ˆç®€åŒ–ç‰ˆï¼‰
        current_temp = self.generation_temperature
        print(f"ğŸŒ¡ï¸  Temperature: {current_temp:.3f}")

        # 2. ä¸ºæ¯ä¸ªé—®é¢˜ç”ŸæˆKä¸ªå·¥ä½œæµï¼ˆGRPOç»„ï¼‰
        all_workflows = []
        all_problems = []
        all_answers = []
        all_rewards = []
        all_log_probs = []

        # âœ¨ æ–°å¢ï¼šå‡†ç¡®ç‡ç»Ÿè®¡
        correctness_scores = []  # å­˜å‚¨æ‰€æœ‰æ­£ç¡®æ€§åˆ†æ•°

        num_sequences = self.config['num_return_sequences_in_group']

        # ğŸš€ Performance Fix: Use parallel processing instead of sequential
        print(f"\nğŸš€ Using parallel processing for {len(batch)} samples")

        # Call parallel processing method
        all_workflows, all_answers, all_rewards, all_log_probs, all_problem_types, all_ground_truths = \
            await self._process_sample_batch_parallel(batch, num_sequences, current_temp)

        # Create problems list for backward compatibility
        all_problems = [s['problem'] for s in batch for _ in range(num_sequences)]

        # Calculate correctness scores for metrics
        correctness_scores = [reward for reward in all_rewards]

        # Add samples to experience buffer (if they meet threshold)
        for i, (workflow, answer, reward, problem_type, ground_truth) in enumerate(zip(all_workflows, all_answers, all_rewards, all_problem_types, all_ground_truths)):
            if reward >= self.experience_buffer.reward_threshold:
                sample = {
                    'problem': all_problems[i],
                    'workflow_code': workflow,
                    'answer': answer,
                    'ground_truth': ground_truth,
                    'reward': reward,
                    'correctness_score': reward,
                    'metadata': {'step': step}
                }
                self.experience_buffer.add_sample(sample, problem_type)

        # 3. ç­–ç•¥æ¢¯åº¦æ›´æ–°
        print(f"\nğŸ”„ æ›´æ–°ç­–ç•¥...")
        # âœ¨ Log memory before policy update
        self._log_gpu_memory("Before Policy Update")

        loss, kl_div = await self._update_policy(
            problems=all_problems,
            workflows=all_workflows,
            old_log_probs=all_log_probs,
            advantages=all_rewards,
            problem_types=[s['problem_type'] for s in batch for _ in range(num_sequences)]
        )

        # âœ¨ Log memory after policy update
        self._log_gpu_memory("After Policy Update")

        # 4. æŒ‡æ ‡ - âœ¨ Updated for 5-tier system
        # âœ¨ Threshold: tier 4+ (reward >= 0.7) = success
        num_correct = sum(1 for score in correctness_scores if score >= 0.7)
        num_total = len(correctness_scores)
        accuracy = (num_correct / num_total * 100) if num_total > 0 else 0.0
        avg_correctness = np.mean(correctness_scores) if correctness_scores else 0.0

        # âœ¨ Calculate problem type stats with 5-tier thresholds
        problem_type_stats = {}
        for problem_type in ['math', 'code', 'qa']:
            type_scores = [s for s, p in zip(correctness_scores,
                          [s['problem_type'] for s in batch for _ in range(num_sequences)])
                          if p == problem_type]
            if type_scores:
                # âœ¨ Tier 4+ (>= 0.7) is considered correct
                type_correct = sum(1 for s in type_scores if s >= 0.7)
                type_accuracy = (type_correct / len(type_scores) * 100)
                type_avg = np.mean(type_scores)
                problem_type_stats[problem_type] = {
                    "accuracy": type_accuracy,
                    "avg_score": type_avg,
                    "count": len(type_scores)
                }

        metrics = {
            "step": step,
            "loss": loss,
            "kl_div": kl_div,
            "avg_reward": np.mean(all_rewards),
            "max_reward": np.max(all_rewards),
            "min_reward": np.min(all_rewards),
            "num_samples": len(all_workflows),
            # âœ¨ æ–°å¢å‡†ç¡®ç‡æŒ‡æ ‡
            "accuracy": accuracy,
            "num_correct": num_correct,
            "num_total": num_total,
            "avg_correctness_score": avg_correctness
        }

        # âœ¨ Update logging for 5-tier system
        print(f"\nğŸ¯ å‡†ç¡®ç‡ç»Ÿè®¡ (Tier 4+): {num_correct}/{num_total} = {accuracy:.1f}% (å¹³å‡æ­£ç¡®æ€§è¯„åˆ†: {avg_correctness:.2f}/1.0)")

        # Calculate 5-tier distribution
        tier_dist = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        for score in correctness_scores:
            if score >= 0.95:
                tier_dist[5] += 1
            elif score >= 0.6:
                tier_dist[4] += 1
            elif score >= 0.4:
                tier_dist[3] += 1
            elif score >= 0.2:
                tier_dist[2] += 1
            else:
                tier_dist[1] += 1

        print(f"\nğŸ“Š 5-Tieråˆ†å¸ƒ: ", end="")
        for tier, count in tier_dist.items():
            pct = 100 * count / num_total if num_total > 0 else 0
            print(f"Tier {tier}={count}({pct:.1f}%) ", end="")
        print()

        print(f"\nğŸ“Š é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        for ptype, stats in problem_type_stats.items():
            print(f"  {ptype}: {stats['accuracy']:.1f}% (avg: {stats['avg_score']:.2f}, n={stats['count']})")

        # âœ¨ è¯¦ç»† wandb logging - NEW 5-tier metrics
        wandb_log_data = {
            "train/loss": loss,
            "train/kl_div": kl_div,
            "train/avg_reward": np.mean(all_rewards) if all_rewards else 0,
            "train/max_reward": np.max(all_rewards) if all_rewards else 0,
            "train/min_reward": np.min(all_rewards) if all_rewards else 0,
            "train/accuracy": accuracy,
            "train/avg_correctness_score": avg_correctness,
            "train/num_correct": num_correct,
            "train/num_total": num_total,
            "train/temperature": current_temp,
            "train/step": step,
        }

        # âœ¨ Add 5-tier distribution metrics
        for tier, count in tier_dist.items():
            pct = 100 * count / num_total if num_total > 0 else 0
            wandb_log_data[f"train/tier_{tier}_count"] = count
            wandb_log_data[f"train/tier_{tier}_pct"] = pct

        # æ·»åŠ é—®é¢˜ç±»å‹çš„åˆ†å¸ƒæŒ‡æ ‡
        for ptype, stats in problem_type_stats.items():
            wandb_log_data[f"train/accuracy_{ptype}"] = stats['accuracy']
            wandb_log_data[f"train/avg_score_{ptype}"] = stats['avg_score']
            wandb_log_data[f"train/count_{ptype}"] = stats['count']

        wandb.log(wandb_log_data, step=step)

        return metrics

    async def _compute_log_prob(
        self,
        problem: str,
        workflow_code: str,
        problem_type: str
    ) -> torch.Tensor:
        """è®¡ç®—å·¥ä½œæµçš„logæ¦‚ç‡ï¼ˆæ—§ç­–ç•¥ï¼‰"""

        self.model.eval()

        with torch.no_grad():
            # æ„å»ºå®Œæ•´æ–‡æœ¬
            prompt = self.generator._build_generation_prompt(problem, problem_type)
            full_text = prompt + workflow_code

            # Tokenize
            inputs = self.tokenizer(full_text, return_tensors="pt").to(self.model.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(**inputs, labels=inputs["input_ids"])

            # è´Ÿå¯¹æ•°ä¼¼ç„¶ -> logæ¦‚ç‡
            log_prob = -outputs.loss

            return log_prob.detach().cpu()

    async def _update_policy(
        self,
        problems: List[str],
        workflows: List[str],
        old_log_probs: List[torch.Tensor],
        advantages: List[float],
        problem_types: List[str]
    ) -> Tuple[float, float]:
        """æ›´æ–°ç­–ç•¥ï¼ˆGRPOï¼‰"""

        self.model.train()

        total_loss = 0.0
        total_kl = 0.0
        num_updates = 0

        # âœ… FUNDAMENTAL FIX: Add micro-batching for forward passes
        # Process workflows in small micro-batches to prevent computation graph accumulation
        # This is the ROOT CAUSE fix for CUDA OOM during policy update
        microbatch_size = self.config.get('forward_pass_microbatch_size', 1)  # Default: 1 workflow at a time
        grad_accum_steps = self.config.get('gradient_accumulation_steps', 1)

        # ğŸš€ Performance Fix: Reduced memory cleanup frequency
        # Only cleanup if memory usage is high (>80%) to avoid excessive interruptions
        if torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() > 0.8:
            torch.cuda.empty_cache()
            gc.collect()

        # Process workflows in micro-batches
        for i in range(0, len(workflows), microbatch_size):
            microbatch_end = min(i + microbatch_size, len(workflows))
            microbatch_loss = 0.0
            microbatch_kl = 0.0

            # Process each workflow in the micro-batch
            for j in range(i, microbatch_end):
                problem = problems[j]
                workflow = workflows[j]
                old_log_prob = old_log_probs[j]
                advantage = advantages[j]
                problem_type = problem_types[j]

                # ğŸš€ Performance Fix: Reduced cleanup frequency (every 20 samples instead of 5)
                if j % 20 == 0 and torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() > 0.8:
                    torch.cuda.empty_cache()

                # è®¡ç®—æ–°logæ¦‚ç‡ WITH gradients
                new_log_prob = await self._compute_log_prob_trainable(problem, workflow, problem_type)

                # Compute PPO loss components (all operations keep gradients)
                old_log_prob_device = old_log_prob.to(self.model.device)
                ratio = torch.exp(new_log_prob - old_log_prob_device)

                # PPOè£å‰ª
                clip_range = self.config['clip_range']
                clipped_ratio = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)

                # Advantage tensor (constant, no gradients needed)
                advantage_tensor = torch.tensor(
                    advantage,
                    device=self.model.device,
                    dtype=torch.bfloat16,
                    requires_grad=False  # Advantage is constant
                )

                # PPOè£å‰ªæŸå¤±
                policy_loss = -torch.min(
                    ratio * advantage_tensor,
                    clipped_ratio * advantage_tensor
                )

                # KLæ­£åˆ™åŒ–
                if self.config.get('use_kl_loss'):
                    kl_loss = self.config['kl_loss_coef'] * (
                        new_log_prob - old_log_prob_device
                    ).pow(2)
                else:
                    kl_loss = 0.0

                # æ€»æŸå¤±
                loss = policy_loss + kl_loss

                # ç´¯ç§¯åˆ°micro-batch
                microbatch_loss += loss
                microbatch_kl += kl_loss if isinstance(kl_loss, torch.Tensor) else 0.0

                # Cleanup
                del old_log_prob_device, advantage_tensor, new_log_prob, ratio, clipped_ratio

            # Normalize loss by micro-batch size
            microbatch_loss = microbatch_loss / (microbatch_end - i)

            # âœ… KEY FIX: Backward IMMEDIATELY after each micro-batch
            # This prevents computation graphs from accumulating
            microbatch_loss.backward()

            # Cleanup AFTER backward
            microbatch_loss_value = microbatch_loss.item()
            microbatch_kl_value = microbatch_kl.item() if isinstance(microbatch_kl, torch.Tensor) else microbatch_kl
            del microbatch_loss, microbatch_kl
            torch.cuda.empty_cache()

            total_loss += microbatch_loss_value
            total_kl += microbatch_kl_value
            num_updates += 1

            # ä¼˜åŒ–å™¨æ­¥éª¤ (every grad_accum_steps micro-batches)
            if (num_updates % grad_accum_steps == 0) or (microbatch_end >= len(workflows)):
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.get('max_grad_norm', 1.0))
                self.optimizer.step()
                # Use set_to_none=True to free memory
                self.optimizer.zero_grad(set_to_none=True)
                torch.cuda.empty_cache()

        avg_loss = total_loss / max(num_updates, 1)
        avg_kl = total_kl / max(num_updates, 1)

        return avg_loss, avg_kl

    async def _compute_log_prob_trainable(
        self,
        problem: str,
        workflow_code: str,
        problem_type: str
    ) -> torch.Tensor:
        """è®¡ç®—å·¥ä½œæµçš„logæ¦‚ç‡ï¼ˆæ–°ç­–ç•¥ï¼Œå¯è®­ç»ƒï¼‰

        âœ… FUNDAMENTAL FIX: Proper gradient flow without premature tensor deletion
        - Forward pass builds computation graph
        - Returns log_prob WITH gradients (no .detach())
        - NO premature deletion of inputs/outputs (breaks gradient graph)
        - Let PyTorch handle tensor lifecycle automatically
        """

        # æ„å»ºå®Œæ•´æ–‡æœ¬
        prompt = self.generator._build_generation_prompt(problem, problem_type)
        full_text = prompt + workflow_code

        # Tokenize
        inputs = self.tokenizer(full_text, return_tensors="pt").to(self.model.device)

        # å‰å‘ä¼ æ’­ WITH gradients (needed for backprop)
        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
            outputs = self.model(**inputs, labels=inputs["input_ids"])
            # âœ… CRITICAL: Keep gradients! No .detach()
            log_prob = -outputs.loss

        # âœ… FUNDAMENTAL FIX: DO NOT delete inputs/outputs here!
        # They are still needed by the computation graph for backward()
        # PyTorch will automatically release them after backward() completes

        return log_prob  # Returns tensor WITH gradients

    async def evaluate_on_val_set(self, num_samples: int = 50) -> Dict:
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            num_samples: éªŒè¯æ ·æœ¬æ•°é‡

        Returns:
            éªŒè¯æŒ‡æ ‡å­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"ğŸ§ª éªŒè¯é›†è¯„ä¼° ({num_samples}ä¸ªæ ·æœ¬)")
        print(f"{'='*60}")

        # é‡‡æ ·éªŒè¯é›†
        val_batch = self.data_manager.sample_batch(
            batch_size=num_samples,
            split="val"  # ä½¿ç”¨éªŒè¯é›†
        )

        # ç»Ÿè®¡
        batch_stats = self.data_manager.get_batch_stats(val_batch)
        print(f"ğŸ“¦ éªŒè¯é›†åˆ†å¸ƒ: {batch_stats}")

        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        correctness_scores = []
        total_cost = 0.0
        successful_executions = 0

        for idx, sample in enumerate(tqdm(val_batch, desc="éªŒè¯é›†è¯„ä¼°"), 1):
            problem = sample['problem']
            ground_truth = sample['ground_truth']
            problem_type = sample['problem_type']

            try:
                # ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆworkflowï¼ˆä½¿ç”¨åŠ¨æ€æç¤ºè¯ï¼‰
                custom_prompt = None
                if self.use_dynamic_prompts:
                    custom_prompt = self.prompt_optimizer.build_dynamic_prompt(
                        problem=problem,
                        problem_type=problem_type
                    )

                result = self.generator.generate_workflow(
                    problem=problem,
                    problem_type=problem_type,
                    temperature=self.config['generation_config']['temperature'],
                    custom_prompt=custom_prompt
                )

                workflow_code = result['workflow_code']

                # æ‰§è¡Œworkflow
                answer, cost, metadata = await self.executor.execute_workflow(
                    workflow_code=workflow_code,
                    problem=problem,
                    problem_type=problem_type,
                    entry_point=sample.get('entry_point', ''),
                    test=sample.get('test', '')  # NEW: pass test cases for HumanEval
                )

                # è®¡ç®—æ­£ç¡®æ€§
                if metadata['success']:
                    correctness_result = self.reward_computer.compute_reward(
                        problem=problem,
                        prediction=answer,
                        ground_truth=ground_truth,
                        problem_type=problem_type,
                        execution_metadata={'success': True}
                    )
                    correctness = correctness_result.get('reward', 0.0) * 10.0  # Convert [0, 1] to [0, 10]
                    correctness_scores.append(correctness)
                    total_cost += cost
                    successful_executions += 1

                    is_correct = correctness >= 5.0
                    status_icon = "âœ…" if is_correct else "âŒ"
                    if idx <= 5:  # åªæ‰“å°å‰5ä¸ªæ ·æœ¬çš„è¯¦æƒ…
                        print(f"  {status_icon} [{idx}/{num_samples}] æ­£ç¡®æ€§: {correctness:.1f}/10.0")
                else:
                    correctness_scores.append(0.0)
                    if idx <= 5:
                        print(f"  âŒ [{idx}/{num_samples}] æ‰§è¡Œå¤±è´¥")

            except Exception as e:
                print(f"  âš ï¸  [{idx}/{num_samples}] é”™è¯¯: {type(e).__name__}")
                correctness_scores.append(0.0)

        # è®¡ç®—æŒ‡æ ‡
        num_correct = sum(1 for score in correctness_scores if score >= 5.0)
        val_accuracy = (num_correct / num_samples * 100) if num_samples > 0 else 0.0
        avg_correctness = np.mean(correctness_scores) if correctness_scores else 0.0
        avg_cost = total_cost / successful_executions if successful_executions > 0 else 0.0
        success_rate = (successful_executions / num_samples * 100) if num_samples > 0 else 0.0

        metrics = {
            "val_accuracy": val_accuracy,
            "val_num_correct": num_correct,
            "val_num_total": num_samples,
            "val_avg_correctness": avg_correctness,
            "val_avg_cost": avg_cost,
            "val_success_rate": success_rate
        }

        print(f"\nğŸ“Š éªŒè¯é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {num_correct}/{num_samples} = {val_accuracy:.1f}%")
        print(f"  å¹³å‡æ­£ç¡®æ€§: {avg_correctness:.2f}/10.0")
        print(f"  æ‰§è¡ŒæˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  å¹³å‡æˆæœ¬: ${avg_cost:.4f}")
        print(f"{'='*60}\n")

        return metrics

    async def train(self):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print("ğŸ“ å¼€å§‹GRPOè®­ç»ƒ")
        print("=" * 60)

        max_steps = self.config['max_steps']
        save_every = self.config.get('save_every', 50)
        log_every = self.config.get('log_every', 5)
        eval_every = self.config.get('eval_every', 10)  # æ¯10æ­¥éªŒè¯ä¸€æ¬¡
        val_samples = self.config.get('val_samples', 50)  # éªŒè¯é›†æ ·æœ¬æ•°

        for step in range(1, max_steps + 1):
            print(f"\n{'=' * 60}")
            print(f"ğŸ“ Step {step}/{max_steps}")
            print(f"{'=' * 60}")

            # è®­ç»ƒæ­¥éª¤
            metrics = await self.train_step(step)

            # æ—¥å¿—
            if step % log_every == 0:
                print(f"\nğŸ“Š Metrics:")
                for key, value in metrics.items():
                    print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")

                # è®°å½•åˆ°wandb
                wandb.log(metrics, step=step)

            # ğŸ§ª éªŒè¯é›†è¯„ä¼°ï¼ˆæ¯Næ­¥ï¼‰
            if eval_every > 0 and step % eval_every == 0:
                val_metrics = await self.evaluate_on_val_set(num_samples=val_samples)

                # åˆå¹¶éªŒè¯æŒ‡æ ‡åˆ°è®­ç»ƒæŒ‡æ ‡
                metrics.update(val_metrics)

                # è®°å½•éªŒè¯æŒ‡æ ‡åˆ°wandb
                wandb.log(val_metrics, step=step)

                print(f"âœ… éªŒè¯é›†è¯„ä¼°å®Œæˆ (Step {step})")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if step % save_every == 0:
                self.save_checkpoint(step)

        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆ")
        print("=" * 60)

    def save_checkpoint(self, step: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path(self.config['output_dir']) / f"step_{step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜LoRAæƒé‡
        self.model.save_pretrained(checkpoint_dir)

        # ğŸ’¾ ä¿å­˜ExperienceBuffer
        self.experience_buffer.save(step=step)

        # ğŸ“Š æ‰“å°ExperienceBufferç»Ÿè®¡ä¿¡æ¯
        buffer_stats = self.experience_buffer.get_stats()
        print(f"\nğŸ“š ExperienceBufferç»Ÿè®¡:")
        for problem_type, stats in buffer_stats.items():
            if stats['count'] > 0:
                print(f"  {problem_type}: {stats['count']}æ ·æœ¬, "
                      f"å¹³å‡å¥–åŠ±={stats['avg_reward']:.2f}, "
                      f"æœ€é«˜å¥–åŠ±={stats['max_reward']:.2f}, "
                      f"å¹³å‡æ­£ç¡®æ€§={stats['avg_correctness']:.2f}")

        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")


async def main():
    """ä¸»å‡½æ•°"""
    trainer = GRPOTrainer(config_path="config/training.yaml")
    await trainer.train()


if __name__ == "__main__":
    asyncio.run(main())
