#!/usr/bin/env python3
"""
AFlowæ‰§è¡Œé€‚é…å™¨ - æ‰§è¡ŒRLç”Ÿæˆçš„å·¥ä½œæµ
"""
import sys
import os
import tempfile
import importlib.util
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import asyncio
import time

# å¯¼å…¥å·¥ä½œæµéªŒè¯å™¨ã€å“åº”æ ‡å‡†åŒ–å™¨å’ŒSymPyä¿®å¤å™¨
try:
    from .workflow_validator_v2 import WorkflowValidatorV2
    from .response_standardizer import ResponseStandardizer
    from .sympy_code_fixer import SymPyCodeFixer
except ImportError:
    from workflow_validator_v2 import WorkflowValidatorV2
    from response_standardizer import ResponseStandardizer
    from sympy_code_fixer import SymPyCodeFixer

# æ·»åŠ AFlowåˆ°è·¯å¾„ï¼ˆæ·»åŠ å¤šä¸ªå¯èƒ½éœ€è¦çš„è·¯å¾„ï¼‰
aflow_path = os.getenv("AFLOW_PATH", "../AFlow")
sys.path.insert(0, aflow_path)
sys.path.insert(0, os.path.join(aflow_path, 'workspace'))

# å¯¼å…¥AFlowç»„ä»¶
from scripts.async_llm import create_llm_instance, LLMsConfig
from scripts import operators as operator_module


class AsyncOpenAILLMWrapper:
    """
    OpenAI å¼‚æ­¥LLMåŒ…è£…å™¨ - ä½œä¸ºFallbackå¤‡ç”¨LLM

    å®ç°AsyncLLMæ¥å£ï¼Œä¸ä¸»LLMå…¼å®¹çš„å¼‚æ­¥æ¥å£
    å½“ä¸»LLMåˆå§‹åŒ–å¤±è´¥æ—¶ä½¿ç”¨ä½œä¸ºTier 2å¤‡ç”¨æ–¹æ¡ˆ
    """

    def __init__(self, api_key: str, model: str = "gpt-4o-mini",
                 base_url: str = "https://api.openai.com/v1",
                 temperature: float = 0.0, top_p: float = 1.0):
        """
        åˆå§‹åŒ–OpenAIå¼‚æ­¥å®¢æˆ·ç«¯

        Args:
            api_key: OpenAI APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°
        """
        try:
            from openai import AsyncOpenAI
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…openaiåº“: pip install openai")

        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.temperature = temperature
        self.top_p = top_p

        # åˆå§‹åŒ–OpenAIå¼‚æ­¥å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )

        # è·Ÿè¸ªä½¿ç”¨ç»Ÿè®¡
        self._total_tokens = 0
        self._total_input_tokens = 0
        self._total_output_tokens = 0
        self._total_cost = 0.0
        self._call_count = 0

    async def __call__(self, prompt: str, **kwargs) -> str:
        """
        è°ƒç”¨OpenAI APIç”Ÿæˆå“åº”

        Args:
            prompt: è¾“å…¥æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆè¢«å¿½ç•¥ä»¥ä¿æŒæ¥å£å…¼å®¹ï¼‰

        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                top_p=self.top_p,
                max_tokens=2048
            )

            # æå–å“åº”
            answer = response.choices[0].message.content

            # è·Ÿè¸ªä½¿ç”¨ç»Ÿè®¡
            if hasattr(response, 'usage') and response.usage:
                self._total_input_tokens += response.usage.prompt_tokens
                self._total_output_tokens += response.usage.completion_tokens
                self._total_tokens += response.usage.total_tokens

                # ä¼°ç®—æˆæœ¬ï¼ˆgpt-4o-mini: $0.15/M input, $0.60/M outputï¼‰
                input_cost = (response.usage.prompt_tokens / 1_000_000) * 0.15
                output_cost = (response.usage.completion_tokens / 1_000_000) * 0.60
                cost = input_cost + output_cost
                self._total_cost += cost

            self._call_count += 1

            return answer

        except Exception as e:
            print(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            raise

    async def call_with_format(self, prompt: str, formatter=None, **kwargs) -> str:
        """
        å¸¦æ ¼å¼åŒ–çš„è°ƒç”¨ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Args:
            prompt: è¾“å…¥æç¤ºè¯
            formatter: æ ¼å¼åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        response = await self(prompt, **kwargs)

        if formatter and callable(formatter):
            try:
                return formatter(response)
            except Exception as e:
                print(f"âš ï¸ æ ¼å¼åŒ–å¤±è´¥: {e}")
                return response

        return response

    def get_usage_summary(self) -> Dict[str, Any]:
        """
        è·å–ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦

        Returns:
            åŒ…å«tokenå’Œæˆæœ¬ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "total_tokens": self._total_tokens,
            "total_input_tokens": self._total_input_tokens,
            "total_output_tokens": self._total_output_tokens,
            "total_cost": self._total_cost,
            "call_count": self._call_count
        }

    def reset_usage(self):
        """é‡ç½®ä½¿ç”¨ç»Ÿè®¡"""
        self._total_tokens = 0
        self._total_input_tokens = 0
        self._total_output_tokens = 0
        self._total_cost = 0.0
        self._call_count = 0


class AFlowExecutor:
    """æ‰§è¡ŒRLç”Ÿæˆçš„å·¥ä½œæµï¼Œä½¿ç”¨AFlowçš„ç®—å­"""

    def __init__(
        self,
        llm_config_path: str = "config/aflow_llm.yaml",
        llm_model_name: str = "gpt-4o-mini",  # ä½¿ç”¨OpenAIå®˜æ–¹gpt-4o-mini
        timeout: int = 300,
        operator_enhancer: Optional[Any] = None,
        enable_fallback: bool = True  # å¯ç”¨Fallbackæœºåˆ¶
    ):
        """
        Args:
            llm_config_path: AFlow LLMé…ç½®æ–‡ä»¶è·¯å¾„
            llm_model_name: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            timeout: æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            operator_enhancer: Layer 2 operatoræç¤ºè¯å¢å¼ºå™¨ï¼ˆå¯é€‰ï¼‰
            enable_fallback: æ˜¯å¦å¯ç”¨Fallbackæœºåˆ¶
        """
        self.llm_config_path = Path(llm_config_path)
        self.llm_model_name = llm_model_name
        self.timeout = timeout
        self.operator_enhancer = operator_enhancer
        self.enable_fallback = enable_fallback
        self.validator_v2 = WorkflowValidatorV2()  # ç»Ÿä¸€éªŒè¯å™¨ï¼ˆæ”¯æŒreactive patching + TASK_PROMPTæå–ï¼‰
        self.standardizer = ResponseStandardizer()  # å“åº”æ ‡å‡†åŒ–å™¨
        self.sympy_fixer = SymPyCodeFixer()  # SymPyä¿®å¤å™¨

        # åŠ è½½LLMé…ç½®
        self._load_llm_config()

        print(f"âœ… AFlowæ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  LLMæ¨¡å‹: {llm_model_name}")
        print(f"  è¶…æ—¶: {timeout}ç§’")
        if operator_enhancer is not None:
            print(f"  Layer 2å¢å¼º: å¯ç”¨")

    def _load_llm_config(self):
        """åŠ è½½LLMé…ç½®"""
        try:
            # è®¾ç½®é…ç½®è·¯å¾„
            abs_config_path = self.llm_config_path.absolute()

            # è¯»å–YAMLé…ç½®æ–‡ä»¶
            import yaml
            with open(abs_config_path, 'r') as f:
                yaml_data = yaml.safe_load(f)

            # LLMsConfigæœŸæœ›çš„æ˜¯modelså­—å…¸
            models_config = yaml_data.get('models', {})

            # ä¸ºæœ¬åœ°LLMæœåŠ¡ç¦ç”¨ä»£ç†ï¼ˆå¦‚æœä½¿ç”¨æœ¬åœ°æœåŠ¡ï¼‰
            import os
            model_config = models_config.get(self.llm_model_name, {})
            if 'localhost' in str(model_config.get('base_url', '')) or \
               '127.0.0.1' in str(model_config.get('base_url', '')):
                os.environ['NO_PROXY'] = 'localhost,127.0.0.1'
                os.environ['no_proxy'] = 'localhost,127.0.0.1'
                print("  ğŸ“Œ è®¾ç½® NO_PROXY=localhost,127.0.0.1 (æœ¬åœ°LLMæœåŠ¡æ— éœ€ä»£ç†)")

            # ç›´æ¥åŠ è½½é…ç½®
            from scripts.async_llm import LLMsConfig
            self.llm_configs = LLMsConfig(models_config)

            print(f"âœ… åŠ è½½LLMé…ç½®: {abs_config_path}")

        except Exception as e:
            print(f"âš ï¸  åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
            print(f"  å°†ä½¿ç”¨ LLMsConfig.default()")
            # ä½¿ç”¨é»˜è®¤é…ç½®è€Œä¸æ˜¯ None
            from scripts.async_llm import LLMsConfig
            try:
                self.llm_configs = LLMsConfig.default()
                print(f"âœ… æˆåŠŸåŠ è½½é»˜è®¤LLMé…ç½®")
            except Exception as e2:
                print(f"  é»˜è®¤é…ç½®ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                # æœ€åçš„é™çº§æ–¹æ¡ˆï¼šè®¾ä¸º Noneï¼Œåç»­ç”¨å­—ç¬¦ä¸²
                self.llm_configs = None

    def _detect_code_leakage(self, answer: str, problem_type: str) -> bool:
        """
        æ£€æµ‹ Programmer ç®—å­æ˜¯å¦è¿”å›äº†æºä»£ç è€Œä¸æ˜¯æ‰§è¡Œç»“æœï¼ˆæ¥è‡ªå‚è€ƒé¡¹ç›®ï¼‰

        æŸäº›æƒ…å†µä¸‹ï¼ŒProgrammer ç®—å­å¯èƒ½è¿”å›æºä»£ç è€Œéæ‰§è¡Œç»“æœï¼Œå¦‚ï¼š
        - def function_name(...):
        - class ClassName:
        - import module_name
        - async def function_name

        å¯¹äºä»£ç é—®é¢˜ï¼Œè¿™ç§æƒ…å†µæ„å‘³ç€ç®—å­æ²¡æœ‰æ­£ç¡®æ‰§è¡Œä»£ç ã€‚

        Args:
            answer: è¿”å›çš„ç­”æ¡ˆ
            problem_type: é—®é¢˜ç±»å‹ï¼ˆ'code', 'math', 'qa'ï¼‰

        Returns:
            å¦‚æœæ£€æµ‹åˆ°ä»£ç æ³„éœ²è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        if not isinstance(answer, str) or problem_type != "code":
            return False

        # ä»£ç æ³„éœ²çš„å…¸å‹æ¨¡å¼
        code_patterns = [
            "def ",
            "class ",
            "import ",
            "from ",
            "async def ",
            "@",  # è£…é¥°å™¨
            "try:",
            "except",
            "while ",
            "for ",
        ]

        answer_stripped = answer.strip()

        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä»¥ä»£ç æ¨¡å¼å¼€å¤´
        for pattern in code_patterns:
            if answer_stripped.startswith(pattern):
                return True

        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«å¤šè¡Œä»£ç ï¼ˆç¼©è¿›ï¼‰
        lines = answer_stripped.split('\n')
        if len(lines) > 1:
            # è®¡ç®—æœ‰ç¼©è¿›çš„è¡Œæ•°ï¼ˆè¡¨ç¤ºä»£ç å—ï¼‰
            indented_lines = sum(1 for line in lines if line and line[0] in (' ', '\t'))
            if indented_lines > len(lines) * 0.3:  # è¶…è¿‡ 30% çš„è¡Œæœ‰ç¼©è¿›
                return True

        return False

    def _clean_answer(self, answer: str) -> str:
        """
        æ¸…ç†ç­”æ¡ˆä¸­çš„æ— æ•ˆæ¨¡å¼ï¼ˆæ¥è‡ªå‚è€ƒé¡¹ç›®ï¼‰

        æŸäº›LLMå¯èƒ½åœ¨ç­”æ¡ˆå‰æ·»åŠ è§£é‡Šæ€§æ–‡æœ¬ï¼Œå¦‚ï¼š
        - "Based on the feedback, ..."
        - "Revised Solution: ..."
        - "Here's the solution: ..."

        è¿™äº›æ¨¡å¼ä¼šæ±¡æŸ“ç­”æ¡ˆï¼Œå½±å“è¯„ä¼°å‡†ç¡®æ€§ã€‚

        Args:
            answer: åŸå§‹ç­”æ¡ˆå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„ç­”æ¡ˆ
        """
        if not isinstance(answer, str):
            return answer

        invalid_patterns = [
            "Based on the feedback",
            "Based on the previous",
            "Revised Solution:",
            "Here's the solution:",
            "Here is the solution:",
            "The solution is:",
            "Here's the revised",
            "Here is the revised",
            "Following the feedback",
            "According to the feedback",
            "Taking the feedback",
            "Let me revise:",
            "Let me reconsider:",
        ]

        for pattern in invalid_patterns:
            if answer.startswith(pattern):
                # æ‰¾åˆ°æ¨¡å¼åçš„å†…å®¹
                idx = answer.find(pattern)
                # è·³è¿‡æ¨¡å¼å’Œå¯èƒ½çš„å†’å·/æ¢è¡Œ
                rest = answer[idx + len(pattern):].lstrip(':').strip()
                if rest:
                    print(f"  ğŸ§¹ æ¸…ç†ç­”æ¡ˆä¸­çš„æ— æ•ˆå‰ç¼€: '{pattern}'")
                    return rest

        return answer

    def _check_operator_problem_type_mismatch(
        self,
        workflow_code: str,
        problem_type: str
    ) -> Optional[str]:
        """
        æ£€æŸ¥å·¥ä½œæµä¸­ä½¿ç”¨çš„æ“ä½œç¬¦æ˜¯å¦ä¸ problem_type åŒ¹é…

        Returns:
            å¦‚æœå­˜åœ¨ä¸åŒ¹é…ï¼Œè¿”å›é”™è¯¯æ¶ˆæ¯ï¼›å¦åˆ™è¿”å› None
        """
        code_lower = workflow_code.lower()

        # æ£€æŸ¥ problem_type ä¸æ“ä½œç¬¦çš„åŒ¹é…
        if problem_type == "math":
            # Math é—®é¢˜ä¸åº”è¯¥ä½¿ç”¨ Test æˆ– Programmer
            if "self.test(" in code_lower or "await self.test(" in code_lower:
                return (
                    "âŒ MATH problem uses Test operator!\n"
                    "   Math problems don't have automated test cases.\n"
                    "   This will cause NoneType errors when Test tries to look up test cases.\n"
                    "   Use only: Custom, AnswerGenerate, Review, Revise, ScEnsemble"
                )
            if "self.programmer(" in code_lower or "await self.programmer(" in code_lower:
                return (
                    "âŒ MATH problem uses Programmer operator!\n"
                    "   Math is not code-related, don't use Programmer.\n"
                    "   Use only: Custom, AnswerGenerate, Review, Revise, ScEnsemble"
                )

        elif problem_type == "qa":
            # QA é—®é¢˜ä¸åº”è¯¥ä½¿ç”¨ Test æˆ– Programmer
            if "self.test(" in code_lower or "await self.test(" in code_lower:
                return (
                    "âŒ QA problem uses Test operator!\n"
                    "   QA problems don't have automated test cases.\n"
                    "   This will cause NoneType errors when Test tries to look up test cases.\n"
                    "   Use only: Custom, AnswerGenerate, Review, Revise, ScEnsemble"
                )
            if "self.programmer(" in code_lower or "await self.programmer(" in code_lower:
                return (
                    "âŒ QA problem uses Programmer operator!\n"
                    "   QA is not code-related, don't use Programmer.\n"
                    "   Use only: Custom, AnswerGenerate, Review, Revise, ScEnsemble"
                )

        elif problem_type == "code":
            # Codeé—®é¢˜ï¼šä¸å¼ºåˆ¶è¦æ±‚Test operator
            # åŸå› ï¼šTest operatorè™½ç„¶æ¨èï¼Œä½†ä¸æ˜¯å¿…éœ€çš„ï¼ˆCustomä¹Ÿå¯ä»¥ç”Ÿæˆä»£ç ï¼‰
            pass

        return None

    def validate_operator_output(self, output: Any, operator_name: str) -> Dict:
        """
        éªŒè¯å¹¶æ ‡å‡†åŒ–ç®—å­è¾“å‡ºæ ¼å¼ï¼ˆä½¿ç”¨ResponseStandardizerï¼‰

        Args:
            output: ç®—å­çš„åŸå§‹è¾“å‡º
            operator_name: ç®—å­åç§°

        Returns:
            æ ‡å‡†åŒ–åçš„è¾“å‡ºå­—å…¸
        """
        # ä½¿ç”¨ResponseStandardizerè¿›è¡Œæ ‡å‡†åŒ–
        standardized = self.standardizer.standardize(output, operator_name)

        # ä¿æŒå‘åå…¼å®¹ï¼ŒåŒæ—¶è¿”å›åŸå§‹å­—æ®µå’Œæ ‡å‡†åŒ–å­—æ®µ
        if isinstance(output, dict):
            result = output.copy()
            result.update({
                '__standardized__': standardized,
                # ç¡®ä¿å…³é”®å­—æ®µå­˜åœ¨
                'response': standardized['content'],
                'success': standardized['success'],
                'error': standardized.get('error')
            })
            return result
        else:
            return standardized

    async def execute_workflow(
        self,
        workflow_code: str,
        problem: str,
        problem_type: str = "math",
        **kwargs
    ) -> Tuple[Any, float, Dict]:
        """
        æ‰§è¡Œå·¥ä½œæµ

        Args:
            workflow_code: RLæ¨¡å‹ç”Ÿæˆçš„Workflowç±»ä»£ç 
            problem: é—®é¢˜æ–‡æœ¬
            problem_type: é—®é¢˜ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚entry_point for codeï¼‰

        Returns:
            (answer, cost, metadata)
        """

        start_time = time.time()

        # 0. æ£€æŸ¥ operator-problem type åŒ¹é…ï¼ˆæ–¹æ¡ˆBï¼šè½¯å­¦ä¹ è€Œéç¡¬é˜»æ­¢ï¼‰
        # CHANGE: ä¸å†ç¡¬æ‹’ç»ï¼Œè€Œæ˜¯æ ‡è®°ä¸ºmetadataï¼Œè®©RLé€šè¿‡rewardå­¦ä¹ 
        operator_type_mismatch = self._check_operator_problem_type_mismatch(
            workflow_code, problem_type
        )
        mismatch_detected = operator_type_mismatch is not None
        mismatch_details = operator_type_mismatch if operator_type_mismatch else None

        if mismatch_detected:
            # è®°å½•è­¦å‘Šä½†ç»§ç»­æ‰§è¡Œï¼ˆå…è®¸æ¨¡å‹æ¢ç´¢ï¼‰
            print(f"âš ï¸  Operator-problem type mismatch detected:")
            print(f"   {mismatch_details}")
            print(f"   â†’ Will mark in metadata and apply penalty in reward")
            # ä¸raiseå¼‚å¸¸ - ç»§ç»­æ‰§è¡Œworkflowï¼Œç¨ååœ¨metadataä¸­æ ‡è®°

        # 1. éªŒè¯å·¥ä½œæµä»£ç 
        # ä½¿ç”¨WorkflowValidatorV2è¿›è¡Œreactive patchingéªŒè¯å’Œä¿®å¤
        print(f"  1ï¸âƒ£ éªŒè¯å’Œä¿®å¤å·¥ä½œæµä»£ç ...")
        fixed_code, is_valid, error_msg, fixes_applied = \
            self.validator_v2.validate_and_fix_workflow(workflow_code, problem_type)

        workflow_code = fixed_code

        # è®°å½•ä¿®å¤å’Œé”™è¯¯åˆ°å…ƒæ•°æ®ï¼ˆç»™GRPOå­¦ä¹ ï¼‰
        metadata = kwargs.get('metadata', {})
        if fixes_applied:
            metadata['auto_fixes_applied'] = fixes_applied
            print(f"  âœ… åº”ç”¨äº†ä»¥ä¸‹ä¿®å¤: {fixes_applied}")

        if not is_valid:
            # ä¿®å¤åä»ç„¶æ— æ•ˆï¼Œæ‰è€ƒè™‘é™çº§
            print(f"  âš ï¸  å·¥ä½œæµä»£ç ä¿®å¤åä»ç„¶æ— æ•ˆ: {error_msg}")

            if self.enable_fallback:
                print(f"  ä½¿ç”¨Fallbackå·¥ä½œæµ")
                # æ ‡è®°è¿™æ˜¯å› ä¸ºéªŒè¯å¤±è´¥è€Œæ‰§è¡Œçš„ Fallback
                answer, cost, fb_metadata = await self._execute_fallback_workflow(problem, problem_type, **kwargs)

                # åˆå¹¶å…ƒæ•°æ®
                metadata['validation_failed'] = True
                metadata['validation_error'] = msg
                metadata['needed_fallback'] = True  # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€keyåç§°ï¼ˆä»'fallback_used'æ”¹ä¸º'needed_fallback'ï¼‰
                metadata.update(fb_metadata)

                return answer, cost, metadata
            else:
                # Fallbackç¦ç”¨ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError(f"å·¥ä½œæµä»£ç æ— æ•ˆä¸”Fallbackå·²ç¦ç”¨: {msg}")

        # ä¿®å¤åæœ‰æ•ˆï¼Œç»§ç»­æ‰§è¡Œï¼ˆä¸é™çº§ï¼ï¼‰
        print(f"  âœ… ä»£ç éªŒè¯é€šè¿‡ï¼ˆ{len(fixes_applied)}ä¸ªä¿®å¤ï¼‰")

        # 2. ä¿®å¤SymPyå…¼å®¹æ€§é—®é¢˜ï¼ˆé’ˆå¯¹Codeç±»å‹ï¼‰
        if problem_type == "code" or 'sympy' in workflow_code.lower():
            fixed_code, was_modified, fixes = self.sympy_fixer.fix_code(workflow_code)
            if was_modified:
                print(f"ğŸ”§ SymPyä»£ç ä¿®å¤: {', '.join(fixes)}")
                workflow_code = fixed_code

        try:
            # åˆ›å»ºä¸´æ—¶å·¥ä½œæµæ¨¡å—
            workflow_class = self._create_workflow_class(workflow_code, problem_type)

            # å®ä¾‹åŒ–å·¥ä½œæµ
            llm_config = self._get_llm_config()

            # ç¡®ä¿ llm_config ä¸æ˜¯ None
            if llm_config is None:
                print(f"âš ï¸  llm_config ä¸º Noneï¼Œé™çº§ä¸ºå­—ç¬¦ä¸²: {self.llm_model_name}")
                llm_config = self.llm_model_name

            try:
                workflow = workflow_class(
                    name="rl_generated_workflow",
                    llm_config=llm_config,
                    dataset=problem_type
                )
            except Exception as e:
                # å·¥ä½œæµå®ä¾‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨fallback
                print(f"âš ï¸  å·¥ä½œæµå®ä¾‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                print(f"  ä½¿ç”¨fallbackå·¥ä½œæµ")
                fallback_class = self._get_fallback_workflow_class(problem_type)
                workflow = fallback_class(
                    name="fallback_workflow",
                    llm_config=llm_config,
                    dataset=problem_type
                )
                # ğŸ”§ ä¿®å¤ï¼šè®°å½•å®ä¾‹åŒ–å¤±è´¥æ ‡è®°ï¼Œåç»­å¦‚æœæˆåŠŸéœ€è¦è®°å½•needed_fallback
                metadata['had_instantiation_error'] = True

            # æ‰§è¡Œï¼ˆå¸¦è¶…æ—¶ï¼‰
            # æ ¹æœ¬æ€§ä¿®å¤ï¼šæ™ºèƒ½3çº§å‚æ•°é™çº§ç­–ç•¥ï¼ˆå‚è€ƒé¡¹ç›®æ–¹æ¡ˆï¼‰
            try:
                if problem_type == "code":
                    # ç­–ç•¥1: å°è¯•ä¼ å…¥æ‰€æœ‰3ä¸ªå‚æ•° (problem, entry_point, test)
                    if "entry_point" in kwargs and "test" in kwargs:
                        try:
                            print(f"  ğŸ“‹ å°è¯•3å‚æ•°æ¨¡å¼: (problem, entry_point, test)")
                            result = await asyncio.wait_for(
                                workflow(problem, kwargs["entry_point"], kwargs["test"]),
                                timeout=self.timeout
                            )
                            print(f"  âœ… 3å‚æ•°æ¨¡å¼æˆåŠŸ")
                        except TypeError as e:
                            # ç­–ç•¥2: é™çº§åˆ°2å‚æ•° (problem, entry_point)
                            if "positional argument" in str(e) or "missing" in str(e).lower():
                                print(f"  âš ï¸  3å‚æ•°å¤±è´¥ï¼Œå°è¯•2å‚æ•°æ¨¡å¼: (problem, entry_point)")
                                try:
                                    result = await asyncio.wait_for(
                                        workflow(problem, kwargs["entry_point"]),
                                        timeout=self.timeout
                                    )
                                    print(f"  âœ… 2å‚æ•°æ¨¡å¼æˆåŠŸ")
                                except TypeError as e2:
                                    # ç­–ç•¥3: é™çº§åˆ°1å‚æ•° (problem only)
                                    if "positional argument" in str(e2) or "missing" in str(e2).lower():
                                        print(f"  âš ï¸  2å‚æ•°å¤±è´¥ï¼Œé™çº§åˆ°1å‚æ•°æ¨¡å¼: (problem)")
                                        result = await asyncio.wait_for(
                                            workflow(problem),
                                            timeout=self.timeout
                                        )
                                        print(f"  âœ… 1å‚æ•°æ¨¡å¼æˆåŠŸ")
                                    else:
                                        raise
                            else:
                                raise
                    elif "entry_point" in kwargs:
                        # åªæœ‰entry_pointï¼Œæ²¡æœ‰test
                        try:
                            print(f"  ğŸ“‹ å°è¯•2å‚æ•°æ¨¡å¼: (problem, entry_point)")
                            result = await asyncio.wait_for(
                                workflow(problem, kwargs["entry_point"]),
                                timeout=self.timeout
                            )
                            print(f"  âœ… 2å‚æ•°æ¨¡å¼æˆåŠŸ")
                        except TypeError as e:
                            if "positional argument" in str(e) or "missing" in str(e).lower():
                                print(f"  âš ï¸  2å‚æ•°å¤±è´¥ï¼Œé™çº§åˆ°1å‚æ•°æ¨¡å¼: (problem)")
                                result = await asyncio.wait_for(
                                    workflow(problem),
                                    timeout=self.timeout
                                )
                                print(f"  âœ… 1å‚æ•°æ¨¡å¼æˆåŠŸ")
                            else:
                                raise
                    else:
                        # æ²¡æœ‰entry_pointï¼Œç›´æ¥ç”¨1å‚æ•°
                        print(f"  ğŸ“‹ ä½¿ç”¨1å‚æ•°æ¨¡å¼: (problem)")
                        result = await asyncio.wait_for(
                            workflow(problem),
                            timeout=self.timeout
                        )
                        print(f"  âœ… 1å‚æ•°æ¨¡å¼æˆåŠŸ")
                else:
                    # Non-code problems (Math/QA) - ä»…ä¼ problemå‚æ•°
                    print(f"  ğŸ“‹ {problem_type.upper()}é—®é¢˜ä½¿ç”¨1å‚æ•°æ¨¡å¼: (problem)")
                    result = await asyncio.wait_for(
                        workflow(problem),
                        timeout=self.timeout
                    )
                    print(f"  âœ… æ‰§è¡ŒæˆåŠŸ")
            except Exception as e:
                # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼ˆoperatoræ‰§è¡Œå¤±è´¥ï¼‰
                print(f"  âŒ Workflowæ‰§è¡Œå¼‚å¸¸: {type(e).__name__}")
                print(f"     å¼‚å¸¸ä¿¡æ¯: {str(e)}")

                # å¿«é€Ÿå¤„ç†Testç®—å­çš„å·²çŸ¥é—®é¢˜
                if "'NoneType' object is not iterable" in str(e) and "test_cases" in str(e):
                    print(f"  ğŸš€ æ£€æµ‹åˆ°Testç®—å­Noneé—®é¢˜ï¼Œå¿«é€Ÿåˆ‡æ¢åˆ°Fallback")
                    import traceback
                else:
                    import traceback
                    print(f"  å®Œæ•´å †æ ˆ:")
                    traceback.print_exc()

                # æ£€æŸ¥æ˜¯å¦å¯ç”¨Fallback
                if self.enable_fallback:
                    print(f"  ğŸ”„ å°è¯•ä½¿ç”¨Fallbackæœºåˆ¶")
                    # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶fallback metadataå¹¶è®°å½•needed_fallbackæ ‡è®°
                    answer, cost, fb_metadata = await self._execute_fallback_workflow(problem, problem_type, **kwargs)
                    metadata['needed_fallback'] = True
                    metadata['fallback_type'] = 'operator_error'
                    metadata.update(fb_metadata)
                    return answer, cost, metadata
                else:
                    print(f"  âš ï¸  Fallbackå·²ç¦ç”¨ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸")
                    # ç›´æ¥æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯ä½¿ç”¨fallback
                    raise

            # å®‰å…¨åœ°è§£åŒ…ç»“æœï¼ˆå¯èƒ½è¿”å›2ä¸ªæˆ–æ›´å¤šå€¼ï¼‰
            if isinstance(result, tuple):
                if len(result) >= 2:
                    answer, cost = result[0], result[1]
                elif len(result) == 1:
                    answer, cost = result[0], 0.0
                else:
                    answer, cost = None, 0.0
            else:
                answer, cost = result, 0.0

            # âœ¨ FIX 1: Cost ç±»å‹éªŒè¯ä¸é¢ å€’æ£€æµ‹ï¼ˆæ¥è‡ªå‚è€ƒé¡¹ç›®ï¼‰
            # é—®é¢˜ï¼šæŸäº›æ ¼å¼é”™è¯¯çš„workflowå¯èƒ½è¿”å› (cost, answer) è€Œé (answer, cost)
            # æˆ–è€… cost å¯èƒ½æ˜¯å­—ç¬¦ä¸²è€Œä¸æ˜¯æ•°å­—ï¼Œå¯¼è‡´å¥–åŠ±è®¡ç®—å¤±è´¥
            if not isinstance(cost, (int, float)):
                # æ£€æµ‹æ˜¯å¦ answer å’Œ cost è¢«é¢ å€’äº†
                if isinstance(answer, (int, float)) and isinstance(cost, str):
                    print(f"  ğŸ”„ æ£€æµ‹åˆ°answer/costé¢ å€’ï¼Œå·²äº¤æ¢")
                    answer, cost = cost, answer
                else:
                    print(f"  âš ï¸  æ— æ•ˆçš„costç±»å‹: {type(cost).__name__}ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.0")
                    cost = 0.0

            execution_time = time.time() - start_time

            # âœ¨ FIX 2: ç©ºç­”æ¡ˆæ£€æµ‹ä¸Fallbackè§¦å‘ï¼ˆæ¥è‡ªå‚è€ƒé¡¹ç›®ï¼‰
            # é—®é¢˜ï¼šWorkflowå¯èƒ½è¿”å›Noneæˆ–ç©ºå­—ç¬¦ä¸²ï¼Œå¯¼è‡´è®­ç»ƒæ±¡æŸ“
            if answer is None or (isinstance(answer, str) and not answer.strip()):
                print(f"  ğŸš¨ æ£€æµ‹åˆ°ç©ºç­”æ¡ˆï¼ˆNoneæˆ–ç©ºå­—ç¬¦ä¸²ï¼‰")

                if self.enable_fallback:
                    print(f"  ğŸ”„ è§¦å‘Fallbackæœºåˆ¶ä»¥å¤„ç†ç©ºç­”æ¡ˆ")
                    # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶fallback metadataå¹¶è®°å½•needed_fallbackæ ‡è®°
                    answer, cost, fb_metadata = await self._execute_fallback_workflow(problem, problem_type, **kwargs)
                    metadata['needed_fallback'] = True
                    metadata['fallback_type'] = 'empty_answer'
                    metadata.update(fb_metadata)
                    return answer, cost, metadata
                else:
                    print(f"  âš ï¸  Fallbackå·²ç¦ç”¨ï¼Œè¿”å›ç©ºç­”æ¡ˆ")
                    metadata = {
                        "success": False,
                        "error": "empty_answer",
                        "error_type": "empty_answer",  # æ–°å¢ï¼ˆæ–¹æ¡ˆBï¼‰ï¼šæ˜ç¡®é”™è¯¯ç±»å‹
                        "execution_time": execution_time,
                        "cost": cost,
                        "problem_type": problem_type,
                        "validation_failed": False,
                        "fallback_executed": False
                    }
                    return None, 0.0, metadata

            # âœ¨ FIX 3: ç­”æ¡ˆæ¨¡å¼æ¸…ç†ï¼ˆæ¥è‡ªå‚è€ƒé¡¹ç›®ï¼‰
            # é—®é¢˜ï¼šæŸäº›LLMå¯èƒ½åœ¨ç­”æ¡ˆå‰æ·»åŠ è§£é‡Šæ€§æ–‡æœ¬ï¼Œå¦‚"Based on feedback..."ã€"Revised Solution:"ç­‰
            # è¿™äº›æ¨¡å¼ä¼šæ±¡æŸ“ç­”æ¡ˆï¼Œéœ€è¦æ¸…ç†
            if isinstance(answer, str):
                answer = self._clean_answer(answer)

            # âœ¨ FIX 4: ä»£ç æ³„éœ²æ£€æµ‹ï¼ˆæ¥è‡ªå‚è€ƒé¡¹ç›®ï¼‰
            # é—®é¢˜ï¼šProgrammer ç®—å­å¯èƒ½è¿”å›æºä»£ç è€Œéæ‰§è¡Œç»“æœï¼ˆç‰¹åˆ«æ˜¯å¯¹ä»£ç é—®é¢˜ï¼‰
            # æ­¤æ—¶åº”è§¦å‘ fallback è€Œä¸æ˜¯è¿”å›æºä»£ç 
            if self._detect_code_leakage(answer, problem_type):
                print(f"  ğŸš¨ æ£€æµ‹åˆ°ä»£ç æ³„éœ²ï¼ˆProgrammerè¿”å›äº†æºä»£ç è€Œéæ‰§è¡Œç»“æœï¼‰")

                if self.enable_fallback:
                    print(f"  ğŸ”„ è§¦å‘Fallbackæœºåˆ¶ä»¥å¤„ç†ä»£ç æ³„éœ²")
                    # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶fallback metadataå¹¶è®°å½•needed_fallbackæ ‡è®°
                    answer, cost, fb_metadata = await self._execute_fallback_workflow(problem, problem_type, **kwargs)
                    metadata['needed_fallback'] = True
                    metadata['fallback_type'] = 'code_leakage'
                    metadata.update(fb_metadata)
                    return answer, cost, metadata
                else:
                    print(f"  âš ï¸  Fallbackå·²ç¦ç”¨ï¼Œè¿”å›æºä»£ç ")
                    metadata = {
                        "success": False,
                        "error": "code_leakage",
                        "error_type": "code_leakage",  # æ–°å¢ï¼ˆæ–¹æ¡ˆBï¼‰ï¼šæ˜ç¡®é”™è¯¯ç±»å‹
                        "execution_time": execution_time,
                        "cost": cost,
                        "problem_type": problem_type,
                        "validation_failed": False,
                        "fallback_executed": False
                    }
                    return None, 0.0, metadata

            # å…ƒæ•°æ®ï¼ˆæ–¹æ¡ˆBï¼šæ·»åŠ operator_problem_type_mismatchæ ‡è®°ç”¨äºsoft learningï¼‰
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨had_instantiation_erroræ ‡è®°ï¼Œå¦‚æœæœ‰åˆ™æ·»åŠ needed_fallback
            if not metadata.get('had_instantiation_error', False):
                # æ­£å¸¸æµç¨‹ï¼šæ›´æ–°metadataï¼ˆä¿ç•™ä¹‹å‰çš„had_signature_errorç­‰æ ‡å¿—ï¼ï¼‰
                metadata.update({
                    "success": True,
                    "execution_time": execution_time,
                    "cost": cost,
                    "problem_type": problem_type,
                    "validation_failed": False,
                    "fallback_executed": False,
                    # æ–°å¢ï¼ˆæ–¹æ¡ˆBï¼‰ï¼šæ ‡è®°operator-problem typeåŒ¹é…æƒ…å†µ
                    "operator_problem_type_mismatch": mismatch_detected,
                    "mismatch_type": mismatch_details.split('\n')[0] if mismatch_details else None
                })
            else:
                # å®ä¾‹åŒ–å¤±è´¥ä½†æœ€ç»ˆæˆåŠŸçš„æµç¨‹ï¼šä¿ç•™had_instantiation_errorï¼Œæ·»åŠ needed_fallback
                metadata['success'] = True
                metadata['needed_fallback'] = True  # ğŸ”§ æ ‡è®°ï¼šè™½ç„¶æœ€ç»ˆæˆåŠŸï¼Œä½†ç”Ÿæˆçš„ä»£ç æ— æ³•å®ä¾‹åŒ–
                metadata['fallback_type'] = 'instantiation_error'
                metadata['execution_time'] = execution_time
                metadata['operator_problem_type_mismatch'] = mismatch_detected
                metadata['mismatch_type'] = mismatch_details.split('\n')[0] if mismatch_details else None

            if mismatch_detected:
                print(f"  âš ï¸  Workflow violates operator-problem constraint")
                print(f"     This will be penalized (-5.0) in training reward")

            return answer, cost, metadata

        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            print(f"â±ï¸  æ‰§è¡Œè¶…æ—¶ ({self.timeout}ç§’)")

            metadata = {
                "success": False,
                "error": "timeout",
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type,
                "validation_failed": False,  # ğŸ”´ æ–°å¢ï¼šå·¥ä½œæµé€šè¿‡éªŒè¯ï¼Œä½†æ‰§è¡Œè¶…æ—¶äº†
                "fallback_executed": False
            }

            return None, 0.0, metadata

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ æ‰§è¡Œé”™è¯¯: {str(e)}")

            import traceback
            traceback.print_exc()

            metadata = {
                "success": False,
                "error": str(e),
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type,
                "validation_failed": False,  # ğŸ”´ æ–°å¢ï¼šå·¥ä½œæµé€šè¿‡éªŒè¯ï¼Œä½†æ‰§è¡Œå¤±è´¥äº†
                "fallback_executed": False
            }

            return None, 0.0, metadata

    def _create_workflow_class(self, workflow_code: str, problem_type: str):
        """
        ä»å·¥ä½œæµä»£ç åŠ¨æ€åˆ›å»ºWorkflowç±»ï¼Œæ”¯æŒTASK_PROMPTæ³¨å…¥

        è®¾è®¡ï¼š
        1. æå–TASK_PROMPTå˜é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        2. åˆ›å»ºåŸºç¡€å·¥ä½œæµç±»
        3. å¦‚æœæœ‰TASK_PROMPTï¼Œåˆ›å»ºEnhancedWorkflowåŒ…è£…å™¨è‡ªåŠ¨æ³¨å…¥
        """

        # 1. æå–TASK_PROMPTï¼ˆå¯é€‰ï¼‰
        task_prompt = self.validator_v2.extract_task_prompt(workflow_code)
        if task_prompt:
            print(f"ğŸ“ æ£€æµ‹åˆ°TASK_PROMPTï¼Œå°†åœ¨æ‰§è¡Œæ—¶æ³¨å…¥")

        # å‡†å¤‡å‘½åç©ºé—´
        namespace = {
            "operator": operator_module,
            "create_llm_instance": create_llm_instance,
            "DatasetType": str
        }

        # æ›¿æ¢importè·¯å¾„ï¼ˆä½¿workspaceè·¯å¾„å¯ç”¨ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨scriptsä¸­çš„operator
        modified_code = workflow_code.replace(
            f"import workspace.{problem_type}.workflows.template.operator as operator",
            "# operator already imported"
        )

        # ä¿®å¤å¸¸è§typoï¼ˆRLæ¨¡å‹å¯èƒ½äº§ç”Ÿçš„é”™è¯¯ï¼‰
        modified_code = modified_code.replace("async_lll", "async_llm")
        modified_code = modified_code.replace("create_lll_instance", "create_llm_instance")

        try:
            # 2. æ‰§è¡Œä»£ç åˆ›å»ºåŸºç¡€ç±»
            exec(modified_code, namespace)

            # è¿”å›Workflowç±»
            if "Workflow" not in namespace:
                raise ValueError("No Workflow class found in generated code")

            base_class = namespace["Workflow"]

            # 3. å¦‚æœæœ‰TASK_PROMPTï¼Œåˆ›å»ºEnhancedWorkflowåŒ…è£…å™¨
            if task_prompt:
                class EnhancedWorkflow:
                    """è‡ªåŠ¨æ³¨å…¥TASK_PROMPTçš„åŒ…è£…å™¨"""
                    def __init__(self, name: str, llm_config, dataset):
                        self.base_workflow = base_class(name, llm_config, dataset)
                        self.task_prompt = task_prompt
                        self.llm = self.base_workflow.llm

                    async def __call__(self, problem: str, *args, **kwargs):
                        """
                        è‡ªåŠ¨åœ¨é—®é¢˜å‰æ³¨å…¥TASK_PROMPT

                        Args:
                            problem: åŸå§‹é—®é¢˜æ–‡æœ¬
                            *args, **kwargs: ä¼ é€’ç»™åŸºç¡€å·¥ä½œæµçš„å…¶ä»–å‚æ•°ï¼ˆå¦‚entry_point, testï¼‰

                        Returns:
                            (answer, cost) å…ƒç»„
                        """
                        # æ³¨å…¥TASK_PROMPTåˆ°é—®é¢˜å‰é¢
                        enhanced_problem = f"{self.task_prompt}\n\n{problem}"
                        return await self.base_workflow(enhanced_problem, *args, **kwargs)

                return EnhancedWorkflow
            else:
                return base_class

        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆçš„å·¥ä½œæµä»£ç æœ‰é”™è¯¯: {e}")
            print(f"  ä½¿ç”¨é»˜è®¤fallbackå·¥ä½œæµ")
            import traceback
            traceback.print_exc()

            # ä½¿ç”¨ç®€å•çš„é»˜è®¤å·¥ä½œæµä½œä¸ºfallback
            return self._get_fallback_workflow_class(problem_type)

    def _get_llm_config(self):
        """è·å–LLMé…ç½®ï¼ˆç¡®ä¿è¿”å›æ­£ç¡®ç±»å‹ï¼‰"""
        from scripts.async_llm import LLMsConfig, LLMConfig

        try:
            if self.llm_configs:
                result = self.llm_configs.get(self.llm_model_name)
            else:
                # å°è¯•ä½¿ç”¨é»˜è®¤é…ç½®
                result = LLMsConfig.default().get(self.llm_model_name)

            # ç±»å‹éªŒè¯ï¼ˆå…³é”®ï¼ï¼‰
            if isinstance(result, LLMConfig):
                return result
            elif isinstance(result, dict):
                # å¦‚æœæ„å¤–è¿”å›äº† dictï¼Œè½¬æ¢ä¸º LLMConfig
                print(f"âš ï¸  è­¦å‘Šï¼šget() è¿”å›äº† dictï¼Œæ­£åœ¨è½¬æ¢ä¸º LLMConfig")
                return LLMConfig(result)
            elif isinstance(result, str):
                return result
            else:
                print(f"âš ï¸  æœªçŸ¥ç±»å‹: {type(result)}ï¼Œé™çº§ä¸ºå­—ç¬¦ä¸²")
                return self.llm_model_name

        except Exception as e:
            print(f"âš ï¸  è·å–LLMé…ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›å­—ç¬¦ä¸²æ¨¡å‹åï¼Œè®© create_llm_instance è‡ªåŠ¨å¤„ç†
            print(f"  é™çº§ä¸ºå­—ç¬¦ä¸²æ¨¡å¼: {self.llm_model_name}")
            return self.llm_model_name

    def _create_qa_fallback_workflow(self) -> str:
        """
        åˆ›å»º QA ä¸“ç”¨ Fallback å·¥ä½œæµä»£ç 

        ç‰¹ç‚¹ï¼š
        - ä»…ä½¿ç”¨ Custom æ“ä½œç¬¦ï¼Œä¸ä½¿ç”¨ Test
        - ç‰¹åˆ«é’ˆå¯¹ QA é—®é¢˜çš„æŒ‡ä»¤
        - ä¸å¤„ç† entry_point å‚æ•°ï¼ˆQA ä¸éœ€è¦ï¼‰
        """
        return '''
import asyncio

class Workflow:
    def __init__(self, name, llm_config, dataset):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.model)

    async def __call__(self, problem, entry_point=None, test=None):
        """QA Fallback å·¥ä½œæµï¼šä½¿ç”¨ Custom æ“ä½œç¬¦ç”Ÿæˆç­”æ¡ˆï¼Œä¸ä½¿ç”¨ Test"""
        instruction = "Answer this question comprehensively. Provide the final answer clearly."
        result = await self.custom(input=problem, instruction=instruction)

        # å®‰å…¨æå–å“åº”
        if isinstance(result, dict):
            response = result.get("response", "")
        else:
            response = str(result)

        # è·å–æˆæœ¬
        try:
            cost = self.model.get_usage_summary().get("total_cost", 0.0)
        except:
            cost = 0.0

        return response, cost
'''

    async def _execute_fallback_workflow(
        self,
        problem: str,
        problem_type: str,
        **kwargs
    ) -> Tuple[Any, float, Dict]:
        """
        æ‰§è¡ŒFallbackå·¥ä½œæµ

        ä½¿ç”¨æœ€ç®€å•ä½†å¯é çš„æ–¹å¼æ‰§è¡Œ
        """
        print(f"ğŸ”„ æ‰§è¡ŒFallbackå·¥ä½œæµï¼ˆç±»å‹: {problem_type}ï¼‰")
        start_time = time.time()

        try:
            # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹© Fallback å·¥ä½œæµ
            if problem_type == "qa":
                # QA ä¸“ç”¨ Fallbackï¼šé¿å… Test æ“ä½œç¬¦
                simple_workflow_code = self._create_qa_fallback_workflow()
                print(f"  â„¹ï¸  ä½¿ç”¨ QA ä¸“ç”¨ Fallbackï¼ˆä¸åŒ…å« Test æ“ä½œç¬¦ï¼‰")
            else:
                # é€šç”¨ Fallbackï¼ˆç”¨äº code å’Œ mathï¼‰
                if problem_type == "code":
                    func_signature = ", entry_point"
                else:
                    func_signature = ""

                simple_workflow_code = f'''
import asyncio

class Workflow:
    def __init__(self, name, llm_config, dataset):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.model)

    async def __call__(self, problem{func_signature}):
        """Simple fallback workflow using only Custom operator"""

        # Use Custom operator with appropriate instruction
        if self.dataset == "code":
            instruction = "Solve this coding problem. Provide a complete Python solution."
        elif self.dataset == "math":
            instruction = "Solve this math problem step by step. Show your work and provide the final answer."
        else:
            instruction = "Answer this question comprehensively."

        result = await self.custom(input=problem, instruction=instruction)

        # Validate and extract response
        if isinstance(result, dict):
            response = result.get("response", "")
        else:
            response = str(result)

        # Get cost
        try:
            cost = self.model.get_usage_summary().get("total_cost", 0.0)
        except:
            cost = 0.0

        return response, cost
'''

            # åˆ›å»ºå·¥ä½œæµç±»
            workflow_class = self._create_workflow_class(simple_workflow_code, problem_type)

            # å®ä¾‹åŒ–
            llm_config = self._get_llm_config()
            workflow = workflow_class(
                name="fallback_workflow",
                llm_config=llm_config,
                dataset=problem_type
            )

            # æ‰§è¡Œ
            if problem_type == "code" and "entry_point" in kwargs:
                result = await asyncio.wait_for(
                    workflow(problem, kwargs["entry_point"]),
                    timeout=self.timeout
                )
            else:
                result = await asyncio.wait_for(
                    workflow(problem),
                    timeout=self.timeout
                )

            # è§£åŒ…ç»“æœ
            if isinstance(result, tuple) and len(result) >= 2:
                answer, cost = result[0], result[1]
            else:
                answer, cost = result, 0.0

            execution_time = time.time() - start_time

            metadata = {
                "success": True,
                "needed_fallback": True,  # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€keyåç§°ï¼ˆä»'fallback_used'æ”¹ä¸º'needed_fallback'ï¼‰
                "execution_time": execution_time,
                "cost": cost,
                "problem_type": problem_type
            }

            print(f"âœ… FallbackæˆåŠŸ (è€—æ—¶: {execution_time:.2f}ç§’)")
            return answer, cost, metadata

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ Fallbackä¹Ÿå¤±è´¥äº†: {e}")

            metadata = {
                "success": False,
                "needed_fallback": True,  # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€keyåç§°ï¼ˆä»'fallback_used'æ”¹ä¸º'needed_fallback'ï¼‰
                "error": str(e),
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type
            }

            # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return "", 0.0, metadata

    def _get_fallback_workflow_class(self, problem_type: str):
        """è¿”å›ä¸€ä¸ªç®€å•çš„é»˜è®¤å·¥ä½œæµç±»ï¼ˆç”¨äºç”Ÿæˆå¤±è´¥æ—¶ï¼‰

        æ”¹è¿›çš„fallbackç­–ç•¥ï¼š
        1. å…ˆå°è¯•ç›´æ¥è°ƒç”¨LLMç”Ÿæˆè§£å†³æ–¹æ¡ˆ
        2. å¦‚æœå¤±è´¥ï¼Œè¿”å›å ä½ç¬¦è€Œä¸æ˜¯None
        3. é¿å…ä¾èµ–å¯èƒ½å¤±è´¥çš„Test operator
        """
        # ä¿å­˜llm_config_pathä¾›FallbackWorkflowä½¿ç”¨
        llm_config_path = self.llm_config_path

        class FallbackWorkflow:
            def __init__(self, name: str, llm_config, dataset):
                self.name = name
                self.dataset = dataset

                # L1.2: 3-tier LLM åˆå§‹åŒ–é™çº§æœºåˆ¶ï¼ˆå¢å¼ºå¯é æ€§ï¼‰
                try:
                    # Tier 1: å°è¯•ä¸» LLM åˆå§‹åŒ–
                    self.model = create_llm_instance(llm_config)
                    print(f"âœ… LLM åˆå§‹åŒ–æˆåŠŸï¼ˆä¸» LLMï¼‰")
                except Exception as e:
                    print(f"âš ï¸  ä¸» LLM åˆå§‹åŒ–å¤±è´¥: {e}")

                    # Tier 2: ä½¿ç”¨ OpenAI å¤‡ç”¨ LLM
                    try:
                        print(f"  å°è¯•ä½¿ç”¨ OpenAI å¤‡ç”¨ LLM...")
                        import os
                        import yaml

                        api_key = None

                        # ç­–ç•¥1: ä»ç¯å¢ƒå˜é‡è·å–
                        api_key = os.getenv("OPENAI_API_KEY")

                        # ç­–ç•¥2: ä»YAMLé…ç½®æ–‡ä»¶è¯»å–
                        if not api_key:
                            try:
                                config_path = Path(llm_config_path).absolute()
                                if config_path.exists():
                                    with open(config_path, 'r') as f:
                                        config_data = yaml.safe_load(f)
                                        model_config = config_data.get('models', {}).get('gpt-4o-mini', {})
                                        api_key = model_config.get('api_key')

                                        # å¦‚æœæ˜¯ç¯å¢ƒå˜é‡å¼•ç”¨ï¼ˆå¦‚ ${OPENAI_API_KEY}ï¼‰ï¼Œè§£æå®ƒ
                                        if api_key and api_key.startswith('${') and api_key.endswith('}'):
                                            env_var_name = api_key[2:-1]
                                            api_key = os.getenv(env_var_name)
                                        elif api_key and api_key.startswith('$'):
                                            env_var_name = api_key[1:]
                                            api_key = os.getenv(env_var_name)
                            except Exception as e_yaml:
                                print(f"    âš ï¸  æ— æ³•è¯»å–YAMLé…ç½®: {e_yaml}")

                        # ç­–ç•¥3: å¦‚æœllm_configæ˜¯dictï¼Œå°è¯•ä»ä¸­æå–
                        if not api_key and isinstance(llm_config, dict):
                            api_key = llm_config.get('api_key')

                        if api_key and not api_key.startswith('$'):
                            # API Key å¯ç”¨ï¼Œä½¿ç”¨ OpenAI å¤‡ç”¨
                            self.model = AsyncOpenAILLMWrapper(api_key=api_key)
                            print(f"âœ… OpenAI å¤‡ç”¨ LLM åˆå§‹åŒ–æˆåŠŸ")
                        else:
                            # æ²¡æœ‰æœ‰æ•ˆçš„ API Keyï¼Œè¿›å…¥ Tier 3
                            raise ValueError(f"æ— æœ‰æ•ˆçš„ OpenAI API Key (api_key={api_key})")

                    except Exception as e2:
                        print(f"âš ï¸  OpenAI å¤‡ç”¨ LLM åˆå§‹åŒ–å¤±è´¥: {e2}")

                        # Tier 3: æœ€åé™çº§ä¸º None
                        self.model = None
                        print(f"âš ï¸  LLM åˆå§‹åŒ–å®Œå…¨å¤±è´¥ï¼Œå°†ä½¿ç”¨å ä½ç¬¦è¿”å›")

            @staticmethod
            def _safe_extract_response(result):
                """
                L1.3: å®‰å…¨æå–å“åº”ï¼Œå¤„ç†å¤šç§è¿”å›æ ¼å¼

                æ”¯æŒçš„æ ¼å¼ï¼š
                - dict: æŸ¥æ‰¾ 'response' / 'answer' / 'solution' é”®
                - tuple: å–ç¬¬ä¸€ä¸ªå…ƒç´ 
                - str: ç›´æ¥è¿”å›
                - None: è¿”å›ç©ºå­—ç¬¦ä¸²
                """
                if result is None:
                    return ""

                # å¤„ç†å­—å…¸æ ¼å¼
                if isinstance(result, dict):
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®
                    response = (result.get('response') or
                               result.get('answer') or
                               result.get('solution') or
                               str(result))
                    return response if response else ""

                # å¤„ç†å…ƒç»„æ ¼å¼
                elif isinstance(result, tuple):
                    return str(result[0]) if result and result[0] is not None else ""

                # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼
                elif isinstance(result, str):
                    return result

                # å…¶ä»–æ ¼å¼ï¼šè½¬ä¸ºå­—ç¬¦ä¸²
                else:
                    return str(result) if result else ""

            async def __call__(self, problem: str, *args, **kwargs):
                """æ”¹è¿›çš„fallbackï¼šä¸ä¾èµ–Test operator"""

                # ç­–ç•¥1: ç›´æ¥è°ƒç”¨LLMç”Ÿæˆï¼Œä¸ç»è¿‡ä»»ä½•operator
                if self.model is not None:
                    try:
                        print(f"  ğŸ“ Fallback: ç›´æ¥è°ƒç”¨LLMç”Ÿæˆè§£å†³æ–¹æ¡ˆ")

                        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„prompt
                        if self.dataset == "code":
                            prompt = f"""Given the following coding problem, provide a Python solution.

Problem:
{problem}

Provide ONLY the Python function code, no explanations."""
                        else:
                            prompt = f"""Solve the following problem step by step and provide the final answer.

Problem:
{problem}

Provide the final answer clearly."""

                        # ğŸ”´ ä¿®å¤: ä½¿ç”¨æ­£ç¡®çš„ AsyncLLM æ¥å£
                        # AsyncLLM çš„æ–¹æ³•æ˜¯ __call__(prompt) è€Œä¸æ˜¯ agenerate(messages=[...])
                        response = await self.model(prompt)

                        if response:
                            usage = self.model.get_usage_summary()
                            if isinstance(usage, dict) and "total_cost" in usage:
                                cost = usage["total_cost"]
                            else:
                                cost = 0.0

                            # L1.3: ä½¿ç”¨å®‰å…¨æå–æ–¹æ³•è·å–å“åº”
                            # response å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼Œéœ€è¦å¤„ç†
                            if isinstance(response, dict):
                                answer = response.get('response', str(response))
                            else:
                                answer = str(response)
                            return answer, cost

                    except Exception as e:
                        print(f"  âš ï¸  Fallbackç›´æ¥è°ƒç”¨LLMå¤±è´¥: {e}")

                # ç­–ç•¥2: å¦‚æœLLMè°ƒç”¨ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨Custom operatorä½†ä¸ä¾èµ–Test
                # ğŸ”´ ä¿®å¤: åªåœ¨ self.model ä¸æ˜¯ None æ—¶æ‰å°è¯•
                if self.model is not None:
                    try:
                        print(f"  ğŸ“ Fallback: å°è¯•ä½¿ç”¨Custom operator")
                        custom = operator_module.Custom(self.model)
                        result = await custom(
                            input=problem,
                            instruction="Generate a solution without requiring test validation."
                        )

                        if result:
                            # L1.3: ä½¿ç”¨å®‰å…¨æå–æ–¹æ³•è·å–å“åº”
                            response_text = self._safe_extract_response(result)
                            if response_text:
                                usage = self.model.get_usage_summary()
                                if isinstance(usage, dict) and "total_cost" in usage:
                                    cost = usage["total_cost"]
                                else:
                                    cost = 0.0
                                return response_text, cost

                    except Exception as e:
                        print(f"  âš ï¸  Fallback Custom operatorå¤±è´¥: {e}")

                # ç­–ç•¥3: æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›å ä½ç¬¦è€Œä¸æ˜¯None
                print(f"  âš ï¸  æ‰€æœ‰fallbackç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›å ä½ç¬¦")
                placeholder = f"[Fallback placeholder for problem: {problem[:80]}...]"
                return placeholder, 0.0

        return FallbackWorkflow


async def test_executor():
    """æµ‹è¯•AFlowæ‰§è¡Œå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•AFlowæ‰§è¡Œå™¨")
    print("=" * 60)

    # åˆ›å»ºæ‰§è¡Œå™¨
    executor = AFlowExecutor(
        llm_config_path="config/aflow_llm.yaml",
        llm_model_name="gpt-4o-mini",
        timeout=60
    )

    # æµ‹è¯•å·¥ä½œæµä»£ç ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
    test_workflow_code = """
import workspace.math.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.model)

    async def __call__(self, problem: str):
        solution = await self.custom(input=problem, instruction="Solve this problem step by step and provide the final answer.")
        return solution['response'], self.model.get_usage_summary()["total_cost"]
"""

    # æµ‹è¯•é—®é¢˜
    test_problem = "What is 15 + 27?"

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_problem}")

    # æ‰§è¡Œå·¥ä½œæµ
    answer, cost, metadata = await executor.execute_workflow(
        workflow_code=test_workflow_code,
        problem=test_problem,
        problem_type="math"
    )

    print(f"\nâœ… æ‰§è¡Œç»“æœ:")
    print(f"  æˆåŠŸ: {metadata['success']}")
    print(f"  ç­”æ¡ˆ: {answer}")
    print(f"  æˆæœ¬: ${cost:.6f}")
    print(f"  æ—¶é—´: {metadata['execution_time']:.2f}ç§’")


if __name__ == "__main__":
    asyncio.run(test_executor())
