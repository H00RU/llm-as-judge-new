#!/usr/bin/env python3
"""
RLå·¥ä½œæµç”Ÿæˆå™¨ - ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ
"""
import torch
import json
import ast
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, LoraConfig, get_peft_model
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import sys
import os

from src.workflow_validator_v2 import WorkflowValidatorV2

class RLWorkflowGenerator:
    """ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ"""

    def __init__(
        self,
        model=None,
        tokenizer=None,
        device=None,
        base_model: Optional[str] = None,
        lora_checkpoint: Optional[str] = None,
        device_ids: Optional[List[int]] = None,
        operator_descriptions_path: Optional[str] = None,
        config: Optional[Dict] = None
    ):
        """
        Args:
            model: å…±äº«çš„æ¨¡å‹å®ä¾‹ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œè‹¥æä¾›åˆ™ä¸åŠ è½½æ–°æ¨¡å‹ï¼‰
            tokenizer: å…±äº«çš„tokenizerå®ä¾‹ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            device: å…±äº«çš„è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            base_model: åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆä»…åœ¨model=Noneæ—¶ä½¿ç”¨ï¼‰
            lora_checkpoint: LoRAæ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨åŸºåº§æ¨¡å‹ï¼‰
            device_ids: ä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼ˆä»…åœ¨device=Noneæ—¶ä½¿ç”¨ï¼‰
            operator_descriptions_path: AFlowç®—å­æè¿°æ–‡ä»¶è·¯å¾„
            config: é¢å¤–é…ç½®
        """
        self.config = config or {}

        # âœ¨ NEW: Support model sharing from GRPO Trainer
        if model is not None:
            print(f"ğŸ”§ åˆå§‹åŒ–RLå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆä½¿ç”¨å…±äº«æ¨¡å‹ï¼‰")
            print(f"  ğŸ”— å…±äº«æ¨¡å‹ID: {id(model)}")
            self.model = model
            self.tokenizer = tokenizer
            self.device = device
            self.base_model = None  # Not needed when sharing
            self.lora_checkpoint = None
            self.device_ids = None
            print(f"  è®¾å¤‡: {self.device}")
            print(f"  âœ… æ¨¡å‹å…±äº«æˆåŠŸ - èŠ‚çœ ~15GB GPUå†…å­˜")
        else:
            # Legacy path: Load own model (for standalone usage)
            print(f"ğŸ”§ åˆå§‹åŒ–RLå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆç‹¬ç«‹æ¨¡å¼ï¼‰")

            if base_model is None:
                base_model = "Qwen/Qwen2.5-7B-Instruct"
            if device_ids is None:
                device_ids = [2, 3]

            self.base_model = base_model
            self.lora_checkpoint = lora_checkpoint
            self.device_ids = device_ids
            self.device = f"cuda:{device_ids[0]}" if torch.cuda.is_available() else "cpu"

            # è®¾ç½®CUDAè®¾å¤‡
            if torch.cuda.is_available():
                os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, device_ids))

            print(f"  è®¾å¤‡: {self.device}")
            print(f"  GPU: {device_ids}")

            # åŠ è½½tokenizer
            print(f"ğŸ“¥ åŠ è½½tokenizer: {base_model}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model,
                trust_remote_code=True
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # åŠ è½½æ¨¡å‹
            print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {base_model}")
            self.model = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch.bfloat16,
                device_map={"": self.device},
                trust_remote_code=True
            )

            # åŠ è½½LoRAæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if lora_checkpoint:
                print(f"ğŸ“¥ åŠ è½½LoRAæ£€æŸ¥ç‚¹: {lora_checkpoint}")
                self.model = PeftModel.from_pretrained(self.model, lora_checkpoint)
                self.model.eval()

        # åŠ è½½ç®—å­æè¿°
        self.operator_descriptions = self._load_operator_descriptions(operator_descriptions_path)

        # åˆå§‹åŒ–ç»Ÿä¸€éªŒè¯å™¨ï¼ˆåˆå¹¶äº†ä»£ç æ„å»ºå™¨å’Œä¸€è‡´æ€§æ£€æŸ¥å™¨åŠŸèƒ½ï¼‰
        self.validator = WorkflowValidatorV2()

        print(f"âœ… RLå·¥ä½œæµç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_operator_descriptions(self, descriptions_path: Optional[str]) -> Dict:
        """åŠ è½½AFlowç®—å­æè¿°"""
        if descriptions_path and Path(descriptions_path).exists():
            with open(descriptions_path, 'r') as f:
                return json.load(f)

        # é»˜è®¤ç®—å­æè¿°
        return {
            "Custom": {
                "description": "Generates anything based on customized input and instruction.",
                "interface": "custom(input: str, instruction: str) -> dict with key 'response'"
            },
            "AnswerGenerate": {
                "description": "Generates step-by-step reasoning and final answer.",
                "interface": "answer_generate(input: str) -> dict with keys 'thought' and 'answer'"
            },
            "Programmer": {
                "description": "Automatically writes and executes Python code.",
                "interface": "programmer(problem: str, analysis: str = 'None') -> dict with keys 'code' and 'output'"
            },
            "ScEnsemble": {
                "description": "Uses self-consistency to select the most frequent solution.",
                "interface": "sc_ensemble(solutions: List[str], problem: str) -> dict with key 'response'"
            },
            "Review": {
                "description": "Reviews and provides feedback on a solution.",
                "interface": "review(problem: str, solution: str) -> dict with keys 'review_result' and 'feedback'"
            },
            "Revise": {
                "description": "Revises solution based on feedback.",
                "interface": "revise(problem: str, solution: str, feedback: str) -> dict with key 'solution'"
            }
        }

    def _build_generation_prompt(self, problem: str, problem_type: str) -> str:
        """æ„å»ºæç¤ºè¯ï¼Œæ˜ç¡®ç®—å­ APIï¼Œè®©æ¨¡å‹è‡ªä¸»å­¦ä¹ é€‰æ‹©"""

        # ä»£ç é¢˜ä¸“ç”¨æ¨¡æ¿
        if problem_type == "code":
            prompt = f"""Generate a Python Workflow class to solve the CODE problem.

ğŸš¨ CRITICAL REQUIREMENTS for CODE problems - DO NOT IGNORE:
âŒ MUST be async def __call__(self, problem: str, entry_point: str, test: str)
âœ… MUST accept THREE parameters (problem, entry_point, test)
âœ… MUST use Programmer to generate code
âœ… MUST use Test to execute the code with test cases
âœ… MUST return (answer, cost) tuple, NOT (code, answer)

CONSEQUENCES OF VIOLATION:
- Using wrong signature â†’ TypeError â†’ execution fails
- Not using Programmer â†’ No code generation â†’ 0 reward
- Not using Test â†’ No testing â†’ 0 reward
- Returning wrong format â†’ Type error â†’ execution fails

CODE PROBLEM WORKFLOW STRUCTURE:
```python
from scripts.operators import Custom, AnswerGenerate, Programmer, Test, Review, Revise, ScEnsemble
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        # REQUIRED: Initialize programmers for code generation
        self.programmer = Programmer(self.llm)
        # REQUIRED: Initialize tester for test execution
        self.test = Test(self.llm)
        # OPTIONAL: Add reviewer if you want feedback
        self.review = Review(self.llm)

    async def __call__(self, problem: str, entry_point: str, test: str):
        # REQUIRED: Three parameters for code problems
        # Step 1: Generate code using Programmer
        code_result = await self.programmer(problem=problem, analysis="")

        # Step 2: Test the code using provided test cases
        test_result = await self.test(
            problem=problem,
            solution=code_result['code'],
            entry_point=entry_point
        )

        # Step 3: Return execution result (NOT the code itself)
        if test_result['result']:
            # Test passed - return the solution
            return test_result['solution'], self.llm.get_usage_summary()["total_cost"]
        else:
            # Test failed - return the output (may contain error messages)
            return code_result['output'], self.llm.get_usage_summary()["total_cost"]
```

Available Operators:

from scripts.operators import Custom, AnswerGenerate, Programmer, Test, Review, Revise, ScEnsemble
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        # Initialize Programmer and Test (required for code problems)
        self.programmer = Programmer(self.llm)
        self.test = Test(self.llm)

    async def __call__(self, problem: str, entry_point: str, test: str):
        # Solve: {problem}
        # Generate code using Programmer
        code_result = await self.programmer(problem=problem, analysis='')

        # Test the code (this returns execution result, not code string)
        test_result = await self.test(
            problem=problem,
            solution=code_result['code'],
            entry_point=entry_point
        )

        # CRITICAL: Return execution result and cost
        # test_result['solution'] contains the final code
        # Return the execution output, not the code
        return code_result['output'], self.llm.get_usage_summary()["total_cost"]
"""
            return prompt

        # QAé¢˜ä¸“ç”¨æ¨¡æ¿
        if problem_type == "qa":
            prompt = f"""Generate a Python Workflow class to solve the QA problem.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš¨ CRITICAL CONSTRAINTS for QA problems - ZERO TOLERANCE   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FORBIDDEN OPERATORS (causes 100% FAILURE):
  âŒ Programmer - generates code, but QA needs natural language answers
  âŒ Test - requires test cases from dataset, QA has NONE

REQUIRED SIGNATURE:
  âœ… async def __call__(self, problem: str) - ONLY 1 parameter!
  âœ… return (answer, cost) tuple where answer is text

REQUIRED OPERATORS:
  âœ… AnswerGenerate - provides natural language answers
  âœ… Custom/Review/Revise - for refinement and verification

WHY THE CONSTRAINTS MATTER:
If you use Programmer or Test:
  1. Programmer will generate code, not answer the question
  2. Test will trigger NoneType error (no test cases for QA)
  3. Workflow execution FAILS immediately
  4. You get ZERO reward - no learning signal

If you use 3 parameters instead of 1:
  1. Executor will try to pass entry_point and test
  2. TypeError: unexpected keyword arguments
  3. Execution fails, Fallback triggered
  4. You get ZERO reward

WRONG EXAMPLE (DO NOT COPY - THIS FAILS 100%):
    # âŒ This is completely wrong for QA:
    async def __call__(self, problem: str, entry_point: str, test: str):
        code = await self.programmer(problem=problem)
        result = await self.test(problem=problem, solution=code)
        return result, cost
    # Result: TypeError + NoneType error, 0 reward, FAIL

CORRECT EXAMPLE (COPY THIS PATTERN):
    # âœ… This is the ONLY correct pattern:
    async def __call__(self, problem: str):
        solution = await self.answer_generate(input=problem)
        return solution['answer'], self.llm.get_usage_summary()["total_cost"]
    # Result: proper QA answer, PASS

REMEMBER:
- ONLY 1 parameter: problem
- NEVER use Programmer or Test
- EVERY operator must match the problem type
- Violating constraints is the #1 cause of failure

# Required imports for QA problems
from scripts.operators import Custom, AnswerGenerate, Review, Revise, ScEnsemble
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        # Initialize QA-appropriate operators
        self.answer_generate = AnswerGenerate(self.llm)
        self.review = Review(self.llm)
        self.revise = Revise(self.llm)

    async def __call__(self, problem: str):
        # Answer this QA question directly
        solution = await self.answer_generate(input=problem)
        return solution['answer'], self.llm.get_usage_summary()["total_cost"]
"""
            return prompt

        # æ•°å­¦é¢˜ä¸“ç”¨æ¨¡æ¿ï¼ˆå¼ºåŒ–ç‰ˆ - åŒ…å«WRONG exampleå’Œåæœè¯´æ˜ï¼‰
        if problem_type == "math":
            prompt = f"""Generate a Python Workflow class to solve the MATH problem.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš¨ CRITICAL CONSTRAINTS for MATH problems - ZERO TOLERANCE â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FORBIDDEN OPERATORS (causes 100% FAILURE):
  âŒ Programmer - generates code, but math needs step-by-step reasoning
  âŒ Test - requires test cases from dataset, math has NONE

REQUIRED OPERATORS:
  âœ… AnswerGenerate - provides step-by-step mathematical reasoning
  âœ… Custom/Review/Revise - for refinement and verification

WHY THE CONSTRAINTS MATTER:
If you use Programmer or Test:
  1. Programmer will generate code, not mathematical reasoning
  2. Test will trigger NoneType error (no test cases exist for math)
  3. Workflow execution FAILS immediately
  4. You get ZERO reward - no learning signal

WRONG EXAMPLE (DO NOT COPY - THIS FAILS 100%):
    # âŒ This is completely wrong for MATH problems:
    code = await self.programmer(problem=problem)
    result = await self.test(problem=problem, solution=code)
    return result, cost
    # Result: NoneType error, 0 reward, FAIL

CORRECT EXAMPLE (COPY THIS PATTERN):
    # âœ… This is the ONLY correct pattern:
    solution = await self.answer_generate(input=problem)
    return solution['answer'], self.llm.get_usage_summary()["total_cost"]
    # Result: proper reasoning, correct answer, PASS

REMEMBER:
- EVERY operator must match the problem type
- Violating constraints is the #1 cause of failure
- Programmer/Test usage = 0 reward in training

# Required imports for MATH problems
from scripts.operators import Custom, AnswerGenerate, Review, Revise, ScEnsemble
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)

        # IMPORTANT: Initialize ALL operators you will use
        self.custom = Custom(self.llm)
        self.answer_generate = AnswerGenerate(self.llm)
        self.review = Review(self.llm)
        self.revise = Revise(self.llm)
        self.sc_ensemble = ScEnsemble(self.llm)

    async def __call__(self, problem: str):
        # Solve this math problem step-by-step
        solution = await self.answer_generate(input=problem)
        return solution['answer'], self.llm.get_usage_summary()["total_cost"]
"""
            return prompt

        # QAé¢˜ä¸“ç”¨æ¨¡æ¿ï¼ˆä¿ç•™ï¼‰
        prompt = f"""Generate a Python Workflow class to solve the given problem.

IMPORTANT: Consider the problem's difficulty and complexity when designing your workflow.
- Some problems are simple and straightforward
- Some problems are complex and require careful handling
- Choose your strategy based on what you observe about the problem

âš ï¸ CRITICAL CONSTRAINTS for NON-CODE problems (MATH/QA):
âŒ DO NOT use Programmer operator - This problem is not about code generation
âŒ DO NOT use Test operator - This problem has no automated test cases
âœ… ONLY use: Custom, AnswerGenerate, Review, Revise, ScEnsemble

Why these constraints matter:
- Programmer generates Python code, but MATH/QA problems need reasoning or text answers
- Test requires code with test cases, which doesn't apply to MATH/QA
- Using Programmer/Test causes NoneType errors and 100% failure rate

CORRECT example for MATH/QA:
async def __call__(self, problem: str):
    solution = await self.answer_generate(input=problem)  # âœ… Step-by-step reasoning
    return solution['answer'], self.llm.get_usage_summary()["total_cost"]

WRONG example (causes failure):
async def __call__(self, problem: str):
    code = await self.programmer(problem=problem)  # âŒ DO NOT use for MATH/QA!
    result = await self.test(problem=problem, solution=code)  # âŒ Fails!

CRITICAL RULES:
- Only use operators listed below with their EXACT parameters
- Initialize ALL variables before using them - never return undefined variables
- If a variable is defined inside an if-block, either initialize it before the if-block OR handle both branches
- Design your workflow freely - you decide which operators to use and how to combine them (but respect the constraints above)

Available Operators:

1. Custom(llm) - Most flexible, for any custom task
   Call: await self.custom(input=str, instruction=str)
   Returns: {{'response': str}}

2. AnswerGenerate(llm) - Step-by-step reasoning
   Call: await self.answer_generate(input=str)  â† NO instruction parameter!
   Returns: {{'thought': str, 'answer': str}}

3. Programmer(llm) - Auto-generate and execute Python code
   Call: await self.programmer(problem=str, analysis=str)
   Returns: {{'code': str, 'output': str}}

4. Test(llm) - Test code with test cases
   Call: await self.test(problem=str, solution=str, entry_point=str)
   Returns: {{'result': bool, 'solution': str}}

5. Review(llm) - Review and validate solution
   Call: await self.review(problem=str, solution=str)
   Returns: {{'review_result': bool, 'feedback': str}}

6. Revise(llm) - Revise solution based on feedback
   Call: await self.revise(problem=str, solution=str, feedback=str)
   Returns: {{'solution': str}}

7. ScEnsemble(llm) - Self-consistency ensemble voting
   Call: await self.sc_ensemble(solutions=list, problem=str)
   Returns: {{'response': str}}

Template (complete the __call__ method):

from scripts.operators import Custom, AnswerGenerate, Programmer, Test, Review, Revise, ScEnsemble
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)

        # âš ï¸ CRITICAL: Initialize ALL operators you will use in __call__!
        # Example 1: If you only need answer_generate:
        # self.answer_generate = AnswerGenerate(self.llm)

        # Example 2: If you need review:
        # self.answer_generate = AnswerGenerate(self.llm)
        # self.review = Review(self.llm)

        # Example 3: Full workflow with programmer and test:
        # self.programmer = Programmer(self.llm)
        # self.test = Test(self.llm)
        # self.review = Review(self.llm)

        # Available operators (initialize only what you need):
        # self.custom = Custom(self.llm)
        # self.answer_generate = AnswerGenerate(self.llm)
        # self.programmer = Programmer(self.llm)
        # self.test = Test(self.llm)
        # self.review = Review(self.llm)
        # self.sc_ensemble = ScEnsemble(self.llm)

    async def __call__(self, problem: str):
        # Solve: {problem}
        # CRITICAL: MUST return (answer_string, cost_float) tuple
        # - First value MUST be the final answer (string)
        # - Second value MUST be the cost (float, from self.llm.get_usage_summary()["total_cost"])
        #
        # WRONG: NEVER return (code, answer) - this will cause type errors
        # CORRECT: ALWAYS return (answer, cost)

        # Example 1 - Simple workflow:
        # solution = await self.answer_generate(input=problem)
        # return solution['answer'], self.llm.get_usage_summary()["total_cost"]

        # Example 2 - Review loop:
        # solution = await self.answer_generate(input=problem)
        # review = await self.review(problem=problem, solution=solution['answer'])
        # if not review['review_result']:
        #     # Regenerate or use feedback to guide next attempt
        #     solution = await self.answer_generate(input=problem + "\n" + review['feedback'])
        # return solution['answer'], self.llm.get_usage_summary()["total_cost"]

        # Example 3 - Code problem workflow:
        # code_result = await self.programmer(problem=problem, analysis='None')
        # test_result = await self.test(problem=problem, solution=code_result['code'], entry_point='solution')
        # if test_result['result']:
        #     return test_result['solution'], self.llm.get_usage_summary()["total_cost"]
        # return code_result['output'], self.llm.get_usage_summary()["total_cost"]

        # IMPORTANT: Always initialize variables before any if-blocks!
        # Good:
        #   answer = await self.answer_generate(input=problem)
        #   final = answer['answer']  # Initialize
        #   if condition:
        #       final = modified  # Modify
        #   return final, cost  # Always defined
        #
        # Bad (NEVER):
        #   if condition:
        #       answer = ...  # Only in if-block
        #   return answer, cost  # ERROR if condition is False!

        pass
"""

        return prompt

    def generate_workflow(
        self,
        problem: str,
        problem_type: str = "math",
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        return_full_output: bool = False,
        custom_prompt: Optional[str] = None
    ) -> Dict:
        """
        ç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ

        Args:
            problem: é—®é¢˜æ–‡æœ¬
            problem_type: é—®é¢˜ç±»å‹ (math/code/qa)
            temperature: é‡‡æ ·æ¸©åº¦
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            return_full_output: æ˜¯å¦è¿”å›å®Œæ•´è¾“å‡º
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼Œå°†è¦†ç›–é»˜è®¤æç¤ºè¯ï¼‰

        Returns:
            {
                "workflow_code": "Pythonä»£ç ",
                "valid": bool,
                "error": Optional[str],
                "metadata": {...}
            }
        """

        # æ„å»ºæç¤ºè¯ï¼ˆæ”¯æŒåŠ¨æ€æ³¨å…¥ï¼‰
        if custom_prompt is not None:
            prompt = custom_prompt
        else:
            prompt = self._build_generation_prompt(problem, problem_type)

        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=self.config.get('top_p', 0.95),
                top_k=self.config.get('top_k', 50),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=False  # âœ… Fix: Disable cache when gradient checkpointing is enabled
            )

        # è§£ç 
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        # è§£æè¾“å‡º
        workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

        result = {
            "workflow_code": workflow_code,
            "valid": is_valid,
            "error": error,
            "metadata": {
                "problem": problem,
                "problem_type": problem_type,
                "temperature": temperature,
                "tokens_generated": outputs.shape[1] - inputs['input_ids'].shape[1]
            }
        }

        if return_full_output:
            result["full_output"] = generated_text
            result["prompt"] = prompt

        return result

    def generate_workflows_batch(
        self,
        problems: List[Dict],
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
    ) -> List[Dict]:
        """
        æ‰¹é‡ç”Ÿæˆå·¥ä½œæµï¼ˆ8å€åŠ é€Ÿï¼‰

        ç”¨é€”ï¼šåœ¨GRPOè®­ç»ƒä¸­æ‰¹é‡ç”Ÿæˆå¤šä¸ªé—®é¢˜çš„å·¥ä½œæµï¼Œæ˜¾è‘—åŠ é€Ÿ

        Args:
            problems: [
                {'text': str, 'type': str},  # problem_type: 'math'/'code'/'qa'
                ...
            ]
            temperature: é‡‡æ ·æ¸©åº¦
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            [{
                "workflow_code": str,
                "valid": bool,
                "error": Optional[str],
                "metadata": {...}
            }, ...]

        æ€§èƒ½å¯¹æ¯”ï¼š
        - Sequential: N problems Ã— 100ms/problem = 100N ms
        - Batch: 1 forward pass + N decode â‰ˆ 300-500 ms (vs 10000-15000 ms)
        - åŠ é€Ÿ: 20-30xï¼ˆå–å†³äºé—®é¢˜å¤æ‚åº¦ï¼‰
        """
        print(f"ğŸš€ æ‰¹é‡ç”Ÿæˆ {len(problems)} ä¸ªå·¥ä½œæµ...")

        # 1. æ„å»ºæ‰€æœ‰æç¤ºè¯
        prompts = []
        for problem in problems:
            prompt = self._build_generation_prompt(
                problem['text'],
                problem['type']
            )
            prompts.append(prompt)

        # 2. æ‰¹é‡tokenizeï¼ˆå¸¦paddingï¼‰
        print(f"  ğŸ“ Tokenizing {len(prompts)} prompts...")
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,  # è‡ªåŠ¨paddingåˆ°æœ€é•¿é•¿åº¦
            truncation=True,
            max_length=2048
        ).to(self.device)

        # 3. æ‰¹é‡ç”Ÿæˆ
        print(f"  ğŸ”¨ Generating...")
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=self.config.get('top_p', 0.95),
                top_k=self.config.get('top_k', 50),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                # å…³é”®ï¼šä½¿ç”¨num_beams=1é¿å…beam searchçš„é¢å¤–å¼€é”€
                num_beams=1,
                use_cache=False  # âœ… Fix: Disable cache when gradient checkpointing is enabled
            )

        # 4. æ‰¹é‡è§£ç å’Œè§£æ
        print(f"  ğŸ” Decoding and parsing...")
        results = []
        for i, (problem, output_seq) in enumerate(zip(problems, outputs)):
            try:
                # è§£ç ï¼šè·³è¿‡è¾“å…¥éƒ¨åˆ†ï¼Œåªå–ç”Ÿæˆçš„éƒ¨åˆ†
                input_length = inputs['input_ids'][i].shape[0]
                generated_text = self.tokenizer.decode(
                    output_seq[input_length:],
                    skip_special_tokens=True
                )

                # è§£æå·¥ä½œæµä»£ç 
                workflow_code, is_valid, error = self._parse_workflow_code(
                    generated_text,
                    problem['type']
                )

                results.append({
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "problem": problem['text'],
                        "problem_type": problem['type'],
                        "temperature": temperature,
                        "tokens_generated": output_seq.shape[0] - input_length
                    }
                })

            except Exception as e:
                print(f"    âš ï¸  Problem {i} è§£æå¤±è´¥: {str(e)}")
                results.append({
                    "workflow_code": self._get_default_workflow(problem['type']),
                    "valid": False,
                    "error": str(e),
                    "metadata": {
                        "problem": problem['text'],
                        "problem_type": problem['type'],
                        "error_type": "parsing"
                    }
                })

        print(f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ: {len(results)} ä¸ªå·¥ä½œæµ")
        return results

    def _parse_workflow_code(self, generated_text: str, problem_type: str) -> Tuple[str, bool, Optional[str]]:
        """
        è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¹¶ä½¿ç”¨reactive patchingè¿›è¡Œä¿®å¤

        æ–°ç­–ç•¥ï¼šä½¿ç”¨WorkflowValidatorV2çš„reactive patchingæ¨¡å¼ï¼ˆå‚è€ƒé¡¹ç›®éªŒè¯è¿‡ï¼‰
        - åªä¿®å¤å®é™…é—®é¢˜ï¼Œä¸åšå®Œæ•´é‡æ„
        - æ›´å¿«ã€æ›´å¯é ã€æ›´å°‘å‰¯ä½œç”¨
        """

        # DEBUG: æ‰“å° Qwen ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬
        print(f"\n{'='*60}")
        print(f"ğŸ” DEBUG: Qwen ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬ (å®Œæ•´):")
        print(f"{'='*60}")
        print(generated_text)  # æ‰“å°å®Œæ•´æ–‡æœ¬
        print(f"{'='*60}\n")

        try:
            # 1. æå–ä»£ç å—ï¼ˆæ”¯æŒmarkdownå’Œçº¯ä»£ç æ ¼å¼ï¼‰
            code = self._extract_code_block(generated_text)
            if not code:
                print(f"âŒ æ— æ³•ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–ä»£ç å—")
                return self._get_default_workflow(problem_type), False, "No code block found"

            # 2. ä½¿ç”¨WorkflowValidatorV2è¿›è¡Œreactive patchingéªŒè¯
            print(f"ğŸ”§ ä½¿ç”¨reactive patchingè¿›è¡ŒéªŒè¯å’Œä¿®å¤...")
            fixed_code, is_valid, error_msg, fixes = self.validator.validate_and_fix_workflow(
                code=code,
                problem_type=problem_type
            )

            if is_valid:
                print(f"âœ… éªŒè¯æˆåŠŸ")
                if fixes:
                    print(f"   åº”ç”¨äº†ä»¥ä¸‹ä¿®å¤: {fixes}")
                return fixed_code, True, None
            else:
                print(f"âŒ éªŒè¯å¤±è´¥: {error_msg}")
                if fixes:
                    print(f"   å°è¯•ä¿®å¤: {fixes}")
                # å¦‚æœä¿®å¤åé€šè¿‡äº†åŸºæœ¬è¯­æ³•æ£€æŸ¥ï¼Œä»è¿”å›ä¿®å¤åçš„ä»£ç 
                # å¦åˆ™ä½¿ç”¨é»˜è®¤å·¥ä½œæµ
                try:
                    compile(fixed_code, '<string>', 'exec')
                    print(f"âš ï¸ ä»£ç å¯ç¼–è¯‘ï¼Œä½¿ç”¨ä¿®å¤ç‰ˆæœ¬")
                    return fixed_code, False, error_msg
                except:
                    print(f"âŒ ä¿®å¤åä»æ— æ³•ç¼–è¯‘ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
                    return self._get_default_workflow(problem_type), False, error_msg

        except Exception as e:
            print(f"âŒ å¼‚å¸¸æ•è·: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._get_default_workflow(problem_type), False, str(e)

    def _get_default_workflow(self, problem_type: str = "math") -> str:
        """é»˜è®¤å·¥ä½œæµï¼ˆå½“ç”Ÿæˆå¤±è´¥æ—¶ï¼‰"""
        return f"""from scripts.operators import Custom, AnswerGenerate, Programmer, Test, Review, Revise, ScEnsemble
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = Custom(self.llm)

    async def __call__(self, problem: str):
        solution = await self.custom(input=problem, instruction="Solve this problem step by step.")
        return solution['response'], self.llm.get_usage_summary()["total_cost"]
"""

    def _extract_code_block(self, generated_text: str) -> str:
        """
        ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–Pythonä»£ç å—

        æ”¯æŒæ ¼å¼ï¼š
        1. Markdownä»£ç å—ï¼š```python ... ```
        2. ç®€å•ä»£ç å—ï¼š``` ... ```
        3. çº¯ä»£ç ï¼ˆæ²¡æœ‰åŒ…è£¹ï¼‰
        """
        import re

        # å°è¯•æå–markdownä»£ç å—
        # Pattern 1: ```python ... ```
        python_pattern = r'```python\s*\n(.*?)\n```'
        match = re.search(python_pattern, generated_text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # Pattern 2: ``` ... ```
        general_pattern = r'```\s*\n(.*?)\n```'
        match = re.search(general_pattern, generated_text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # Pattern 3: æŸ¥æ‰¾class Workflowå®šä¹‰
        class_pattern = r'(class\s+Workflow\s*:.*?(?=\n\n|\Z))'
        match = re.search(class_pattern, generated_text, re.DOTALL)
        if match:
            # æ‰¾åˆ°classå¼€å§‹ä½ç½®
            start_pos = match.start()
            # è·å–classä¹‹åçš„æ‰€æœ‰å†…å®¹
            code_after_class = generated_text[start_pos:]

            # å°è¯•æ‰¾åˆ°åˆé€‚çš„ç»“æŸç‚¹
            lines = code_after_class.split('\n')
            code_lines = []
            indent_level = None

            for line in lines:
                # å¦‚æœæ˜¯ç©ºè¡Œï¼Œç»§ç»­
                if not line.strip():
                    code_lines.append(line)
                    continue

                # è·å–å½“å‰è¡Œçš„ç¼©è¿›
                current_indent = len(line) - len(line.lstrip())

                # å¦‚æœè¿™æ˜¯ç¬¬ä¸€è¡Œä»£ç ï¼Œè®°å½•ç¼©è¿›çº§åˆ«
                if indent_level is None and line.strip().startswith(('class', 'def', 'import', 'from')):
                    indent_level = current_indent

                # å¦‚æœé‡åˆ°åŒçº§æˆ–æ›´å°ç¼©è¿›ï¼ˆä¸”ä¸æ˜¯ç©ºè¡Œï¼‰ï¼Œå¯èƒ½ç»“æŸäº†
                if indent_level is not None and current_indent <= indent_level - 4:
                    break

                code_lines.append(line)

            return '\n'.join(code_lines)

        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸæ–‡æœ¬ï¼ˆä½†å»é™¤å‰åçš„è§£é‡Šæ–‡å­—ï¼‰
        lines = generated_text.split('\n')
        code_start = -1
        code_end = len(lines)

        for i, line in enumerate(lines):
            if 'class Workflow' in line:
                code_start = i
                break

        if code_start >= 0:
            # ä»class Workflowå¼€å§‹
            return '\n'.join(lines[code_start:code_end])

        # æœ€åå°è¯•ï¼šå¦‚æœæ–‡æœ¬åŒ…å«Pythonä»£ç ç‰¹å¾ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬
        if any(keyword in generated_text for keyword in ['class Workflow', 'def __call__', 'import', 'from']):
            return generated_text.strip()

        return ""


def test_generator():
    """æµ‹è¯•ç”Ÿæˆå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•RLå·¥ä½œæµç”Ÿæˆå™¨")
    print("=" * 60)

    # æ³¨æ„ï¼šè¿™éœ€è¦Qwenæ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰ä¸‹è½½ä¼šå¾ˆæ…¢
    generator = RLWorkflowGenerator(
        base_model="Qwen/Qwen2.5-7B-Instruct",
        device_ids=[2, 3],
        operator_descriptions_path="/home/yijia/.claude/11/AFlow/workspace/MATH/workflows/template/operator.json"
    )

    # æµ‹è¯•é—®é¢˜
    test_problem = "What is 15 + 27?"

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_problem}")

    # ç”Ÿæˆå·¥ä½œæµ
    result = generator.generate_workflow(
        problem=test_problem,
        problem_type="math",
        temperature=0.7,
        max_new_tokens=1024
    )

    print(f"\nâœ… ç”Ÿæˆç»“æœ:")
    print(f"  æœ‰æ•ˆæ€§: {result['valid']}")
    if result['error']:
        print(f"  é”™è¯¯: {result['error']}")

    print(f"\nğŸ“„ ç”Ÿæˆçš„å·¥ä½œæµä»£ç :")
    print(result['workflow_code'])


def _extract_code_block(self, generated_text: str) -> str:
        """
        ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–Pythonä»£ç å—

        æ”¯æŒæ ¼å¼ï¼š
        1. Markdownä»£ç å—ï¼š```python ... ```
        2. ç®€å•ä»£ç å—ï¼š``` ... ```
        3. çº¯ä»£ç ï¼ˆæ²¡æœ‰åŒ…è£¹ï¼‰
        """
        import re

        # å°è¯•æå–markdownä»£ç å—
        # Pattern 1: ```python ... ```
        python_pattern = r'```python\s*\n(.*?)\n```'
        match = re.search(python_pattern, generated_text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # Pattern 2: ``` ... ```
        general_pattern = r'```\s*\n(.*?)\n```'
        match = re.search(general_pattern, generated_text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # Pattern 3: æŸ¥æ‰¾class Workflowå®šä¹‰
        class_pattern = r'(class\s+Workflow\s*:.*?(?=\n\n|\Z))'
        match = re.search(class_pattern, generated_text, re.DOTALL)
        if match:
            # æ‰¾åˆ°classå¼€å§‹ä½ç½®
            start_pos = match.start()
            # è·å–classä¹‹åçš„æ‰€æœ‰å†…å®¹
            code_after_class = generated_text[start_pos:]

            # å°è¯•æ‰¾åˆ°åˆé€‚çš„ç»“æŸç‚¹
            lines = code_after_class.split('\n')
            code_lines = []
            indent_level = None

            for line in lines:
                # å¦‚æœæ˜¯ç©ºè¡Œï¼Œç»§ç»­
                if not line.strip():
                    code_lines.append(line)
                    continue

                # è·å–å½“å‰è¡Œçš„ç¼©è¿›
                current_indent = len(line) - len(line.lstrip())

                # å¦‚æœè¿™æ˜¯ç¬¬ä¸€è¡Œä»£ç ï¼Œè®°å½•ç¼©è¿›çº§åˆ«
                if indent_level is None and line.strip().startswith(('class', 'def', 'import', 'from')):
                    indent_level = current_indent

                # å¦‚æœé‡åˆ°åŒçº§æˆ–æ›´å°ç¼©è¿›ï¼ˆä¸”ä¸æ˜¯ç©ºè¡Œï¼‰ï¼Œå¯èƒ½ç»“æŸäº†
                if indent_level is not None and current_indent <= indent_level - 4:
                    break

                code_lines.append(line)

            return '\n'.join(code_lines)

        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸæ–‡æœ¬ï¼ˆä½†å»é™¤å‰åçš„è§£é‡Šæ–‡å­—ï¼‰
        lines = generated_text.split('\n')
        code_start = -1
        code_end = len(lines)

        for i, line in enumerate(lines):
            if 'class Workflow' in line:
                code_start = i
                break

        if code_start >= 0:
            # ä»class Workflowå¼€å§‹
            return '\n'.join(lines[code_start:code_end])

        # æœ€åå°è¯•ï¼šå¦‚æœæ–‡æœ¬åŒ…å«Pythonä»£ç ç‰¹å¾ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬
        if any(keyword in generated_text for keyword in ['class Workflow', 'def __call__', 'import', 'from']):
            return generated_text.strip()

        return ""


if __name__ == "__main__":
    test_generator()
