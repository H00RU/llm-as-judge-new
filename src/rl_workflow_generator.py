#!/usr/bin/env python3
"""
RLå·¥ä½œæµç”Ÿæˆå™¨ - ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ
"""
import torch
import json
import ast
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, LoraConfig, get_peft_model
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import sys
import os

class RLWorkflowGenerator:
    """ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ"""

    def __init__(
        self,
        base_model: str = "Qwen/Qwen2.5-7B-Instruct",
        lora_checkpoint: Optional[str] = None,
        device_ids: List[int] = [2, 3],
        operator_descriptions_path: Optional[str] = None,
        config: Optional[Dict] = None
    ):
        """
        Args:
            base_model: åŸºåº§æ¨¡å‹è·¯å¾„
            lora_checkpoint: LoRAæ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨åŸºåº§æ¨¡å‹ï¼‰
            device_ids: ä½¿ç”¨çš„GPU IDåˆ—è¡¨
            operator_descriptions_path: AFlowç®—å­æè¿°æ–‡ä»¶è·¯å¾„
            config: é¢å¤–é…ç½®
        """
        self.base_model = base_model
        self.lora_checkpoint = lora_checkpoint
        self.device_ids = device_ids
        self.device = f"cuda:{device_ids[0]}" if torch.cuda.is_available() else "cpu"
        self.config = config or {}

        # è®¾ç½®CUDAè®¾å¤‡
        if torch.cuda.is_available():
            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, device_ids))

        print(f"ğŸ”§ åˆå§‹åŒ–RLå·¥ä½œæµç”Ÿæˆå™¨")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  GPU: {device_ids}")

        # åŠ è½½tokenizer
        print(f"ğŸ“¥ åŠ è½½tokenizer: {base_model}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model,
            trust_remote_code=True
        )

        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {base_model}")
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16,
            device_map={"": self.device},
            trust_remote_code=True
        )

        # åŠ è½½LoRAæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if lora_checkpoint:
            print(f"ğŸ“¥ åŠ è½½LoRAæ£€æŸ¥ç‚¹: {lora_checkpoint}")
            self.model = PeftModel.from_pretrained(self.model, lora_checkpoint)
            self.model.eval()

        # åŠ è½½ç®—å­æè¿°
        self.operator_descriptions = self._load_operator_descriptions(operator_descriptions_path)

        print(f"âœ… RLå·¥ä½œæµç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_operator_descriptions(self, descriptions_path: Optional[str]) -> Dict:
        """åŠ è½½AFlowç®—å­æè¿°"""
        if descriptions_path and Path(descriptions_path).exists():
            with open(descriptions_path, 'r') as f:
                return json.load(f)

        # é»˜è®¤ç®—å­æè¿°
        return {
            "Custom": {
                "description": "Generates anything based on customized input and instruction.",
                "interface": "custom(input: str, instruction: str) -> dict with key 'response'"
            },
            "AnswerGenerate": {
                "description": "Generates step-by-step reasoning and final answer.",
                "interface": "answer_generate(input: str) -> dict with keys 'thought' and 'answer'"
            },
            "Programmer": {
                "description": "Automatically writes and executes Python code.",
                "interface": "programmer(problem: str, analysis: str = 'None') -> dict with keys 'code' and 'output'"
            },
            "ScEnsemble": {
                "description": "Uses self-consistency to select the most frequent solution.",
                "interface": "sc_ensemble(solutions: List[str], problem: str) -> dict with key 'response'"
            },
            "Review": {
                "description": "Reviews and provides feedback on a solution.",
                "interface": "review(problem: str, solution: str) -> dict with keys 'review_result' and 'feedback'"
            },
            "Revise": {
                "description": "Revises solution based on feedback.",
                "interface": "revise(problem: str, solution: str, feedback: str) -> dict with key 'solution'"
            }
        }

    def _build_generation_prompt(self, problem: str, problem_type: str) -> str:
        """æ„å»ºæç¤ºè¯ï¼Œæ˜ç¡®ç®—å­ APIï¼ˆå¢å¼ºç‰ˆ - å«Few-shotç¤ºä¾‹ï¼‰"""

        # Few-shotæ­£ç¡®ç¤ºä¾‹ï¼ˆ3ä¸ªç¤ºä¾‹è¦†ç›–ä¸åŒåœºæ™¯ï¼‰
        few_shot_example = """â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… EXAMPLE 1: SIMPLE QA WORKFLOW (MOST COMMON)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

```python
import workspace.qa.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)  # âœ“ CORRECT: 'model'
        self.answer_generate = operator.AnswerGenerate(self.model)  # âœ“ CORRECT

    async def __call__(self, problem: str, entry_point: str = None):
        result = await self.answer_generate(input=problem)
        answer = result.get('answer', '') if isinstance(result, dict) else str(result)
        cost = self.model.get_usage_summary()["total_cost"]
        return answer, cost
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… EXAMPLE 2: CODE WORKFLOW WITH TEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ï¿½ï¿½â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

```python
import workspace.code.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)  # âœ“ CORRECT: 'model'
        self.programmer = operator.Programmer(self.model)  # âœ“ CORRECT
        self.test = operator.Test(self.model)  # âœ“ CORRECT

    async def __call__(self, problem: str, entry_point: str = None):
        # Generate code
        prog_result = await self.programmer(problem=problem, analysis='')
        code = prog_result.get('code', '') if isinstance(prog_result, dict) else str(prog_result)

        # Test code if entry_point available
        if entry_point:
            test_result = await self.test(problem=problem, solution=code, entry_point=entry_point)
            if isinstance(test_result, dict) and test_result.get('result', False):
                code = test_result.get('solution', code)

        cost = self.model.get_usage_summary()["total_cost"]
        return code, cost
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… EXAMPLE 3: MATH WITH REVIEW-REVISE LOOP
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

```python
import workspace.math.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)  # âœ“ CORRECT: 'model'
        self.answer_generate = operator.AnswerGenerate(self.model)
        self.review = operator.Review(self.model)
        self.revise = operator.Revise(self.model)  # âœ“ All three initialized

    async def __call__(self, problem: str, entry_point: str = None):
        # Generate initial answer
        result = await self.answer_generate(input=problem)
        answer = result.get('answer', '') if isinstance(result, dict) else str(result)

        # Review and potentially revise
        review_result = await self.review(problem=problem, solution=answer)
        if isinstance(review_result, dict) and not review_result.get('review_result', True):
            feedback = review_result.get('feedback', '')
            revise_result = await self.revise(problem=problem, solution=answer, feedback=feedback)
            answer = revise_result.get('solution', answer) if isinstance(revise_result, dict) else str(revise_result)

        cost = self.model.get_usage_summary()["total_cost"]
        return answer, cost
```

ğŸš« COMMON MISTAKES - NEVER DO THESE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âŒ MISTAKE 1: Wrong variable name
   IMPORTANT: The LLM instance variable MUST be named 'model' (single token)
   âœ… CORRECT:   self.model = create_llm_instance(llm_config)
   âŒ WRONG:     self.llm = ...  (causes tokenizer issues)
   âŒ WRONG:     self.language_model = ...

âŒ MISTAKE 2: Using undefined variables
   if cond: code = ...
   return code  # âŒ code undefined if cond is False!
   â†’ âœ… CORRECT:
   code = None  # Initialize first!
   if cond: code = ...
   return code

âŒ MISTAKE 3: Calling .get() on non-dict (causes NoneType errors)
   result = await operator()  # might return str!
   value = result.get('key')  # âŒ AttributeError if result is str
   â†’ âœ… CORRECT:
   value = result.get('key') if isinstance(result, dict) else result

âŒ MISTAKE 4: Confusing Review vs Revise operators
   self.revise = operator.Revise(self.model)  # âŒ Revise not initialized
   await self.revise(...)  # âŒ AttributeError: 'Workflow' has no 'revise'
   â†’ âœ… CORRECT:
   # In __init__: Initialize what you use
   self.review = operator.Review(self.model)  # âœ“
   # In __call__:
   await self.review(problem=problem, solution=solution)  # âœ“

   # If you need Revise, initialize it too:
   self.revise_op = operator.Revise(self.model)  # âœ“ Different name
   await self.revise_op(problem=problem, solution=sol, feedback=fb)  # âœ“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""

        prompt = few_shot_example + f"""Now generate YOUR Workflow for the following problem.

CRITICAL RULES:
1. Use EXACT variable name: 'model' NOT 'llm', 'll_m', or 'language_model'
2. Initialize ALL variables before if-blocks
3. Always check isinstance(result, dict) before calling .get()
4. __call__ signature: async def __call__(self, problem: str, entry_point: str = None)
5. Always return (solution_string, cost_float) tuple

Available Operators:

1. Custom(model) - Most flexible, for any custom task
   Call: await self.custom(input=str, instruction=str)
   Returns: {{'response': str}}

2. AnswerGenerate(model) - Step-by-step reasoning
   Call: await self.answer_generate(input=str)  â† NO instruction parameter!
   Returns: {{'thought': str, 'answer': str}}

3. Programmer(model) - Auto-generate and execute Python code
   Call: await self.programmer(problem=str, analysis=str)
   Returns: {{'code': str, 'output': str}}

4. Test(model) - Test code with test cases (uses entry_point to look up test cases automatically)
   Call: await self.test(problem=str, solution=str, entry_point=str)  â† NO 'test' parameter!
   Returns: {{'result': bool, 'solution': str}}

5. Review(model) - Review and validate solution
   Call: await self.review(problem=str, solution=str)
   Returns: {{'review_result': bool, 'feedback': str}}

6. Revise(model) - Revise solution based on feedback
   Call: await self.revise(problem=str, solution=str, feedback=str)
   Returns: {{'solution': str}}

7. ScEnsemble(model) - Self-consistency ensemble voting
   Call: await self.sc_ensemble(solutions=list, problem=str)
   Returns: {{'response': str}}

"""

        # L2.1: æ·»åŠ é—®é¢˜ç±»å‹ç‰¹å®šçš„çº¦æŸï¼ˆæ–¹æ¡ˆBï¼šè½¯å»ºè®®è€Œéç¡¬å‘½ä»¤ï¼‰
        if problem_type == "qa":
            problem_specific = """
ğŸ“‹ RECOMMENDED: QA PROBLEMS (problem_type="qa")
================================================================================
âš ï¸  CONSTRAINTS (violation penalty: -5.0 reward):
  âŒ Avoid Test operator - QA typically has no automated test cases
     Using Test will likely cause NoneType errors (penalty: -5.0)
  âŒ Avoid Programmer operator - QA is text-based, not code-related
     Using Programmer is inefficient (penalty: -5.0)
  âŒ Avoid entry_point parameter - QA problems don't have entry_point
     Using entry_point will cause parameter errors (penalty: -5.0)

âœ… PREFERRED operators for QA:
  âœ… Custom(model) - Most flexible for text-based tasks
  âœ… AnswerGenerate(model) - Generate reasoning and answers (RECOMMENDED)
  âœ… Review(model) - Validate answer quality
  âœ… Revise(model) - Improve answers based on feedback
  âœ… ScEnsemble(model) - Ensemble multiple candidates

Example workflow structure for QA:
  answer = await self.answer_generate(input=problem)
  # ... optionally review and revise ...
  return answer['answer'], cost

Note: You can try other operators, but they will receive penalty in reward.
================================================================================
"""
        elif problem_type == "code":
            problem_specific = """
âœ… CRITICAL: CODE PROBLEMS (problem_type="code") - REQUIRE Test OPERATOR!
================================================================================
MUST use these operators with CODE problems:
  âœ… Programmer(model) - Generate and improve Python code
  âœ… Test(model) - Validate code with entry_point (CRITICAL!)

Test operator MUST be used to verify code correctness:
  - Test signature: await self.test(problem=str, solution=str, entry_point=str)
  - entry_point is the function name you're implementing (e.g., "has_close_elements")
  - Test operator finds test cases automatically using entry_point
  - DO NOT pass 'test' parameter - Test finds it automatically!

Example workflow for CODE:
  code_result = await self.programmer(problem=problem, analysis='')
  code = code_result['code']
  test_result = await self.test(problem=problem, solution=code, entry_point=entry_point)
  if test_result['result']:
      return code, cost
  else:
      # Optionally revise based on test failure
      ...

CRITICAL: entry_point will NOT be None/empty for code problems!
================================================================================
"""
        elif problem_type == "math":
            problem_specific = """
ğŸ“Š RECOMMENDED: MATH PROBLEMS (problem_type="math")
================================================================================
âš ï¸  CONSTRAINTS (violation penalty: -5.0 reward):
  âŒ Avoid Test operator - Math has no automated test cases
     Using Test will cause NoneType errors (penalty: -5.0)
  âŒ Avoid Programmer operator - Math is not code-related
     Using Programmer is inefficient (penalty: -5.0)
  âŒ Avoid entry_point parameter - Math problems don't have entry_point
     Using entry_point will cause parameter errors (penalty: -5.0)

âœ… PREFERRED operators for MATH:
  âœ… Custom(model) - Flexible mathematical reasoning
  âœ… AnswerGenerate(model) - Step-by-step mathematical reasoning (RECOMMENDED)
  âœ… Review(model) - Verify mathematical correctness
  âœ… Revise(model) - Improve solution based on feedback

Example workflow for MATH:
  answer = await self.answer_generate(input=problem)
  return answer['answer'], cost

Note: You can try other operators, but they will receive penalty in reward.
================================================================================
"""
        else:
            problem_specific = ""

        prompt += problem_specific + """
Template (complete the __call__ method):

import workspace.{problem_type}.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)
        # Initialize operators you need (ONLY the ones you will use):
        # self.custom = operator.Custom(self.model)
        # self.answer_generate = operator.AnswerGenerate(self.model)
        # self.programmer = operator.Programmer(self.model)
        # self.test = operator.Test(self.model)
        # self.review = operator.Review(self.model)
        # self.revise = operator.Revise(self.model)
        # self.sc_ensemble = operator.ScEnsemble(self.model)

    async def __call__(self, problem: str, entry_point: str = None):
        # Solve: {problem}
        # MUST return (solution, cost) tuple
        # Example: return solution['response'], self.model.get_usage_summary()["total_cost"]
        # Note: entry_point is optional, used for code problems (ignored for other types)

        # IMPORTANT: Initialize solution variable before any if-blocks!
        # Good example:
        #   solution = await self.answer_generate(input=problem)
        #   answer = solution.get('answer', '')
        #   if some_condition:
        #       answer = improved_answer  # Modify existing variable
        #   return answer, cost  # Always defined
        #
        # Bad example (NEVER do this):
        #   if some_condition:
        #       answer = ...  # Only defined in if-block
        #   return answer, cost  # ERROR: answer may be undefined!

        pass
"""

        return prompt

    def generate_workflow(
        self,
        problem: str,
        problem_type: str = "math",
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        return_full_output: bool = False,
        custom_prompt: Optional[str] = None
    ) -> Dict:
        """
        ç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ

        Args:
            problem: é—®é¢˜æ–‡æœ¬
            problem_type: é—®é¢˜ç±»å‹ (math/code/qa)
            temperature: é‡‡æ ·æ¸©åº¦
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            return_full_output: æ˜¯å¦è¿”å›å®Œæ•´è¾“å‡º
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼Œå°†è¦†ç›–é»˜è®¤æç¤ºè¯ï¼‰

        Returns:
            {
                "workflow_code": "Pythonä»£ç ",
                "valid": bool,
                "error": Optional[str],
                "metadata": {...}
            }
        """

        # æ„å»ºæç¤ºè¯ï¼ˆæ”¯æŒåŠ¨æ€æ³¨å…¥ï¼‰
        if custom_prompt is not None:
            prompt = custom_prompt
        else:
            prompt = self._build_generation_prompt(problem, problem_type)

        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=self.config.get('top_p', 0.95),
                top_k=self.config.get('top_k', 50),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç 
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        # è§£æè¾“å‡º
        workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

        result = {
            "workflow_code": workflow_code,
            "valid": is_valid,
            "error": error,
            "metadata": {
                "problem": problem,
                "problem_type": problem_type,
                "temperature": temperature,
                "tokens_generated": outputs.shape[1] - inputs['input_ids'].shape[1]
            }
        }

        if return_full_output:
            result["full_output"] = generated_text
            result["prompt"] = prompt

        return result

    def _parse_workflow_code(self, generated_text: str, problem_type: str) -> Tuple[str, bool, Optional[str]]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¹¶éªŒè¯å·¥ä½œæµä»£ç """

        # DEBUG: æ‰“å° Qwen ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬
        print(f"\n{'='*60}")
        print(f"ğŸ” DEBUG: Qwen ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬ (å®Œæ•´):")
        print(f"{'='*60}")
        print(generated_text)  # æ‰“å°å®Œæ•´æ–‡æœ¬
        print(f"{'='*60}\n")

        # æå–ä»£ç å—
        code_start = generated_text.find("```python")
        if code_start == -1:
            # æ²¡æœ‰markdownä»£ç å—ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾classå®šä¹‰
            code_start = generated_text.find("class Workflow:")
            if code_start == -1:
                print(f"âš ï¸  æœªæ‰¾åˆ° 'class Workflow:'ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
                return self._get_default_workflow(problem_type), False, "No Workflow class found in output"

            code = generated_text[code_start:]
        else:
            code_start += len("```python\n")
            code_end = generated_text.find("```", code_start)

            if code_end == -1:
                code = generated_text[code_start:]
            else:
                code = generated_text[code_start:code_end]

        # å»é™¤é¦–å°¾ç©ºç™½
        code = code.strip()

        # ===== å¢å¼ºçš„è¯­æ³•å’Œæ‹¼å†™éªŒè¯ =====
        # Step 1: ASTè¯­æ³•éªŒè¯
        try:
            tree = ast.parse(code)
            is_valid = True
            error = None
        except SyntaxError as e:
            is_valid = False
            error = f"Syntax error: {str(e)}"
            print(f"âš ï¸  è¯­æ³•é”™è¯¯: {error}")
            return self._get_default_workflow(problem_type), False, error

        # Step 2: å˜é‡åæ£€æŸ¥ï¼ˆç¡®ä¿ä½¿ç”¨'model'è€Œé'llm'ï¼‰
        # ç”±äºtokenizerå°†'llm'åˆ†ä¸º['ll', 'm']ä¸¤ä¸ªtokenï¼Œå¯¼è‡´ç”Ÿæˆ'll_m'é”™è¯¯
        # è§£å†³æ–¹æ¡ˆï¼šå¼ºåˆ¶ä½¿ç”¨'model'ï¼ˆå•tokenï¼‰
        typo_patterns = [
            ('self.llm', 'self.model'),  # æ£€æµ‹æ—§çš„self.llmå¹¶ä¿®å¤
            ('.llm', '.model'),           # æ£€æµ‹ä»»ä½•.llmå¹¶ä¿®å¤
        ]

        found_typos = []
        for typo, correct in typo_patterns:
            # ä½¿ç”¨æ­£åˆ™é¿å…åŒ¹é…llm_config
            import re
            pattern = re.escape(typo) + r'(?![a-z_])'  # ç¡®ä¿åé¢ä¸æ˜¯å­—æ¯æˆ–ä¸‹åˆ’çº¿
            if re.search(pattern, code):
                found_typos.append(f"{typo} (should be {correct})")

        if found_typos:
            error = f"Variable name issues detected: {', '.join(found_typos)}"
            print(f"âš ï¸  å˜é‡åé—®é¢˜: {error}")
            # è‡ªåŠ¨ä¿®å¤ï¼šå°†self.llmæ›¿æ¢ä¸ºself.model
            for typo, correct in typo_patterns:
                pattern = re.escape(typo) + r'(?![a-z_])'
                code = re.sub(pattern, correct, code)
            print(f"âœ… å·²è‡ªåŠ¨ä¿®å¤å˜é‡åï¼ˆllmâ†’modelï¼‰")
            error = None

        # Step 3: æ£€æŸ¥æ˜¯å¦å®šä¹‰äº†å¿…è¦çš„æ–¹æ³•
        if 'async def __call__' not in code:
            is_valid = False
            error = "Missing '__call__' method"
            print(f"âš ï¸  ç¼ºå°‘__call__æ–¹æ³•: {error}")
            return self._get_default_workflow(problem_type), False, error

        if 'def __init__' not in code:
            is_valid = False
            error = "Missing '__init__' method"
            print(f"âš ï¸  ç¼ºå°‘__init__æ–¹æ³•: {error}")
            return self._get_default_workflow(problem_type), False, error

        return code, is_valid, error

    def _get_default_workflow(self, problem_type: str = "math") -> str:
        """é»˜è®¤å·¥ä½œæµï¼ˆå½“ç”Ÿæˆå¤±è´¥æ—¶ï¼‰"""
        return f"""import workspace.{problem_type}.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.model = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.model)

    async def __call__(self, problem: str, entry_point: str = None):
        # entry_point is optional, used for code problems
        solution = await self.custom(input=problem, instruction="Solve this problem step by step.")
        response = solution.get('response', '') if isinstance(solution, dict) else str(solution)
        return response, self.model.get_usage_summary()["total_cost"]
"""


def test_generator():
    """æµ‹è¯•ç”Ÿæˆå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•RLå·¥ä½œæµç”Ÿæˆå™¨")
    print("=" * 60)

    # æ³¨æ„ï¼šè¿™éœ€è¦Qwenæ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰ä¸‹è½½ä¼šå¾ˆæ…¢
    generator = RLWorkflowGenerator(
        base_model="Qwen/Qwen2.5-7B-Instruct",
        device_ids=[2, 3],
        operator_descriptions_path=os.path.join(os.getenv("AFLOW_PATH", "./AFlow"), "workspace/MATH/workflows/template/operator.json")
    )

    # æµ‹è¯•é—®é¢˜
    test_problem = "What is 15 + 27?"

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_problem}")

    # ç”Ÿæˆå·¥ä½œæµ
    result = generator.generate_workflow(
        problem=test_problem,
        problem_type="math",
        temperature=0.7,
        max_new_tokens=1024
    )

    print(f"\nâœ… ç”Ÿæˆç»“æœ:")
    print(f"  æœ‰æ•ˆæ€§: {result['valid']}")
    if result['error']:
        print(f"  é”™è¯¯: {result['error']}")

    print(f"\nğŸ“„ ç”Ÿæˆçš„å·¥ä½œæµä»£ç :")
    print(result['workflow_code'])


if __name__ == "__main__":
    test_generator()
