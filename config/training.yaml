# GRPO Training Configuration
# æ·±åº¦èåˆAFlow + ROLLçš„åœ¨çº¿å­¦ä¹ æ¡†æ¶

# ============================================
# Model Configuration
# ============================================
base_model: "Qwen/Qwen2.5-7B-Instruct"
device_mapping: [0]      # GPU ID
physical_gpus: [0]       # å®é™…ç‰©ç†GPU ID
bf16: false              # ä½¿ç”¨float16ç²¾åº¦

# ============================================
# Training Parameters
# ============================================
# æ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼š500 steps x 8 batch x 6 workflow per batch = 24,000ä¸ªå·¥ä½œæµ
max_steps: 500                        # âœ… ç”¨æˆ·éœ€æ±‚ï¼š500æ­¥
rollout_batch_size: 8                 # ğŸ”§ ä¼˜åŒ–ï¼šå¢åŠ åˆ°8ï¼Œå‡å°‘æ¢¯åº¦å™ªå£°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
num_return_sequences_in_group: 6      # âœ… ç”¨æˆ·éœ€æ±‚ï¼š6 workflows per problem (GRPOç»„)

# Learning & Optimization
learning_rate: 0.00005  # ğŸ”§ ä¼˜åŒ–ï¼šé™ä½ä» 0.0001 â†’ 0.00005ï¼Œæƒé‡æ›´æ–°æ›´æ¸©å’Œï¼Œé˜²æ­¢æ¿€è¿›å˜åŒ–
warmup_steps: 100       # ğŸ”§ ä¼˜åŒ–ï¼šè°ƒæ•´ä¸º100æ­¥é€‚åº”500æ­¥è®­ç»ƒï¼ˆæ€»æ­¥æ•°çš„20%ï¼‰
gradient_accumulation_steps: 1
clip_range: 0.2
max_grad_norm: 1.0
weight_decay: 0.01
use_kl_loss: true
kl_loss_coef: 0.1

# ============================================
# Temperature Scheduling
# ============================================
# âœ… ç”¨æˆ·éœ€æ±‚ï¼šå›ºå®šæ¸©åº¦ä¸º0.2ï¼ˆç¦ç”¨åŠ¨æ€è°ƒåº¦ï¼‰
temperature_schedule:
  enabled: false              # âŒ ç¦ç”¨åŠ¨æ€è°ƒåº¦
  initial: 0.2                # å›ºå®šå€¼
  final: 0.2                  # å›ºå®šå€¼ï¼ˆä¸ä½¿ç”¨ï¼‰
  warmup_steps: 0             # ä¸ä½¿ç”¨

# ç”Ÿæˆé…ç½®ï¼ˆå½“temperature_schedule.enabled=falseæ—¶ä½¿ç”¨ï¼‰
generation_config:
  temperature: 0.2            # âœ… å›ºå®šä¸º0.2
  max_tokens: 2048
  top_p: 0.95
  top_k: 40

# ============================================
# LoRA Configuration
# ============================================
use_lora: true
lora_rank: 32  # ğŸ”§ ä¼˜åŒ–ï¼šé™ä½ä» 64 â†’ 32ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼Œé˜²æ­¢æƒé‡é€€åŒ–
lora_alpha: 16  # ğŸ”§ ä¼˜åŒ–ï¼šå¯¹åº”é™ä½ alphaï¼ˆç»´æŒ alpha/rank = 0.5 æ¯”ä¾‹ï¼‰
lora_dropout: 0.1
lora_target_modules: "q_proj,v_proj,up_proj,down_proj"

# ============================================
# Data Configuration
# ============================================
data_dir: "data"      # æ•°æ®æ ¹ç›®å½•ï¼ŒDataManagerä¼šè‡ªåŠ¨æŸ¥æ‰¾mixed/{split}_mixed.jsonl
train_dataset: "data/mixed/train_mixed.jsonl"
test_dataset: "data/mixed/test_mixed.jsonl"

# åŸŸçº§åˆ«æƒé‡é…ç½®
domain_ratios:
  math: 0.4                 # æ•°å­¦40%
  qa: 0.3                   # QA30%
  code: 0.3                 # Code30%

# ============================================
# Reward & Evaluation
# ============================================
reward_weights:
  correctness: 0.7          # æ­£ç¡®æ€§æƒé‡
  efficiency: 0.2           # æ•ˆç‡æƒé‡
  code_quality: 0.1         # ä»£ç è´¨é‡æƒé‡

# è¯„ä¼°ç­–ç•¥ï¼ˆæ— éªŒè¯é›†è®¾è®¡ï¼‰
eval_every: 0               # ç¦ç”¨åœ¨çº¿æµ‹è¯•é›†è¯„ä¼°ï¼ˆé¿å…æµ‹è¯•é›†æ³„éœ²ï¼‰
val_samples: 50             # ä¿ç•™å‚æ•°ï¼ˆæœ€åæ‰‹åŠ¨è¯„ä¼°æ—¶ä½¿ç”¨ï¼‰
log_every: 5                # æ¯5æ­¥è®°å½•è®­ç»ƒæŒ‡æ ‡
save_every: 25              # ğŸ”§ ä¼˜åŒ–ï¼šæ¯25æ­¥ä¿å­˜checkpointï¼ˆæ›´é¢‘ç¹ï¼Œä¾¿äºæ¢å¤å’Œé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼‰

# ============================================
# Checkpointing & Output
# ============================================
output_dir: "checkpoints/qwen25-7b/grpo_mixed"
checkpointing:
  save_dir: "checkpoints/qwen25-7b/grpo_mixed"

# ============================================
# AFlow Configuration
# ============================================
aflow_config_path: "config/aflow_llm.yaml"
aflow_executor_model: "gpt-4o-mini"             # âœ… æ‰§è¡Œå¼•æ“ï¼šä½¿ç”¨OpenAIå®˜æ–¹gpt-4o-mini
aflow_operator_descriptions_path: "config/aflow_operators.yaml"
execution_timeout: 180                     # æ‰§è¡Œè¶…æ—¶ï¼ˆç§’ï¼‰

# ============================================
# Experience Buffer (é«˜è´¨é‡æ ·æœ¬å­˜å‚¨)
# ============================================
experience_buffer:
  buffer_size: 100
  reward_threshold: 8.0      # ä»…ä¿å­˜reward>=8.0çš„æ ·æœ¬
  persistence_dir: "data/experience_buffer"

# ============================================
# Prompt Optimization (Layer 1)
# ============================================
prompt_optimizer:
  enabled: true              # å¯ç”¨åŠ¨æ€æç¤ºè¯ä¼˜åŒ–

# ============================================
# Operator Prompt Enhancement (Layer 2)
# ============================================
operator_prompt_enhancer:
  enabled: true              # å¯ç”¨Operatoræç¤ºè¯å¢å¼º

# ============================================
# W&B Monitoring
# ============================================
wandb:
  enabled: true
  project: "aflow-roll-integration"
  run_name: "grpo-500steps-8batch-6workflows-tuned-planD"
  # api_keyä»ç¯å¢ƒå˜é‡è¯»å–: WANDB_API_KEY

# ============================================
# è®­ç»ƒè¯´æ˜ - OPTIMIZED VERSION (æ–¹æ¡ˆD)
# ============================================
# æ€»æ ·æœ¬æ•°ï¼š500 steps Ã— 8 batch Ã— 6 workflows = 24,000 ä¸ªå·¥ä½œæµç”Ÿæˆå’Œæ‰§è¡Œï¼ˆåŸæ¥12,000ï¼‰
#
# ä¼˜åŒ–ç›®æ ‡ï¼ˆè§£å†³æƒé‡é€€åŒ–é—®é¢˜ï¼‰ï¼š
# 1. é™ä½ LoRA ç§©ï¼ˆ64 â†’ 32ï¼‰é˜²æ­¢è¿‡æ‹Ÿåˆ
# 2. é™ä½å­¦ä¹ ç‡ï¼ˆ0.0001 â†’ 0.00005ï¼‰æƒé‡æ›´æ–°æ›´æ¸©å’Œ
# 3. å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆ4 â†’ 8ï¼‰å‡å°‘æ¢¯åº¦å™ªå£°
# 4. æ›´é¢‘ç¹çš„æ£€æŸ¥ç‚¹ä¿å­˜ï¼ˆæ¯50æ­¥ â†’ æ¯25æ­¥ï¼‰
#
# è®­ç»ƒæµç¨‹ï¼š
# 1. é‡‡æ ·ï¼šbatch_size=8ï¼Œä»mixedæ•°æ®é›†ï¼ˆmath:qa:code=4:3:3ï¼‰å‡è¡¡é‡‡æ ·
# 2. ç”Ÿæˆï¼šæ¯ä¸ªsampleç”Ÿæˆ6ä¸ªworkflowï¼ˆGRPOç»„ï¼‰
# 3. æ‰§è¡Œï¼šAFlowæ‰§è¡Œå™¨è¿è¡Œworkflowsï¼Œè¶…æ—¶180ç§’
# 4. å¥–åŠ±ï¼šLLM Judgeï¼ˆgpt-4o-miniï¼‰è®¡ç®—å¥–åŠ±
# 5. æ›´æ–°ï¼šGRPOä¼˜åŒ–å™¨æ›´æ–°LoRAæƒé‡ï¼ˆæ›´æ¸©å’Œï¼Œé˜²æ­¢æƒé‡å´©æºƒï¼‰
# 6. éªŒè¯ï¼šæ¯5æ­¥è®°å½•æŒ‡æ ‡ï¼Œæ¯25æ­¥ä¿å­˜checkpoint
#
# å…³é”®é…ç½®ç¡®è®¤ï¼ˆä¼˜åŒ–åï¼‰ï¼š
# ğŸ”§ max_steps: 500
# ğŸ”§ rollout_batch_size: 8ï¼ˆä»4å¢åŠ ï¼‰
# ğŸ”§ learning_rate: 0.00005ï¼ˆä»0.0001é™ä½50%ï¼‰
# ğŸ”§ lora_rank: 32ï¼ˆä»64é™ä½50%ï¼Œå…³é”®ä¼˜åŒ–ï¼‰
# ğŸ”§ lora_alpha: 16ï¼ˆå¯¹åº”é™ä½ï¼Œç»´æŒæ¯”ä¾‹ï¼‰
# ğŸ”§ warmup_steps: 100ï¼ˆä»50è°ƒæ•´ï¼Œå¯¹åº”500æ­¥è®­ç»ƒï¼‰
# ğŸ”§ save_every: 25ï¼ˆä»50æ›´é¢‘ç¹ï¼‰
# âœ… num_return_sequences_in_group: 6
# âœ… temperature: å›ºå®šä¸º0.2ï¼ˆéåŠ¨æ€ï¼‰
# âœ… domain_ratios: math 40%, qa 30%, code 30%
# âœ… 5:1æ•°æ®åˆ†å‰²ï¼ˆtrain:test=83.3%:16.7%ï¼‰
