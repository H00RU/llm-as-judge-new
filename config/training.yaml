# GRPO Training Configuration
# 深度融合AFlow + ROLL的在线学习框架

# ============================================
# Model Configuration
# ============================================
base_model: "/root/llm-as-judge-new/models/Qwen2.5-7B-Instruct"  # ✅ 完整的本地模型路径
device_mapping: [0]      # GPU ID
physical_gpus: [0]       # 实际物理GPU ID
bf16: false              # 使用float16精度

# ============================================
# Training Parameters
# ============================================
# ✨ PHASE 3 OPTIMIZATION: Reduced K, optimized KL and gradient accumulation
max_steps: 500
rollout_batch_size: 5                 # ✨ Increased from 4 to 5 (5 samples per batch)
num_return_sequences_in_group: 2      # ✨ PHASE 3: Changed 6→2 (Total: 5×2=10 workflows, reduced from 24)

# Learning & Optimization - ✨ OPTIMIZED
learning_rate: 2.0e-5
warmup_steps: 100
gradient_accumulation_steps: 4        # ✨ PHASE 3: Changed 1→4 (Stabilize training, reduce OOM)
clip_range: 0.2
max_grad_norm: 1.0
weight_decay: 0.01
use_kl_loss: true
kl_loss_coef: 0.005                   # ✨ PHASE 3: Changed 0.1→0.005 (Allow more policy drift, faster learning)

# ============================================
# Temperature Scheduling - ✨ PHASE 3: ENABLED
# ============================================
# ✨ PHASE 3: Enable dynamic temperature for better exploration-exploitation tradeoff
temperature_schedule:
  enabled: true               # ✨ PHASE 3: Changed false→true (Enable temperature scheduling)
  initial: 0.5                # ✨ Early: Higher temp for exploration
  final: 0.15                 # ✨ Late: Lower temp for exploitation
  warmup_steps: 150           # ✨ 150 steps to reach from 0.5→0.15

# 生成配置（备用，当temperature_schedule.enabled=false时使用）
generation_config:
  temperature: 0.2
  max_tokens: 4096            # ✅ Prevent truncation
  top_p: 0.95
  top_k: 50

# ============================================
# LoRA Configuration
# ============================================
use_lora: true
lora_rank: 64          # ✅ 恢复到64（用户要求）
lora_alpha: 64         # ✅ 恢复到64（维持 alpha/rank = 1.0 比例）
lora_dropout: 0.05     # ✅ 改为0.05（参考reference）
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"  # ✅ 更新为reference的target modules

# ============================================
# Data Configuration
# ============================================
data_dir: "data"      # 数据根目录，DataManager会自动查找mixed/{split}_mixed.jsonl
train_dataset: "data/mixed/train_mixed.jsonl"
test_dataset: "data/mixed/test_mixed.jsonl"

# 域级别权重配置
domain_ratios:
  math: 0.4                 # 数学40%
  qa: 0.3                   # QA30%
  code: 0.3                 # Code30%

# ============================================
# Reward & Evaluation
# ============================================
reward_weights:
  correctness: 0.7          # 正确性权重
  efficiency: 0.2           # 效率权重
  code_quality: 0.1         # 代码质量权重

# 评估策略（无验证集设计）
eval_every: 0               # 禁用在线测试集评估（避免测试集泄露）
val_samples: 50             # 保留参数（最后手动评估时使用）
log_every: 5                # 每5步记录训练指标
save_every: 20              # 每20步保存checkpoint

# ============================================
# Checkpointing & Output
# ============================================
output_dir: "checkpoints/qwen25-7b/grpo_mixed"
checkpointing:
  save_dir: "checkpoints/qwen25-7b/grpo_mixed"

# ============================================
# AFlow Configuration
# ============================================
aflow_config_path: "config/aflow_llm.yaml"
aflow_executor_model: "gpt-4o-mini"             # ✅ 执行引擎：使用OpenAI官方gpt-4o-mini
aflow_operator_descriptions_path: "config/aflow_operators.yaml"
execution_timeout: 180                     # 执行超时（秒）

# ============================================
# Experience Buffer (高质量样本存储)
# ============================================
# ✨ PHASE 1 UPDATE: Adjusted threshold for 5-tier system
experience_buffer:
  buffer_size: 100
  reward_threshold: 0.7      # ✨ Changed 8.0→0.7 (Tier 4: good/very close answers)
  persistence_dir: "data/experience_buffer"

# ============================================
# Prompt Optimization (Layer 1)
# ============================================
prompt_optimizer:
  enabled: true              # 启用动态提示词优化

# ============================================
# Operator Prompt Enhancement (Layer 2)
# ============================================
operator_prompt_enhancer:
  enabled: true              # 启用Operator提示词增强

# ============================================
# W&B Monitoring
# ============================================
wandb:
  enabled: true
  project: "aflow-roll-integration"
  run_name: "grpo-500steps-4batch-6workflows-reference-restored"
  # api_key从环境变量读取: WANDB_API_KEY

# ============================================
# 训练说明 - ✨ PHASE 3 OPTIMIZED (优化后的最终配置)
# ============================================
# 总样本数：500 steps × 5 batch × 2 workflows = 5,000 个工作流生成和执行
#
# ✨ 主要优化 (Phase 3)：
# 1. KL系数：0.1 → 0.005（20倍降低，允许更大的策略偏移）
# 2. K值：6 → 2（并发从24→10，减少超时问题）
# 3. 批次大小：4 → 5（增加样本多样性）
# 4. 梯度累积：1 → 4（稳定训练，减少OOM）
# 5. 温度调度：禁用 → 启用（0.5→0.15，平衡探索-开发）
# 6. Experience buffer阈值：8.0 → 0.7（适配5-tier系统）
#
# 训练流程：
# 1. 采样：batch_size=5，从mixed数据集（math:qa:code=4:3:3）均衡采样
# 2. 生成：每个sample生成2个workflow（GRPO组，总10个/step）
# 3. 执行：AFlow执行器运行workflows，超时180秒
# 4. 奖励：RewardComputer v2计算5-tier奖励 [0.0, 0.2, 0.4, 0.7, 1.0]
# 5. 更新：GRPO优化器使用低KL系数更新LoRA权重
# 6. 验证：每5步记录指标，每20步保存checkpoint
#
# ✨ 关键配置（Phase 3优化）：
# ✨ max_steps: 500（保持）
# ✨ rollout_batch_size: 5（改from 4）
# ✨ num_return_sequences_in_group: 2（改from 6，减少并发）
# ✨ kl_loss_coef: 0.005（改from 0.1，允许更大的策略学习）
# ✨ gradient_accumulation_steps: 4（改from 1）
# ✨ temperature_schedule.enabled: true（改from false）
# ✨ temperature_schedule: 0.5→0.15 over 150 steps
# ✨ experience_buffer.reward_threshold: 0.7（改from 8.0，5-tier系统）
# ✅ learning_rate: 2.0e-5（保持）
# ✅ warmup_steps: 100（保持）
# ✅ lora_rank: 64（保持）
# ✅ max_tokens: 4096（保持）
# ✅ domain_ratios: math 40%, qa 30%, code 30%（保持均衡采样）
