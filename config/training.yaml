# GRPO Training Configuration
# æ·±åº¦èåˆAFlow + ROLLçš„åœ¨çº¿å­¦ä¹ æ¡†æ¶

# ============================================
# Model Configuration
# ============================================
base_model: "/root/llm-as-judge-new/models"  # âœ… æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæ— åµŒå¥—ï¼‰
device_mapping: [0]      # GPU ID
physical_gpus: [0]       # å®é™…ç‰©ç†GPU ID
bf16: false              # ä½¿ç”¨float16ç²¾åº¦

# ============================================
# Training Parameters
# ============================================
# æ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼š500 steps x 4 batch x 6 workflow per batch = 12,000ä¸ªå·¥ä½œæµ
max_steps: 500                        # âœ… ç”¨æˆ·éœ€æ±‚ï¼š500æ­¥ï¼ˆæ€»æ­¥æ•°ä¿æŒï¼‰
rollout_batch_size: 4                 # âœ… æ¢å¤åˆ°4ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰
num_return_sequences_in_group: 6      # âœ… ç”¨æˆ·éœ€æ±‚ï¼š6 workflows per problem (GRPOç»„)

# Learning & Optimization
learning_rate: 2.0e-5   # âœ… æ¢å¤åˆ°referenceå€¼ï¼ˆ2.0e-5 = 0.00002ï¼‰
warmup_steps: 100       # âœ… ä¿æŒ100æ­¥ï¼ˆæ€»æ­¥æ•°çš„20%ï¼‰
gradient_accumulation_steps: 1
clip_range: 0.2
max_grad_norm: 1.0
weight_decay: 0.01
use_kl_loss: true
kl_loss_coef: 0.1

# ============================================
# Temperature Scheduling
# ============================================
# âœ… ç”¨æˆ·éœ€æ±‚ï¼šå›ºå®šæ¸©åº¦ä¸º0.4ï¼ˆç¦ç”¨åŠ¨æ€è°ƒåº¦ï¼‰
temperature_schedule:
  enabled: false              # âŒ ç¦ç”¨åŠ¨æ€è°ƒåº¦
  initial: 0.4                # å›ºå®šå€¼
  final: 0.4                  # å›ºå®šå€¼ï¼ˆä¸ä½¿ç”¨ï¼‰
  warmup_steps: 0             # ä¸ä½¿ç”¨

# ç”Ÿæˆé…ç½®ï¼ˆå½“temperature_schedule.enabled=falseæ—¶ä½¿ç”¨ï¼‰
generation_config:
  temperature: 0.4            # âœ… å›ºå®šä¸º0.4ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰
  max_tokens: 4096            # âœ… å¢å¤§é˜²æ­¢æˆªæ–­ï¼ˆå‚è€ƒreferenceï¼‰
  top_p: 0.95
  top_k: 50

# ============================================
# LoRA Configuration
# ============================================
use_lora: true
lora_rank: 64          # âœ… æ¢å¤åˆ°64ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰
lora_alpha: 64         # âœ… æ¢å¤åˆ°64ï¼ˆç»´æŒ alpha/rank = 1.0 æ¯”ä¾‹ï¼‰
lora_dropout: 0.05     # âœ… æ”¹ä¸º0.05ï¼ˆå‚è€ƒreferenceï¼‰
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"  # âœ… æ›´æ–°ä¸ºreferenceçš„target modules

# ============================================
# Data Configuration
# ============================================
data_dir: "data"      # æ•°æ®æ ¹ç›®å½•ï¼ŒDataManagerä¼šè‡ªåŠ¨æŸ¥æ‰¾mixed/{split}_mixed.jsonl
train_dataset: "data/mixed/train_mixed.jsonl"
test_dataset: "data/mixed/test_mixed.jsonl"

# åŸŸçº§åˆ«æƒé‡é…ç½®
domain_ratios:
  math: 0.4                 # æ•°å­¦40%
  qa: 0.3                   # QA30%
  code: 0.3                 # Code30%

# ============================================
# Reward & Evaluation
# ============================================
reward_weights:
  correctness: 0.7          # æ­£ç¡®æ€§æƒé‡
  efficiency: 0.2           # æ•ˆç‡æƒé‡
  code_quality: 0.1         # ä»£ç è´¨é‡æƒé‡

# è¯„ä¼°ç­–ç•¥ï¼ˆæ— éªŒè¯é›†è®¾è®¡ï¼‰
eval_every: 0               # ç¦ç”¨åœ¨çº¿æµ‹è¯•é›†è¯„ä¼°ï¼ˆé¿å…æµ‹è¯•é›†æ³„éœ²ï¼‰
val_samples: 50             # ä¿ç•™å‚æ•°ï¼ˆæœ€åæ‰‹åŠ¨è¯„ä¼°æ—¶ä½¿ç”¨ï¼‰
log_every: 5                # æ¯5æ­¥è®°å½•è®­ç»ƒæŒ‡æ ‡
save_every: 25              # ğŸ”§ ä¼˜åŒ–ï¼šæ¯25æ­¥ä¿å­˜checkpointï¼ˆæ›´é¢‘ç¹ï¼Œä¾¿äºæ¢å¤å’Œé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼‰

# ============================================
# Checkpointing & Output
# ============================================
output_dir: "checkpoints/qwen25-7b/grpo_mixed"
checkpointing:
  save_dir: "checkpoints/qwen25-7b/grpo_mixed"

# ============================================
# AFlow Configuration
# ============================================
aflow_config_path: "config/aflow_llm.yaml"
aflow_executor_model: "gpt-4o-mini"             # âœ… æ‰§è¡Œå¼•æ“ï¼šä½¿ç”¨OpenAIå®˜æ–¹gpt-4o-mini
aflow_operator_descriptions_path: "config/aflow_operators.yaml"
execution_timeout: 180                     # æ‰§è¡Œè¶…æ—¶ï¼ˆç§’ï¼‰

# ============================================
# Experience Buffer (é«˜è´¨é‡æ ·æœ¬å­˜å‚¨)
# ============================================
experience_buffer:
  buffer_size: 100
  reward_threshold: 8.0      # ä»…ä¿å­˜reward>=8.0çš„æ ·æœ¬
  persistence_dir: "data/experience_buffer"

# ============================================
# Prompt Optimization (Layer 1)
# ============================================
prompt_optimizer:
  enabled: true              # å¯ç”¨åŠ¨æ€æç¤ºè¯ä¼˜åŒ–

# ============================================
# Operator Prompt Enhancement (Layer 2)
# ============================================
operator_prompt_enhancer:
  enabled: true              # å¯ç”¨Operatoræç¤ºè¯å¢å¼º

# ============================================
# W&B Monitoring
# ============================================
wandb:
  enabled: true
  project: "aflow-roll-integration"
  run_name: "grpo-500steps-4batch-6workflows-reference-restored"
  # api_keyä»ç¯å¢ƒå˜é‡è¯»å–: WANDB_API_KEY

# ============================================
# è®­ç»ƒè¯´æ˜ - RESTORED VERSION (æ¢å¤Referenceé…ç½®)
# ============================================
# æ€»æ ·æœ¬æ•°ï¼š500 steps Ã— 4 batch Ã— 6 workflows = 12,000 ä¸ªå·¥ä½œæµç”Ÿæˆå’Œæ‰§è¡Œ
#
# å‚æ•°è¯´æ˜ï¼ˆä¸Referenceé¡¹ç›®ä¸€è‡´ï¼‰ï¼š
# 1. LoRAç§©æ¢å¤ä¸º64ï¼ˆé˜²æ­¢è¡¨è¾¾èƒ½åŠ›ä¸è¶³ï¼‰
# 2. å­¦ä¹ ç‡è°ƒæ•´ä¸º2.0e-5ï¼ˆå¹³è¡¡å­¦ä¹ é€Ÿåº¦å’Œç¨³å®šæ€§ï¼‰
# 3. æ‰¹æ¬¡å¤§å°æ¢å¤ä¸º4ï¼ˆæ ‡å‡†é…ç½®ï¼‰
# 4. æ¸©åº¦è®¾å®šä¸º0.4ï¼ˆå›ºå®šï¼Œä¸ä½¿ç”¨åŠ¨æ€è°ƒåº¦ï¼‰
# 5. max_tokenså¢åŠ åˆ°4096ï¼ˆé˜²æ­¢æˆªæ–­ï¼‰
#
# è®­ç»ƒæµç¨‹ï¼š
# 1. é‡‡æ ·ï¼šbatch_size=4ï¼Œä»mixedæ•°æ®é›†ï¼ˆmath:qa:code=4:3:3ï¼‰å‡è¡¡é‡‡æ ·
# 2. ç”Ÿæˆï¼šæ¯ä¸ªsampleç”Ÿæˆ6ä¸ªworkflowï¼ˆGRPOç»„ï¼‰
# 3. æ‰§è¡Œï¼šAFlowæ‰§è¡Œå™¨è¿è¡Œworkflowsï¼Œè¶…æ—¶180ç§’
# 4. å¥–åŠ±ï¼šLLM Judgeï¼ˆgpt-4o-miniï¼‰è®¡ç®—å¥–åŠ±
# 5. æ›´æ–°ï¼šGRPOä¼˜åŒ–å™¨æ›´æ–°LoRAæƒé‡
# 6. éªŒè¯ï¼šæ¯5æ­¥è®°å½•æŒ‡æ ‡ï¼Œæ¯25æ­¥ä¿å­˜checkpoint
#
# å…³é”®é…ç½®ç¡®è®¤ï¼ˆæ¢å¤åï¼‰ï¼š
# âœ… max_steps: 500ï¼ˆä¿æŒï¼Œæ€»æ­¥æ•°ä¸å˜ï¼‰
# âœ… rollout_batch_size: 4ï¼ˆæ¢å¤åˆ°åŸå€¼ï¼‰
# âœ… learning_rate: 2.0e-5ï¼ˆæ¢å¤åˆ°Referenceå€¼ï¼‰
# âœ… lora_rank: 64ï¼ˆæ¢å¤åˆ°åŸå€¼ï¼‰
# âœ… lora_alpha: 64ï¼ˆå¯¹åº”æ¢å¤ï¼Œç»´æŒ alpha/rank = 1.0ï¼‰
# âœ… lora_dropout: 0.05ï¼ˆè°ƒæ•´åˆ°Referenceå€¼ï¼‰
# âœ… warmup_steps: 100ï¼ˆä¿æŒï¼Œå¯¹åº”500æ­¥è®­ç»ƒçš„20%ï¼‰
# âœ… save_every: 25ï¼ˆä¿æŒé¢‘ç‡ï¼‰
# âœ… num_return_sequences_in_group: 6ï¼ˆä¿æŒï¼‰
# âœ… temperature: å›ºå®šä¸º0.4ï¼ˆéåŠ¨æ€è°ƒåº¦ï¼‰
# âœ… max_tokens: 4096ï¼ˆå¢å¤§é˜²æ­¢æˆªæ–­ï¼‰
# âœ… domain_ratios: math 40%, qa 30%, code 30%ï¼ˆä¿æŒå‡è¡¡é‡‡æ ·ï¼‰
