# Model Configuration
base_model: "/root/llm-as-judge-new/models/Qwen2.5-7B-Instruct"
device_mapping: [0]
physical_gpus: [0]
bf16: false
use_gradient_checkpointing: true

# Training Parameters
max_steps: 500
rollout_batch_size: 4
num_return_sequences_in_group: 4

# Learning & Optimization
learning_rate: 2.0e-5
warmup_steps: 100
gradient_accumulation_steps: 1
forward_pass_microbatch_size: 4
clip_range: 0.2
max_grad_norm: 1.0
weight_decay: 0.01
use_kl_loss: true
kl_loss_coef: 0.02

# Generation Config (simplified - fixed temperature)
generation_config:
  temperature: 0.4  # Fixed temperature - no scheduling
  max_tokens: 8192
  top_p: 0.95
  top_k: 50

# LoRA Configuration
use_lora: true
lora_rank: 128
lora_alpha: 128
lora_dropout: 0.05
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"

# Data Configuration
data_dir: "data"
train_dataset: "data/mixed/train_mixed.jsonl"
test_dataset: "data/mixed/test_mixed.jsonl"
domain_ratios:
  math: 0.4
  qa: 0.3
  code: 0.3

# Evaluation Strategy
eval_every: 0  # Disabled to prevent test leakage
val_samples: 50
log_every: 5
save_every: 20

# Checkpointing & Output
output_dir: "checkpoints/qwen25-7b/grpo_mixed"
checkpointing:
  save_dir: "checkpoints/qwen25-7b/grpo_mixed"

# AFlow Configuration
aflow_config_path: "config/aflow_llm.yaml"
aflow_executor_model: "gpt-4o-mini"
aflow_operator_descriptions_path: "config/aflow_operators.yaml"
execution_timeout: 180

# Reward System Configuration (5-Tier with Clear Learning Gradient)
reward_system:
  tier_system: "5-tier"  # [0.0, 0.25, 0.5, 0.75, 1.0]
  auto_fix: false        # No auto-fix - model must learn correct generation
  strict_validation: true # Strict mode - validation errors fail immediately

# Experience Buffer
experience_buffer:
  buffer_size: 100
  reward_threshold: 0.0  # Keep all experiences for learning from failures
  persistence_dir: "data/experience_buffer"

# W&B Monitoring
wandb:
  enabled: true
  project: "aflow-roll-integration"
  run_name: "grpo-500steps-4batch-4workflows-per-sample"
