# Minimal GRPO Training Configuration - 10 Steps for Testing
# æ·±åº¦èåˆAFlow + ROLLçš„åœ¨çº¿å­¦ä¹ æ¡†æ¶

# ============================================
# Model Configuration
# ============================================
base_model: "Qwen/Qwen2.5-7B-Instruct"
device_mapping: [0]      # GPU ID
physical_gpus: [0]       # å®é™…ç‰©ç†GPU ID
bf16: false              # ä½¿ç”¨float16ç²¾åº¦

# ============================================
# Training Parameters - MINIMAL VERSION
# ============================================
# æ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼š15 steps x 8 batch x 6 workflow per batch = 720ä¸ªå·¥ä½œæµï¼ˆæ–¹æ¡ˆDä¼˜åŒ–ï¼‰
max_steps: 15                         # ğŸ”§ ä¼˜åŒ–ï¼šå¢åŠ åˆ°15æ­¥ï¼Œç¡®ä¿å……åˆ†è®­ç»ƒå’Œç¨³å®šæ€§
rollout_batch_size: 8                 # ğŸ”§ ä¼˜åŒ–ï¼šå¢åŠ åˆ°8ï¼Œå‡å°‘æ¢¯åº¦å™ªå£°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
num_return_sequences_in_group: 6      # âœ… ä¿æŒï¼š6 workflows per problem (GRPOç»„)

# Learning & Optimization
learning_rate: 0.00005  # ğŸ”§ ä¼˜åŒ–ï¼šé™ä½ä» 0.0001 â†’ 0.00005ï¼Œæƒé‡æ›´æ–°æ›´æ¸©å’Œï¼Œé˜²æ­¢æ¿€è¿›å˜åŒ–
warmup_steps: 3                     # ğŸ”§ ä¼˜åŒ–ï¼šè°ƒæ•´ä¸º3æ­¥é€‚åº”15æ­¥è®­ç»ƒï¼ˆæ€»æ­¥æ•°çš„20%ï¼‰
gradient_accumulation_steps: 1
clip_range: 0.2
max_grad_norm: 1.0
weight_decay: 0.01
use_kl_loss: true
kl_loss_coef: 0.1

# ============================================
# Temperature Scheduling
# ============================================
# âœ… ä¿æŒå›ºå®šæ¸©åº¦ä¸º0.2ï¼ˆç¦ç”¨åŠ¨æ€è°ƒåº¦ï¼‰
temperature_schedule:
  enabled: false              # âŒ ç¦ç”¨åŠ¨æ€è°ƒåº¦
  initial: 0.2                # å›ºå®šå€¼
  final: 0.2                  # å›ºå®šå€¼ï¼ˆä¸ä½¿ç”¨ï¼‰
  warmup_steps: 0             # ä¸ä½¿ç”¨

# ç”Ÿæˆé…ç½®ï¼ˆå½“temperature_schedule.enabled=falseæ—¶ä½¿ç”¨ï¼‰
generation_config:
  temperature: 0.2            # âœ… å›ºå®šä¸º0.2
  max_tokens: 2048
  top_p: 0.95
  top_k: 40

# ============================================
# LoRA Configuration
# ============================================
use_lora: true
lora_rank: 32  # ğŸ”§ ä¼˜åŒ–ï¼šé™ä½ä» 64 â†’ 32ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼Œé˜²æ­¢æƒé‡é€€åŒ–
lora_alpha: 16  # ğŸ”§ ä¼˜åŒ–ï¼šå¯¹åº”é™ä½ alphaï¼ˆç»´æŒ alpha/rank = 0.5 æ¯”ä¾‹ï¼‰
lora_dropout: 0.1
lora_target_modules: "q_proj,v_proj,up_proj,down_proj"

# ============================================
# Data Configuration
# ============================================
data_dir: "data"      # æ•°æ®æ ¹ç›®å½•ï¼ŒDataManagerä¼šè‡ªåŠ¨æŸ¥æ‰¾mixed/{split}_mixed.jsonl
train_dataset: "data/mixed/train_mixed.jsonl"
test_dataset: "data/mixed/test_mixed.jsonl"

# åŸŸçº§åˆ«æƒé‡é…ç½®
domain_ratios:
  math: 0.4                 # æ•°å­¦40%
  qa: 0.3                   # QA30%
  code: 0.3                 # Code30%

# ============================================
# Reward & Evaluation
# ============================================
reward_weights:
  correctness: 0.7          # æ­£ç¡®æ€§æƒé‡
  efficiency: 0.2           # æ•ˆç‡æƒé‡
  code_quality: 0.1         # ä»£ç è´¨é‡æƒé‡

# è¯„ä¼°ç­–ç•¥ï¼ˆé¢‘ç¹è®°å½•ç”¨äºæµ‹è¯•ï¼‰
eval_every: 0               # ç¦ç”¨åœ¨çº¿æµ‹è¯•é›†è¯„ä¼°ï¼ˆé¿å…æµ‹è¯•é›†æ³„éœ²ï¼‰
val_samples: 10             # å‡å°‘éªŒè¯æ ·æœ¬æ•°
log_every: 1                # æ¯1æ­¥è®°å½•è®­ç»ƒæŒ‡æ ‡ï¼ˆè¯¦ç»†ç›‘æ§ï¼‰
save_every: 5               # ğŸ”§ ä¼˜åŒ–ï¼šæ¯5æ­¥ä¿å­˜checkpointï¼ˆæ›´é¢‘ç¹çš„æ£€æŸ¥ç‚¹ï¼Œä¾¿äºæ¢å¤ï¼‰

# ============================================
# Checkpointing & Output
# ============================================
output_dir: "checkpoints/qwen25-7b/grpo_minimal"
checkpointing:
  save_dir: "checkpoints/qwen25-7b/grpo_minimal"

# ============================================
# AFlow Configuration
# ============================================
aflow_config_path: "config/aflow_llm.yaml"
aflow_executor_model: "gpt-4o-mini"             # âœ… æ‰§è¡Œå¼•æ“ï¼šä½¿ç”¨OpenAIå®˜æ–¹gpt-4o-mini
aflow_operator_descriptions_path: "config/aflow_operators.yaml"
execution_timeout: 180                     # æ‰§è¡Œè¶…æ—¶ï¼ˆç§’ï¼‰

# ============================================
# Experience Buffer (é«˜è´¨é‡æ ·æœ¬å­˜å‚¨)
# ============================================
experience_buffer:
  buffer_size: 20              # å‡å°‘buffer sizeé€‚åº”æµ‹è¯•
  reward_threshold: 8.0        # ä»…ä¿å­˜reward>=8.0çš„æ ·æœ¬
  persistence_dir: "data/experience_buffer"

# ============================================
# Prompt Optimization (Layer 1)
# ============================================
prompt_optimizer:
  enabled: false              # âŒ æµ‹è¯•æ—¶ç¦ç”¨åŠ¨æ€æç¤ºè¯ä¼˜åŒ–

# ============================================
# Operator Prompt Enhancement (Layer 2)
# ============================================
operator_prompt_enhancer:
  enabled: false              # âŒ æµ‹è¯•æ—¶ç¦ç”¨Operatoræç¤ºè¯å¢å¼º

# ============================================
# W&B Monitoring
# ============================================
wandb:
  enabled: true
  project: "aflow-roll-minimal-test"
  run_name: "grpo-15steps-8batch-6workflows-tuned-planD"
  # api_keyä»ç¯å¢ƒå˜é‡è¯»å–: WANDB_API_KEY

# ============================================
# è®­ç»ƒè¯´æ˜ - OPTIMIZED VERSION (æ–¹æ¡ˆD)
# ============================================
# æ€»æ ·æœ¬æ•°ï¼š15 steps Ã— 8 batch Ã— 6 workflows = 720 ä¸ªå·¥ä½œæµç”Ÿæˆå’Œæ‰§è¡Œ
#
# ä¼˜åŒ–ç›®æ ‡ï¼ˆè§£å†³ Step 8 æƒé‡é€€åŒ–é—®é¢˜ï¼‰ï¼š
# 1. é™ä½ LoRA ç§©ï¼ˆ64 â†’ 32ï¼‰é˜²æ­¢è¿‡æ‹Ÿåˆ
# 2. é™ä½å­¦ä¹ ç‡ï¼ˆ0.0001 â†’ 0.00005ï¼‰æƒé‡æ›´æ–°æ›´æ¸©å’Œ
# 3. å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆ4 â†’ 8ï¼‰å‡å°‘æ¢¯åº¦å™ªå£°
# 4. å¢åŠ è®­ç»ƒæ­¥æ•°ï¼ˆ10 â†’ 15ï¼‰ç¡®ä¿å……åˆ†å­¦ä¹ å’Œç¨³å®šæ€§
# 5. é¢„æœŸï¼šç»´æŒ 50-60% å‡†ç¡®ç‡å…¨ç¨‹ï¼Œæ—  Step 8 çš„æƒé‡å´©æºƒ
#
# å…³é”®é…ç½®ç¡®è®¤ï¼ˆä¼˜åŒ–åï¼‰ï¼š
# ğŸ”§ max_steps: 15ï¼ˆä» 10 å¢åŠ ï¼Œç¡®ä¿å……åˆ†è®­ç»ƒï¼‰
# ğŸ”§ rollout_batch_size: 8ï¼ˆä» 4 å¢åŠ ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼‰
# ğŸ”§ learning_rate: 0.00005ï¼ˆä» 0.0001 é™ä½ 50%ï¼‰
# ğŸ”§ lora_rank: 32ï¼ˆä» 64 é™ä½ 50%ï¼Œå…³é”®ä¼˜åŒ–ï¼‰
# ğŸ”§ save_every: 5ï¼ˆæ¯ 5 æ­¥ä¿å­˜ï¼‰
# âœ… num_return_sequences_in_group: 6
# âœ… temperature: å›ºå®šä¸º0.2ï¼ˆéåŠ¨æ€ï¼‰
# âœ… domain_ratios: math 40%, qa 30%, code 30%
# âœ… 5:1æ•°æ®åˆ†å‰²ï¼ˆtrain:test=83.3%:16.7%ï¼‰