# Minimal GRPO Training Configuration - 10 Steps for Testing
# 深度融合AFlow + ROLL的在线学习框架

# ============================================
# Model Configuration
# ============================================
base_model: "/root/llm-as-judge-new/models"  # ✅ 本地模型路径（无嵌套）
device_mapping: [0]      # GPU ID
physical_gpus: [0]       # 实际物理GPU ID
bf16: false              # 使用float16精度

# ============================================
# Training Parameters - MINIMAL VERSION (对齐到training.yaml)
# ============================================
# 核心训练目标：10 steps x 4 batch x 6 workflow per batch = 240个工作流（最小化测试）
max_steps: 10                         # ✅ 快速测试：10步（用户要求）
rollout_batch_size: 4                 # ✅ 对齐到training.yaml
num_return_sequences_in_group: 6      # ✅ 保持：6 workflows per problem (GRPO组)

# Learning & Optimization
learning_rate: 2.0e-5   # ✅ 对齐到training.yaml（2.0e-5 = 0.00002）
warmup_steps: 2                     # ✅ 对齐：10步的20% = 2步
gradient_accumulation_steps: 1
clip_range: 0.2
max_grad_norm: 1.0
weight_decay: 0.01
use_kl_loss: true
kl_loss_coef: 0.1

# ============================================
# Temperature Scheduling
# ============================================
# ✅ 对齐到training.yaml：固定温度为0.4（禁用动态调度）
temperature_schedule:
  enabled: false              # ❌ 禁用动态调度
  initial: 0.4                # ✅ 对齐到training.yaml
  final: 0.4                  # ✅ 对齐到training.yaml
  warmup_steps: 0             # 不使用

# 生成配置（当temperature_schedule.enabled=false时使用）
generation_config:
  temperature: 0.4            # ✅ 对齐到training.yaml
  max_tokens: 4096            # ✅ 对齐到training.yaml（防止截断）
  top_p: 0.95
  top_k: 50                   # ✅ 对齐到training.yaml

# ============================================
# LoRA Configuration
# ============================================
use_lora: true
lora_rank: 64          # ✅ 对齐到training.yaml
lora_alpha: 64         # ✅ 对齐到training.yaml（维持 alpha/rank = 1.0 比例）
lora_dropout: 0.05     # ✅ 对齐到training.yaml
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"  # ✅ 对齐到training.yaml

# ============================================
# Data Configuration
# ============================================
data_dir: "data"      # 数据根目录，DataManager会自动查找mixed/{split}_mixed.jsonl
train_dataset: "data/mixed/train_mixed.jsonl"
test_dataset: "data/mixed/test_mixed.jsonl"

# 域级别权重配置
domain_ratios:
  math: 0.4                 # 数学40%
  qa: 0.3                   # QA30%
  code: 0.3                 # Code30%

# ============================================
# Reward & Evaluation
# ============================================
reward_weights:
  correctness: 0.7          # 正确性权重
  efficiency: 0.2           # 效率权重
  code_quality: 0.1         # 代码质量权重

# 评估策略（对齐到training.yaml）
eval_every: 0               # ✅ 禁用在线测试集评估（避免测试集泄露）
val_samples: 50             # ✅ 对齐到training.yaml
log_every: 5                # ✅ 对齐到training.yaml（每5步记录训练指标）
save_every: 5               # ✅ 保留：适合快速测试（每5步保存checkpoint，便于恢复）

# ============================================
# Checkpointing & Output
# ============================================
output_dir: "checkpoints/qwen25-7b/grpo_minimal"
checkpointing:
  save_dir: "checkpoints/qwen25-7b/grpo_minimal"

# ============================================
# AFlow Configuration
# ============================================
aflow_config_path: "config/aflow_llm.yaml"
aflow_executor_model: "gpt-4o-mini"             # ✅ 执行引擎：使用OpenAI官方gpt-4o-mini
aflow_operator_descriptions_path: "config/aflow_operators.yaml"
execution_timeout: 180                     # 执行超时（秒）

# ============================================
# Experience Buffer (高质量样本存储)
# ============================================
experience_buffer:
  buffer_size: 100             # ✅ 对齐到training.yaml
  reward_threshold: 8.0        # ✅ 保持相同
  persistence_dir: "data/experience_buffer"

# ============================================
# Prompt Optimization (Layer 1)
# ============================================
prompt_optimizer:
  enabled: true              # ✅ 对齐到training.yaml（启用动态提示词优化）

# ============================================
# Operator Prompt Enhancement (Layer 2)
# ============================================
operator_prompt_enhancer:
  enabled: true              # ✅ 对齐到training.yaml（启用Operator提示词增强）

# ============================================
# W&B Monitoring
# ============================================
wandb:
  enabled: true
  project: "aflow-roll-integration"
  run_name: "grpo-10steps-4batch-6workflows-minimal-aligned"
  # api_key从环境变量读取: WANDB_API_KEY

# ============================================
# 训练说明 - MINIMAL VERSION (快速测试)
# ============================================
# 总样本数：10 steps × 4 batch × 6 workflows = 240 个工作流生成和执行
#
# 配置说明（对齐到training.yaml）：
# 1. LoRA秩：64（与training.yaml一致，防止表达能力不足）
# 2. 学习率：2.0e-5（与training.yaml一致，平衡学习速度）
# 3. 批次大小：4（与training.yaml一致，标准配置）
# 4. 温度：0.4（与training.yaml一致，固定不使用动态调度）
# 5. max_tokens：4096（与training.yaml一致，防止截断）
# 6. 训练步数：10（快速测试，不同于500步的完整训练）
# 7. 保存频率：5步（适合快速测试的checkpoint间隔）
#
# 关键配置确认（对齐后）：
# ✅ max_steps: 10（快速测试，用户要求）
# ✅ rollout_batch_size: 4（对齐到training.yaml）
# ✅ learning_rate: 2.0e-5（对齐到training.yaml）
# ✅ lora_rank: 64（对齐到training.yaml）
# ✅ lora_alpha: 64（对齐到training.yaml，维持alpha/rank=1.0）
# ✅ lora_dropout: 0.05（对齐到training.yaml）
# ✅ warmup_steps: 2（对齐比例：10步的20%）
# ✅ temperature: 0.4（对齐到training.yaml）
# ✅ max_tokens: 4096（对齐到training.yaml）
# ✅ save_every: 5（保留：适合quick test）
# ✅ num_return_sequences_in_group: 6
# ✅ domain_ratios: math 40%, qa 30%, code 30%（保持均衡采样）